{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proyecto Big Data: Marco Ferraro\n",
    "Profesor: Luis Alexander Calvo\n",
    "### Módulo de Machine Learning\n",
    "\n",
    "En esta sección del proyecto, vamos a generar una ingesta de datos sobre nuestro servidor de postgreSQL.\n",
    "Vamos a pre procesar los datos utilizando pyspark y vamos a realizar predicciones sobre dos modelos:\n",
    "* Un modelo de regresión lineal con multiples dimensiones\n",
    "* Un random forrest\n",
    "\n",
    "La idea es comparar el rendimiento de estos modelos sobre varias metricas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Ingesta de Datos\n",
    "\n",
    "Vamos a iniciar una sesión de Spark "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import findspark\n",
    "\n",
    "\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ps: unrecognized option: p\n",
      "BusyBox v1.30.1 (2019-10-26 11:23:07 UTC) multi-call binary.\n",
      "\n",
      "Usage: ps [-o COL1,COL2=HEADER]\n",
      "\n",
      "Show list of processes\n",
      "\n",
      "\t-o COL1,COL2=HEADER\tSelect columns for display\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/01/07 20:47:44 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "/usr/lib/python3.7/site-packages/pyspark/context.py:317: FutureWarning: Python 3.7 support is deprecated in Spark 3.4.\n",
      "  warnings.warn(\"Python 3.7 support is deprecated in Spark 3.4.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import (IntegerType, FloatType,\n",
    "                               StructField, StructType)\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"ML Solution\") \\\n",
    "    .config(\"spark.driver.extraClassPath\", \"postgresql-42.2.14.jar\") \\\n",
    "    .config(\"spark.executor.extraClassPath\", \"postgresql-42.2.14.jar\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+\n",
      "|climate_date|           AVG_T2M|       AVG_T2M_MAX|       AVG_T2M_MIN|         Commodity|       AVG_Minimum|       AVG_Maximum|       AVG_Average|\n",
      "+------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+\n",
      "|     2013-09| 18.75804301075269|22.539913978494617|15.620290322580646|            Bakula|              90.0|             100.0|              95.0|\n",
      "|     2013-12| 7.968272632674298|14.078792924037462| 4.012570239334026|    Cabbage(Local)|26.291666666666668|            31.125|28.708333333333332|\n",
      "|     2014-05| 20.86644120707596|27.129656607700316| 14.89005202913632|    Papaya(Nepali)|49.130434782608695|              55.0| 52.06521739130435|\n",
      "|     2014-06| 23.16148387096774| 28.63832258064516| 17.95374193548387|          Brocauli|             120.0| 130.8695652173913|125.43478260869566|\n",
      "|     2014-10|15.086690946930283|19.896191467221644| 11.35987513007284|      Smooth Gourd| 45.90909090909091| 53.18181818181818| 49.54545454545455|\n",
      "|     2014-10|15.086690946930283|19.896191467221644| 11.35987513007284|             Neuro|41.666666666666664|             49.75|45.708333333333336|\n",
      "|     2015-02| 9.691474654377881| 16.45479262672811| 4.602523041474655|      Squash(Long)|             24.24|             28.24|             26.24|\n",
      "|     2015-07|20.828033298647245|24.606930280957336|17.552726326742974|        Chilli Dry|200.32258064516128|210.32258064516128|205.32258064516128|\n",
      "|     2015-08|20.269323621227887| 23.75020811654526|17.379011446409987|Onion Dry (Indian)| 81.38709677419355| 85.38709677419355| 83.38709677419355|\n",
      "|     2015-08|20.269323621227887| 23.75020811654526|17.379011446409987|      Red Cabbbage| 71.93548387096774| 81.45161290322581| 76.69354838709677|\n",
      "|     2015-10|15.961529656607702| 21.27947970863684|11.957429760665972|             Lemon|              28.0|             32.85|            30.425|\n",
      "|     2015-12| 8.324526534859523|14.959011446409992| 4.146420395421435|       Onion Green| 67.74193548387096| 77.58064516129032| 72.66129032258064|\n",
      "|     2016-02|10.742658509454952|18.234549499443826| 5.226607341490545|      Bottle Gourd| 32.93103448275862| 40.51724137931034|36.724137931034484|\n",
      "|     2016-02|10.742658509454952|18.234549499443826| 5.226607341490545|      Squash(Long)| 37.51724137931034|45.689655172413794| 41.60344827586207|\n",
      "|     2016-03|14.888095733610822| 22.10498439125911|  9.03404786680541|        Chilli Dry|             270.0|             280.0|             275.0|\n",
      "|     2016-06| 21.85152688172043|26.350462365591397| 17.89176344086022|            Banana|              78.0| 87.66666666666667| 82.83333333333333|\n",
      "|     2016-09|19.032086021505375|22.758913978494622|16.028096774193546| Garlic Dry Nepali|             255.0|             266.0|             260.5|\n",
      "|     2013-11|11.253344086021507| 16.93023655913979| 7.375397849462367|      Mustard Leaf| 40.95238095238095|46.666666666666664| 43.80952380952381|\n",
      "|     2014-03|12.582622268470347| 19.39669094693028|  6.95369406867846|         Sugarbeet| 57.95454545454545| 67.72727272727273| 62.84090909090909|\n",
      "|     2014-05| 20.86644120707596|27.129656607700316| 14.89005202913632|           Pumpkin|21.217391304347824|24.130434782608695| 22.67391304347826|\n",
      "+------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark \\\n",
    "    .read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", \"jdbc:postgresql://host.docker.internal:5433/postgres\") \\\n",
    "    .option(\"user\", \"postgres\") \\\n",
    "    .option(\"password\", \"testPassword\") \\\n",
    "    .option(\"dbtable\", \"gold_table\") \\\n",
    "    .load()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos que contamos con un df de casi 6 mil records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5985"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cambiamos el nombre de varias columnas por motivos de comprension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+---------+-----------------+------------------+------------------+-------------+-------------+---------+\n",
      "|climate_date|Commodity|         AVG_TEMP|      AVG_MAX_TEMP|      AVG_MIN_TEMP|AVG_MIN_PRICE|AVG_MAX_PRICE|AVG_PRICE|\n",
      "+------------+---------+-----------------+------------------+------------------+-------------+-------------+---------+\n",
      "|     2013-09|   Bakula|18.75804301075269|22.539913978494617|15.620290322580646|         90.0|        100.0|     95.0|\n",
      "+------------+---------+-----------------+------------------+------------------+-------------+-------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df.select(\n",
    "    df[\"climate_date\"],\n",
    "    df[\"Commodity\"],\n",
    "    df[\"AVG_T2M\"].alias(\"AVG_TEMP\"),\n",
    "    df[\"AVG_T2M_MAX\"].alias(\"AVG_MAX_TEMP\"),\n",
    "    df[\"AVG_T2M_MIN\"].alias(\"AVG_MIN_TEMP\"),\n",
    "    df[\"AVG_Minimum\"].alias(\"AVG_MIN_PRICE\"),\n",
    "    df[\"AVG_Maximum\"].alias(\"AVG_MAX_PRICE\"),\n",
    "    df[\"AVG_Average\"].alias(\"AVG_PRICE\")\n",
    ")\n",
    "\n",
    "df.limit(1).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Preprocesamiento de Datos\n",
    "\n",
    "Tenemos una columna llamada Commodity que representa los productos de venda en el mercado. Vamos a hacer una exploración sobre estos datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import FeatureHasher\n",
    "from pyspark.sql.functions import countDistinct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos que contamos con 128 datos únicos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "product_count = df.agg(countDistinct(\"Commodity\")).collect()[0][0]\n",
    "product_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por la gran cantidad de valores categóricos, vamos a hacer un mapeo hashing para representar cada valor de forma única sin tratar de alterar las predicciones. El mayor contrapeso de esta técnica es que no se puede hacer un reverse hashing, pero no hay problema ya que solo nos vamos a enfocar en la eficiencia de los modelos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|hashed_products  |\n",
      "+-----------------+\n",
      "|(128,[118],[1.0])|\n",
      "|(128,[30],[1.0]) |\n",
      "|(128,[0],[1.0])  |\n",
      "|(128,[15],[1.0]) |\n",
      "|(128,[84],[1.0]) |\n",
      "|(128,[114],[1.0])|\n",
      "|(128,[26],[1.0]) |\n",
      "|(128,[102],[1.0])|\n",
      "|(128,[6],[1.0])  |\n",
      "|(128,[88],[1.0]) |\n",
      "|(128,[53],[1.0]) |\n",
      "|(128,[0],[1.0])  |\n",
      "|(128,[58],[1.0]) |\n",
      "|(128,[26],[1.0]) |\n",
      "|(128,[102],[1.0])|\n",
      "|(128,[119],[1.0])|\n",
      "|(128,[105],[1.0])|\n",
      "|(128,[33],[1.0]) |\n",
      "|(128,[16],[1.0]) |\n",
      "|(128,[86],[1.0]) |\n",
      "+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hasher = FeatureHasher(inputCols=[\"Commodity\"], outputCol=\"hashed_products\", numFeatures=product_count)\n",
    "\n",
    "\n",
    "hashed_df = hasher.transform(df)\n",
    "hashed_df.select(\"hashed_products\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------------+------------------+------------------+------------------+------------------+------------------+-----------------+\n",
      "|climate_date|          AVG_TEMP|      AVG_MAX_TEMP|      AVG_MIN_TEMP|     AVG_MIN_PRICE|     AVG_MAX_PRICE|         AVG_PRICE|  hashed_products|\n",
      "+------------+------------------+------------------+------------------+------------------+------------------+------------------+-----------------+\n",
      "|     2013-09| 18.75804301075269|22.539913978494617|15.620290322580646|              90.0|             100.0|              95.0|(128,[118],[1.0])|\n",
      "|     2013-12| 7.968272632674298|14.078792924037462| 4.012570239334026|26.291666666666668|            31.125|28.708333333333332| (128,[30],[1.0])|\n",
      "|     2014-05| 20.86644120707596|27.129656607700316| 14.89005202913632|49.130434782608695|              55.0| 52.06521739130435|  (128,[0],[1.0])|\n",
      "|     2014-06| 23.16148387096774| 28.63832258064516| 17.95374193548387|             120.0| 130.8695652173913|125.43478260869566| (128,[15],[1.0])|\n",
      "|     2014-10|15.086690946930283|19.896191467221644| 11.35987513007284| 45.90909090909091| 53.18181818181818| 49.54545454545455| (128,[84],[1.0])|\n",
      "|     2014-10|15.086690946930283|19.896191467221644| 11.35987513007284|41.666666666666664|             49.75|45.708333333333336|(128,[114],[1.0])|\n",
      "|     2015-02| 9.691474654377881| 16.45479262672811| 4.602523041474655|             24.24|             28.24|             26.24| (128,[26],[1.0])|\n",
      "|     2015-07|20.828033298647245|24.606930280957336|17.552726326742974|200.32258064516128|210.32258064516128|205.32258064516128|(128,[102],[1.0])|\n",
      "|     2015-08|20.269323621227887| 23.75020811654526|17.379011446409987| 81.38709677419355| 85.38709677419355| 83.38709677419355|  (128,[6],[1.0])|\n",
      "|     2015-08|20.269323621227887| 23.75020811654526|17.379011446409987| 71.93548387096774| 81.45161290322581| 76.69354838709677| (128,[88],[1.0])|\n",
      "|     2015-10|15.961529656607702| 21.27947970863684|11.957429760665972|              28.0|             32.85|            30.425| (128,[53],[1.0])|\n",
      "|     2015-12| 8.324526534859523|14.959011446409992| 4.146420395421435| 67.74193548387096| 77.58064516129032| 72.66129032258064|  (128,[0],[1.0])|\n",
      "|     2016-02|10.742658509454952|18.234549499443826| 5.226607341490545| 32.93103448275862| 40.51724137931034|36.724137931034484| (128,[58],[1.0])|\n",
      "|     2016-02|10.742658509454952|18.234549499443826| 5.226607341490545| 37.51724137931034|45.689655172413794| 41.60344827586207| (128,[26],[1.0])|\n",
      "|     2016-03|14.888095733610822| 22.10498439125911|  9.03404786680541|             270.0|             280.0|             275.0|(128,[102],[1.0])|\n",
      "|     2016-06| 21.85152688172043|26.350462365591397| 17.89176344086022|              78.0| 87.66666666666667| 82.83333333333333|(128,[119],[1.0])|\n",
      "|     2016-09|19.032086021505375|22.758913978494622|16.028096774193546|             255.0|             266.0|             260.5|(128,[105],[1.0])|\n",
      "|     2013-11|11.253344086021507| 16.93023655913979| 7.375397849462367| 40.95238095238095|46.666666666666664| 43.80952380952381| (128,[33],[1.0])|\n",
      "|     2014-03|12.582622268470347| 19.39669094693028|  6.95369406867846| 57.95454545454545| 67.72727272727273| 62.84090909090909| (128,[16],[1.0])|\n",
      "|     2014-05| 20.86644120707596|27.129656607700316| 14.89005202913632|21.217391304347824|24.130434782608695| 22.67391304347826| (128,[86],[1.0])|\n",
      "+------------+------------------+------------------+------------------+------------------+------------------+------------------+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hashed_df = hashed_df.drop(\"Commodity\")\n",
    "hashed_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|         AVG_PRICE|\n",
      "+------------------+\n",
      "|              95.0|\n",
      "|28.708333333333332|\n",
      "| 52.06521739130435|\n",
      "|125.43478260869566|\n",
      "| 49.54545454545455|\n",
      "|45.708333333333336|\n",
      "|             26.24|\n",
      "|205.32258064516128|\n",
      "| 83.38709677419355|\n",
      "| 76.69354838709677|\n",
      "|            30.425|\n",
      "| 72.66129032258064|\n",
      "|36.724137931034484|\n",
      "| 41.60344827586207|\n",
      "|             275.0|\n",
      "| 82.83333333333333|\n",
      "|             260.5|\n",
      "| 43.80952380952381|\n",
      "| 62.84090909090909|\n",
      "| 22.67391304347826|\n",
      "+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X = hashed_df.select(\"climate_date\", \"hashed_products\", \"AVG_TEMP\", \"AVG_MAX_TEMP\", \"AVG_MIN_TEMP\")\n",
    "y = hashed_df.select(\"AVG_PRICE\")\n",
    "\n",
    "y.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a separar la columna de fecha a una columna de año y otra de mes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+------------------+------------------+------------------+----+-----+\n",
      "|  hashed_products|          AVG_TEMP|      AVG_MAX_TEMP|      AVG_MIN_TEMP|year|month|\n",
      "+-----------------+------------------+------------------+------------------+----+-----+\n",
      "|(128,[118],[1.0])| 18.75804301075269|22.539913978494617|15.620290322580646|2013|    9|\n",
      "| (128,[30],[1.0])| 7.968272632674298|14.078792924037462| 4.012570239334026|2013|   12|\n",
      "|  (128,[0],[1.0])| 20.86644120707596|27.129656607700316| 14.89005202913632|2014|    5|\n",
      "| (128,[15],[1.0])| 23.16148387096774| 28.63832258064516| 17.95374193548387|2014|    6|\n",
      "| (128,[84],[1.0])|15.086690946930283|19.896191467221644| 11.35987513007284|2014|   10|\n",
      "|(128,[114],[1.0])|15.086690946930283|19.896191467221644| 11.35987513007284|2014|   10|\n",
      "| (128,[26],[1.0])| 9.691474654377881| 16.45479262672811| 4.602523041474655|2015|    2|\n",
      "|(128,[102],[1.0])|20.828033298647245|24.606930280957336|17.552726326742974|2015|    7|\n",
      "|  (128,[6],[1.0])|20.269323621227887| 23.75020811654526|17.379011446409987|2015|    8|\n",
      "| (128,[88],[1.0])|20.269323621227887| 23.75020811654526|17.379011446409987|2015|    8|\n",
      "| (128,[53],[1.0])|15.961529656607702| 21.27947970863684|11.957429760665972|2015|   10|\n",
      "|  (128,[0],[1.0])| 8.324526534859523|14.959011446409992| 4.146420395421435|2015|   12|\n",
      "| (128,[58],[1.0])|10.742658509454952|18.234549499443826| 5.226607341490545|2016|    2|\n",
      "| (128,[26],[1.0])|10.742658509454952|18.234549499443826| 5.226607341490545|2016|    2|\n",
      "|(128,[102],[1.0])|14.888095733610822| 22.10498439125911|  9.03404786680541|2016|    3|\n",
      "|(128,[119],[1.0])| 21.85152688172043|26.350462365591397| 17.89176344086022|2016|    6|\n",
      "|(128,[105],[1.0])|19.032086021505375|22.758913978494622|16.028096774193546|2016|    9|\n",
      "| (128,[33],[1.0])|11.253344086021507| 16.93023655913979| 7.375397849462367|2013|   11|\n",
      "| (128,[16],[1.0])|12.582622268470347| 19.39669094693028|  6.95369406867846|2014|    3|\n",
      "| (128,[86],[1.0])| 20.86644120707596|27.129656607700316| 14.89005202913632|2014|    5|\n",
      "+-----------------+------------------+------------------+------------------+----+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import year, month\n",
    "\n",
    "\n",
    "X = X.withColumn(\"year\", year(\"climate_date\")) \\\n",
    "       .withColumn(\"month\", month(\"climate_date\"))\n",
    "\n",
    "X = X.drop(\"climate_date\")\n",
    "\n",
    "X.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez segregado, vamos a hacer un escalamiento de datos. Acá buscamos que para cada feature, la media sea cercana a cero y la desviación éstandar tenga un valor de 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 18:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|  hashed_products|           feature_1|           feature_2|           feature_3|           feature_4|           feature_5|\n",
      "+-----------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|(128,[118],[1.0])|  0.6086600420501682|  0.3189700695422222|  0.7977327175764956| -1.6320227077331853|  0.6695934116039529|\n",
      "| (128,[30],[1.0])| -1.4780452953662864| -1.5894818423247634| -1.3170124234967804| -1.6320227077331853|  1.5400357592255112|\n",
      "|  (128,[0],[1.0])|  1.0164171689864085|  1.3542115037811033|  0.6646947159868585| -1.1145858318644635| -0.4909963852247915|\n",
      "| (128,[15],[1.0])|   1.460270708823682|  1.6944993263880777|  1.2228511236826922| -1.1145858318644635|-0.20084893601760537|\n",
      "| (128,[84],[1.0])|-0.10136711132448283| -0.2773359339544261|0.021551670592574686| -1.1145858318644635|  0.9597408608111391|\n",
      "|(128,[114],[1.0])|-0.10136711132448283| -0.2773359339544261|0.021551670592574686| -1.1145858318644635|  0.9597408608111391|\n",
      "| (128,[26],[1.0])| -1.1447838143396538| -1.0535621756504896| -1.2095322454047779| -0.5971489559957417| -1.3614387328463498|\n",
      "|(128,[102],[1.0])|  1.0089892085471357|  0.7851968483468059|   1.149792348660771| -0.5971489559957417| 0.08929851318958074|\n",
      "|  (128,[6],[1.0])|   0.900936634202654|  0.5919585006044868|  1.1181442130681043| -0.5971489559957417|  0.3794459623967668|\n",
      "| (128,[88],[1.0])|   0.900936634202654|  0.5919585006044868|  1.1181442130681043| -0.5971489559957417|  0.3794459623967668|\n",
      "| (128,[53],[1.0])|  0.0678237556629492|0.034672258813514274| 0.13041678299277126| -0.5971489559957417|  0.9597408608111391|\n",
      "|  (128,[0],[1.0])|  -1.409146987699266| -1.3909437631422568|  -1.292627017474948| -0.5971489559957417|  1.5400357592255112|\n",
      "| (128,[58],[1.0])| -0.9414883887528508| -0.6521283249193903| -1.0958338419617495|-0.07971208012701995| -1.3614387328463498|\n",
      "| (128,[26],[1.0])| -0.9414883887528508| -0.6521283249193903| -1.0958338419617495|-0.07971208012701995| -1.3614387328463498|\n",
      "|(128,[102],[1.0])|-0.13977475829916133| 0.22086933360606928| -0.4021776981978953|-0.07971208012701995| -1.0712912836391637|\n",
      "|(128,[119],[1.0])|   1.206929428567895|  1.1784600009367894|    1.21155961084327|-0.07971208012701995|-0.20084893601760537|\n",
      "|(128,[105],[1.0])|  0.6616590395433054|   0.368366711666697|   0.872028678011605|-0.07971208012701995|  0.6695934116039529|\n",
      "| (128,[33],[1.0])| -0.8427235141519868| -0.9463232086203883|  -0.704357802452808| -1.6320227077331853|  1.2498883100183251|\n",
      "| (128,[16],[1.0])| -0.5856455800503676| -0.3900009953326037| -0.7811856390123855| -1.1145858318644635| -1.0712912836391637|\n",
      "| (128,[86],[1.0])|  1.0164171689864085|  1.3542115037811033|  0.6646947159868585| -1.1145858318644635| -0.4909963852247915|\n",
      "+-----------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StandardScaler, VectorAssembler\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "feature_columns = [\"AVG_TEMP\", \"AVG_MAX_TEMP\", \"AVG_MIN_TEMP\", \"year\", \"month\"]\n",
    "\n",
    "vector_assembler = VectorAssembler(inputCols=feature_columns, outputCol=\"features\")\n",
    "X_assembled = vector_assembler.transform(X)\n",
    "\n",
    "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaled_features\", withStd=True, withMean=True)\n",
    "scaler_model = scaler.fit(X_assembled)\n",
    "X_scaled = scaler_model.transform(X_assembled)\n",
    "\n",
    "max_length = len(feature_columns)\n",
    "\n",
    "for i in range(max_length):\n",
    "    col_name = f\"feature_{i + 1}\"\n",
    "    extract_feature_udf = udf(lambda x: float(x[i]), DoubleType())\n",
    "    X_scaled = X_scaled.withColumn(col_name, extract_feature_udf(\"scaled_features\"))\n",
    "\n",
    "X_scaled = X_scaled.drop(\"features\", \"scaled_features\", \"AVG_TEMP\", \"AVG_MAX_TEMP\", \"AVG_MIN_TEMP\", \"year\", \"month\")\n",
    "\n",
    "X_scaled.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Posteriormente, vamos a separar los datasets en entrenamiento y validación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4782"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test = X_scaled.randomSplit([0.8, 0.2], seed=123)\n",
    "y_train, y_test = y.randomSplit([0.8, 0.2], seed=123)\n",
    "\n",
    "X_train.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|         AVG_PRICE|\n",
      "+------------------+\n",
      "|               5.5|\n",
      "| 6.947368421052632|\n",
      "|               9.0|\n",
      "| 9.366666666666667|\n",
      "| 9.826086956521738|\n",
      "|10.790322580645162|\n",
      "|10.866666666666667|\n",
      "|10.935483870967742|\n",
      "|11.071428571428571|\n",
      "|11.113636363636363|\n",
      "|11.433333333333334|\n",
      "|11.545454545454545|\n",
      "|11.595238095238095|\n",
      "|              11.6|\n",
      "|11.689655172413794|\n",
      "|              12.0|\n",
      "| 12.14516129032258|\n",
      "|             12.18|\n",
      "|12.296296296296296|\n",
      "|12.466666666666667|\n",
      "+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_train.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Modelos y Análisis de Resultados\n",
    "\n",
    "### 3.1 Modelo de Regresión Lineal\n",
    "\n",
    "Primero, vamos a utilizar un modelo de regresión lineal para predecir el precio optimo de cada material, dependiendo de las features. Por el tipo de implementación en pyspark, tenemos que juntar los datasets bajo un mismo indice para entrenarlo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/01/07 20:47:52 WARN Instrumentation: [89c6c972] regParam is zero, which might cause numerical instability and overfitting.\n",
      "24/01/07 20:47:53 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n",
      "24/01/07 20:47:53 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.lapack.JNILAPACK\n",
      "24/01/07 20:47:53 WARN Instrumentation: [89c6c972] Cholesky solver failed due to singular covariance matrix. Retrying with Quasi-Newton solver.\n",
      "24/01/07 20:47:53 ERROR LBFGS: Failure! Resetting history: breeze.optimize.FirstOrderException: Line search zoom failed\n",
      "24/01/07 20:47:53 ERROR LBFGS: Failure again! Giving up and returning. Maybe the objective is just poorly behaved?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-------------------+--------------------+--------------------+--------------------+------------------+--------------------+------------------+\n",
      "|  hashed_products|          feature_1|          feature_2|           feature_3|           feature_4|           feature_5|         AVG_PRICE|            features|        prediction|\n",
      "+-----------------+-------------------+-------------------+--------------------+--------------------+--------------------+------------------+--------------------+------------------+\n",
      "|  (128,[0],[1.0])| 0.8872781222813996| 0.4903458906664576|   1.171284795840241|  0.4377247957417018|  0.3794459623967668|19.041666666666668|(133,[0,128,129,1...|18.605957759265166|\n",
      "|  (128,[0],[1.0])| 0.9877840015168924| 1.2255322406241784|   0.739869938749024|  0.4377247957417018| -0.4909963852247915|19.596774193548388|(133,[0,128,129,1...|19.016145889727667|\n",
      "| (128,[39],[1.0])| 1.0207196572610788| 0.9110288554319433|  1.0764924305426848| -1.6320227077331853|-0.20084893601760537|51.774193548387096|(133,[39,128,129,...|52.972016486184735|\n",
      "|(128,[102],[1.0])| 0.6512093823823941| 0.4061169318595303|  0.8418174165595602|  0.9551616716104236|  0.6695934116039529|             114.9|(133,[102,128,129...|114.95479480516836|\n",
      "|  (128,[5],[1.0])| 1.2743291973723394|  1.418596512451907|  1.0879974712966638| -0.5971489559957417|-0.20084893601760537|              23.5|(133,[5,128,129,1...| 24.35522322393504|\n",
      "| (128,[17],[1.0])|  0.586847812406225|0.27634979203186977|  0.7990942029032481| -1.1145858318644635|  0.6695934116039529| 33.61666666666667|(133,[17,128,129,...|34.620368225684835|\n",
      "| (128,[36],[1.0])|-1.7608813408869668|-1.7622230445103209| -1.7368232169745603| -0.5971489559957417| -1.6515861820535358|47.708333333333336|(133,[36,128,129,...| 44.73472329948571|\n",
      "| (128,[46],[1.0])|-1.4780452953662864|-1.5894818423247634| -1.3170124234967804| -1.6320227077331853|  1.5400357592255112| 56.12903225806452|(133,[46,128,129,...|53.433737912120115|\n",
      "| (128,[46],[1.0])| 0.9296663993695653| 0.6189077992742142|   1.128527394968251| -1.6320227077331853| 0.08929851318958074|              57.5|(133,[46,128,129,...| 59.11670827868326|\n",
      "|(128,[105],[1.0])|-0.5486104253595123|-0.2986756851741747| -0.7962722487551938|  0.4377247957417018| -1.0712912836391637|133.07142857142858|(133,[105,128,129...|   134.33738016092|\n",
      "| (128,[20],[1.0])| 0.9752927117724952| 1.1829208038231964|  0.7883734459455237|-0.07971208012701995| -0.4909963852247915|36.206896551724135|(133,[20,128,129,...|  37.9833377958126|\n",
      "| (128,[26],[1.0])| 0.6512093823823941| 0.4061169318595303|  0.8418174165595602|  0.9551616716104236|  0.6695934116039529|             39.55|(133,[26,128,129,...|40.748070956311246|\n",
      "| (128,[27],[1.0])| -1.409146987699266|-1.3909437631422568|  -1.292627017474948| -0.5971489559957417|  1.5400357592255112|           40.3125|(133,[27,128,129,...|37.172610325304774|\n",
      "| (128,[75],[1.0])|-1.0092627072641909|-0.8864271679277915|  -0.914788772339185|-0.07971208012701995|  1.5400357592255112| 75.33333333333333|(133,[75,128,129,...| 74.42223962649048|\n",
      "|(128,[100],[1.0])|  0.590335190527389| 1.0326250674195983|  0.1790600722984745|  0.4377247957417018| -0.7811438344319775|106.07142857142857|(133,[100,128,129...| 107.8650817219704|\n",
      "|(128,[117],[1.0])|0.12245465050645439| 0.1802558971227631|-0.03429741171958973| -0.5971489559957417| -0.7811438344319775|215.58333333333334|(133,[117,128,129...|217.87407535990366|\n",
      "|(128,[118],[1.0])|-0.8427235141519868|-0.9463232086203883|  -0.704357802452808| -1.6320227077331853|  1.2498883100183251| 236.8548387096774|(133,[118,128,129...|240.94604115244113|\n",
      "| (128,[24],[1.0])|-0.5486104253595123|-0.2986756851741747| -0.7962722487551938|  0.4377247957417018| -1.0712912836391637|37.666666666666664|(133,[24,128,129,...| 36.58897446804443|\n",
      "| (128,[26],[1.0])| 0.9334357220200017| 0.5599864272305953|  1.1956417651762048|-0.07971208012701995| 0.08929851318958074|39.766666666666666|(133,[26,128,129,...| 41.49799974713268|\n",
      "| (128,[32],[1.0])| 0.7218594624589787|  0.424103159482406|  0.9358147553128667|  0.4377247957417018|  0.6695934116039529|44.910714285714285|(133,[32,128,129,...| 46.28003725729869|\n",
      "+-----------------+-------------------+-------------------+--------------------+--------------------+--------------------+------------------+--------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "\n",
    "X_train_lr = X_train.withColumn(\"index\", monotonically_increasing_id())\n",
    "y_train_lr = y_train.withColumn(\"index\", monotonically_increasing_id())\n",
    "\n",
    "X_test_lr = X_test.withColumn(\"index\", monotonically_increasing_id())\n",
    "y_test_lr = y_test.withColumn(\"index\", monotonically_increasing_id())\n",
    "\n",
    "combined_train_df = X_train_lr.join(y_train_lr.select(\"AVG_PRICE\", \"index\"), on=\"index\").drop(\"index\")\n",
    "combined_test_df = X_test_lr.join(y_test_lr.select(\"AVG_PRICE\", \"index\"), on=\"index\").drop(\"index\")\n",
    "\n",
    "feature_columns = [\"hashed_products\", \"feature_1\", \"feature_2\", \"feature_3\", \"feature_4\", \"feature_5\"]\n",
    "assembler = VectorAssembler(inputCols=feature_columns, outputCol=\"features\")\n",
    "\n",
    "combined_train_df = assembler.transform(combined_train_df)\n",
    "combined_test_df = assembler.transform(combined_test_df)\n",
    "\n",
    "lr = LinearRegression(featuresCol=\"features\", labelCol=\"AVG_PRICE\")\n",
    "\n",
    "lr_model = lr.fit(combined_train_df)\n",
    "\n",
    "predictions = lr_model.transform(combined_test_df)\n",
    "\n",
    "\n",
    "predictions.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Posteriomente usamos un evaluador para analizar las predicciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE) on test data: 8.245760434257217\n",
      "Mean Squared Error (MSE) on test data: 67.99256513916177\n",
      "Mean Absolute Error (MAE) on test data: 2.5362482466187077\n",
      "Mean Squared Error (MSE) on test data: 67.99256513916177\n",
      "R-Squared (R²) on test data: 0.9878486384510242\n"
     ]
    }
   ],
   "source": [
    "evaluator = RegressionEvaluator(labelCol=\"AVG_PRICE\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(f\"Root Mean Squared Error (RMSE) on test data: {rmse}\")\n",
    "\n",
    "mse_evaluator = RegressionEvaluator(labelCol=\"AVG_PRICE\", predictionCol=\"prediction\", metricName=\"mse\")\n",
    "mse = mse_evaluator.evaluate(predictions)\n",
    "print(f\"Mean Squared Error (MSE) on test data: {mse}\")\n",
    "\n",
    "mae_evaluator = RegressionEvaluator(labelCol=\"AVG_PRICE\", predictionCol=\"prediction\", metricName=\"mae\")\n",
    "mae = mae_evaluator.evaluate(predictions)\n",
    "print(f\"Mean Absolute Error (MAE) on test data: {mae}\")\n",
    "\n",
    "mse_evaluator = RegressionEvaluator(labelCol=\"AVG_PRICE\", predictionCol=\"prediction\", metricName=\"mse\")\n",
    "mse = mse_evaluator.evaluate(predictions)\n",
    "print(f\"Mean Squared Error (MSE) on test data: {mse}\")\n",
    "\n",
    "r2_evaluator = RegressionEvaluator(labelCol=\"AVG_PRICE\", predictionCol=\"prediction\", metricName=\"r2\")\n",
    "r2 = r2_evaluator.evaluate(predictions)\n",
    "print(f\"R-Squared (R²) on test data: {r2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos ver que, el modelo mantiene un RMSE bajo, de igual forma presenta un R-Squared bajo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Modelo de Regresión Random Forrest\n",
    "\n",
    "Igualmente, como segundo modelo, vamos a usar un algoritmo regresivo de Random Forrest para realizar predicciones. Vamos a usar un random forrest con 150 arboles de clasificación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/01/07 20:49:28 WARN DAGScheduler: Broadcasting large task binary with size 1230.4 KiB\n",
      "24/01/07 20:49:32 WARN DAGScheduler: Broadcasting large task binary with size 2.4 MiB\n",
      "24/01/07 20:49:36 WARN DAGScheduler: Broadcasting large task binary with size 4.5 MiB\n",
      "24/01/07 20:49:41 WARN DAGScheduler: Broadcasting large task binary with size 7.8 MiB\n",
      "24/01/07 20:49:46 WARN DAGScheduler: Broadcasting large task binary with size 1509.8 KiB\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-------------------+--------------------+--------------------+--------------------+------------------+--------------------+------------------+\n",
      "|  hashed_products|          feature_1|          feature_2|           feature_3|           feature_4|           feature_5|         AVG_PRICE|            features|        prediction|\n",
      "+-----------------+-------------------+-------------------+--------------------+--------------------+--------------------+------------------+--------------------+------------------+\n",
      "|  (128,[0],[1.0])| 0.8872781222813996| 0.4903458906664576|   1.171284795840241|  0.4377247957417018|  0.3794459623967668|19.041666666666668|(133,[0,128,129,1...| 65.95601484112309|\n",
      "|  (128,[0],[1.0])| 0.9877840015168924| 1.2255322406241784|   0.739869938749024|  0.4377247957417018| -0.4909963852247915|19.596774193548388|(133,[0,128,129,1...| 65.96999183717865|\n",
      "| (128,[39],[1.0])| 1.0207196572610788| 0.9110288554319433|  1.0764924305426848| -1.6320227077331853|-0.20084893601760537|51.774193548387096|(133,[39,128,129,...| 73.56736249222405|\n",
      "|(128,[102],[1.0])| 0.6512093823823941| 0.4061169318595303|  0.8418174165595602|  0.9551616716104236|  0.6695934116039529|             114.9|(133,[102,128,129...| 73.62185918013378|\n",
      "|  (128,[5],[1.0])| 1.2743291973723394|  1.418596512451907|  1.0879974712966638| -0.5971489559957417|-0.20084893601760537|              23.5|(133,[5,128,129,1...| 73.62185918013378|\n",
      "| (128,[17],[1.0])|  0.586847812406225|0.27634979203186977|  0.7990942029032481| -1.1145858318644635|  0.6695934116039529| 33.61666666666667|(133,[17,128,129,...| 73.62185918013378|\n",
      "| (128,[36],[1.0])|-1.7608813408869668|-1.7622230445103209| -1.7368232169745603| -0.5971489559957417| -1.6515861820535358|47.708333333333336|(133,[36,128,129,...| 73.62185918013378|\n",
      "| (128,[46],[1.0])|-1.4780452953662864|-1.5894818423247634| -1.3170124234967804| -1.6320227077331853|  1.5400357592255112| 56.12903225806452|(133,[46,128,129,...| 73.62185918013378|\n",
      "| (128,[46],[1.0])| 0.9296663993695653| 0.6189077992742142|   1.128527394968251| -1.6320227077331853| 0.08929851318958074|              57.5|(133,[46,128,129,...| 73.62185918013378|\n",
      "|(128,[105],[1.0])|-0.5486104253595123|-0.2986756851741747| -0.7962722487551938|  0.4377247957417018| -1.0712912836391637|133.07142857142858|(133,[105,128,129...| 75.50550749201503|\n",
      "| (128,[20],[1.0])| 0.9752927117724952| 1.1829208038231964|  0.7883734459455237|-0.07971208012701995| -0.4909963852247915|36.206896551724135|(133,[20,128,129,...| 73.62185918013378|\n",
      "| (128,[26],[1.0])| 0.6512093823823941| 0.4061169318595303|  0.8418174165595602|  0.9551616716104236|  0.6695934116039529|             39.55|(133,[26,128,129,...|  73.3908235519838|\n",
      "| (128,[27],[1.0])| -1.409146987699266|-1.3909437631422568|  -1.292627017474948| -0.5971489559957417|  1.5400357592255112|           40.3125|(133,[27,128,129,...| 73.62185918013378|\n",
      "| (128,[75],[1.0])|-1.0092627072641909|-0.8864271679277915|  -0.914788772339185|-0.07971208012701995|  1.5400357592255112| 75.33333333333333|(133,[75,128,129,...| 73.62185918013378|\n",
      "|(128,[100],[1.0])|  0.590335190527389| 1.0326250674195983|  0.1790600722984745|  0.4377247957417018| -0.7811438344319775|106.07142857142857|(133,[100,128,129...| 73.62185918013378|\n",
      "|(128,[117],[1.0])|0.12245465050645439| 0.1802558971227631|-0.03429741171958973| -0.5971489559957417| -0.7811438344319775|215.58333333333334|(133,[117,128,129...|123.41408423186024|\n",
      "|(128,[118],[1.0])|-0.8427235141519868|-0.9463232086203883|  -0.704357802452808| -1.6320227077331853|  1.2498883100183251| 236.8548387096774|(133,[118,128,129...|199.84467292985778|\n",
      "| (128,[24],[1.0])|-0.5486104253595123|-0.2986756851741747| -0.7962722487551938|  0.4377247957417018| -1.0712912836391637|37.666666666666664|(133,[24,128,129,...| 73.62185918013378|\n",
      "| (128,[26],[1.0])| 0.9334357220200017| 0.5599864272305953|  1.1956417651762048|-0.07971208012701995| 0.08929851318958074|39.766666666666666|(133,[26,128,129,...| 73.39178409588538|\n",
      "| (128,[32],[1.0])| 0.7218594624589787|  0.424103159482406|  0.9358147553128667|  0.4377247957417018|  0.6695934116039529|44.910714285714285|(133,[32,128,129,...| 73.62185918013378|\n",
      "+-----------------+-------------------+-------------------+--------------------+--------------------+--------------------+------------------+--------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "\n",
    "rf = RandomForestRegressor(featuresCol=\"features\", labelCol=\"AVG_PRICE\", numTrees=150)\n",
    "\n",
    "rf_model = rf.fit(combined_train_df)\n",
    "\n",
    "rf_predictions = rf_model.transform(combined_test_df)\n",
    "\n",
    "rf_predictions.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE) on test data for Random Forest: 41.262625106216795\n",
      "Mean Squared Error (MSE) on test data for Random Forest: 1702.6042306561928\n",
      "Mean Absolute Error (MAE) on test data for Random Forest: 34.22545413763678\n",
      "R-Squared (R²) on test data for Random Forest: 0.6957173252814526\n"
     ]
    }
   ],
   "source": [
    "rf_evaluator = RegressionEvaluator(labelCol=\"AVG_PRICE\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "rf_rmse = rf_evaluator.evaluate(rf_predictions)\n",
    "print(f\"Root Mean Squared Error (RMSE) on test data for Random Forest: {rf_rmse}\")\n",
    "\n",
    "rf_mse_evaluator = RegressionEvaluator(labelCol=\"AVG_PRICE\", predictionCol=\"prediction\", metricName=\"mse\")\n",
    "rf_mse = rf_mse_evaluator.evaluate(rf_predictions)\n",
    "print(f\"Mean Squared Error (MSE) on test data for Random Forest: {rf_mse}\")\n",
    "\n",
    "rf_mae_evaluator = RegressionEvaluator(labelCol=\"AVG_PRICE\", predictionCol=\"prediction\", metricName=\"mae\")\n",
    "rf_mae = rf_mae_evaluator.evaluate(rf_predictions)\n",
    "print(f\"Mean Absolute Error (MAE) on test data for Random Forest: {rf_mae}\")\n",
    "\n",
    "rf_r2_evaluator = RegressionEvaluator(labelCol=\"AVG_PRICE\", predictionCol=\"prediction\", metricName=\"r2\")\n",
    "rf_r2 = rf_r2_evaluator.evaluate(rf_predictions)\n",
    "print(f\"R-Squared (R²) on test data for Random Forest: {rf_r2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Asimismo, podemos ver que el rendimiento es más alto que el modelo de regresión lineal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Análisis de Resultados\n",
    "\n",
    "\n",
    "1. **Errores de Precisión:**\n",
    "   - El modelo de regresión lineal muestra un RMSE (Root Mean Squared Error) de aproximadamente 8.25, lo que indica una buena precisión en la predicción, mientras que el Random Forest tiene un RMSE de alrededor de 41.26, lo que significa que, en promedio, sus predicciones están más alejadas de los valores reales en comparación con el modelo lineal.\n",
    "   \n",
    "   - El MAE (Mean Absolute Error) también es mucho más bajo en el modelo de regresión lineal (2.53) en comparación con el Random Forest (34.22), lo que sugiere que el modelo lineal está haciendo predicciones más precisas en promedio.\n",
    "\n",
    "2. **R-Cuadrado (R²):**\n",
    "   - El valor de R² para el modelo de regresión lineal es muy alto (0.988), lo que indica que el modelo explica aproximadamente el 98.8% de la variabilidad en los datos. Por otro lado, el valor de R² para Random Forest es 0.696, lo que significa que el modelo explica aproximadamente el 69.6% de la variabilidad en los datos. Un R² más alto sugiere un mejor ajuste del modelo a los datos.\n",
    "\n",
    "### Diferencias de los modelos:\n",
    "\n",
    "1. **Complejidad del Modelo:**\n",
    "   - El modelo de regresión lineal es un modelo paramétrico que asume una relación lineal entre las variables predictoras y la variable de respuesta. Si los datos realmente siguen una relación lineal, este modelo funcionará bien, como se evidencia por los altos valores de R² y los errores bajos.\n",
    "   \n",
    "   - Por otro lado, el Random Forest es un modelo no paramétrico y se basa en árboles de decisión. Puede capturar relaciones no lineales entre las variables, lo que puede ser una ventaja cuando hay relaciones complejas en los datos. Sin embargo, también puede llevar a un sobreajuste cuando el modelo es demasiado complejo para el conjunto de datos.\n",
    "\n",
    "2. **Interpretación:**\n",
    "   - El modelo de regresión lineal es más interpretable, ya que podemos identificar el impacto individual de cada predictor en la variable de respuesta.\n",
    "   \n",
    "   - El Random Forest es menos interpretable en comparación, ya que se basa en múltiples árboles de decisión y no es fácil identificar cómo cada predictor afecta la predicción.\n",
    "\n",
    "### Siguientes Pasos:\n",
    "\n",
    "1. **Optimización de Hiperparámetros para Random Forest:**\n",
    "   - Dado que Random Forest tiene un rendimiento inferior en comparación con la regresión lineal en este caso, puedes intentar mejorar su rendimiento ajustando sus hiperparámetros. Algunos de los hiperparámetros que puedes ajustar incluyen:\n",
    "     - Número de árboles en el bosque (`n_estimators`).\n",
    "     - Profundidad máxima de los árboles (`max_depth`).\n",
    "     - Número mínimo de muestras requeridas para dividir un nodo (`min_samples_split`).\n",
    "     - Número mínimo de muestras requeridas en cada hoja del árbol (`min_samples_leaf`).\n",
    "     - Máximo número de características a considerar para dividir un nodo (`max_features`).\n",
    "\n",
    "2. **Validación Cruzada:**\n",
    "   - Realiza validación cruzada para evaluar el rendimiento del modelo Random Forest en diferentes subconjuntos de datos y asegurarte de que el modelo no esté sobreajustando o subajustando los datos.\n",
    "\n",
    "3. **Feature Engineering:**\n",
    "   - Considera realizar ingeniería de características para mejorar la calidad de los datos y potencialmente mejorar el rendimiento del modelo Random Forest.\n",
    "\n",
    "En resumen, aunque Random Forest es un modelo más flexible y puede capturar relaciones no lineales, en este caso particular, el modelo de regresión lineal muestra un rendimiento superior en términos de precisión y ajuste a los datos. Sin embargo, aún se puede intentar mejorar el rendimiento de Random Forest ajustando sus hiperparámetros y realizando validación cruzada."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
