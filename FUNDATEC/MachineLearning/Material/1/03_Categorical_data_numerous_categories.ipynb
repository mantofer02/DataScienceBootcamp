{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e411fc25",
   "metadata": {},
   "source": [
    "## Escuela de Ingeniería en Computación, ITCR \n",
    "\n",
    "## Ciencia de Datos\n",
    "\n",
    "## Preprocesamiento de datos categóricos de alta cardinalidad\n",
    "\n",
    "**Profesora: María Auxiliadora Mora**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3479377a",
   "metadata": {},
   "source": [
    "### Introducción\n",
    "\n",
    "El método de codificación de características categóricas con **baja cardinalidad más conocido es One Hot Encoding**. Este método produce vectores ortogonales y equidistantes para cada categoría. Sin embargo, cuando se trata de características de alta cardinalidad, una codificación One Hot presenta varias deficiencias (Mougan, 2021): \n",
    "\n",
    "- (a) la dimensión del espacio de entrada aumenta con la cardinalidad de la variable codificada, \n",
    "- (b) las características codificadas son dispersas \n",
    "- (c) OneHot Encoding no maneja categorías nuevas.\n",
    "\n",
    "**Otros métodos, más apropiados a alta cardinalidad de las categorías**\n",
    "\n",
    "**Hashing** es un método que permite trabajar con alta cardinalidad de datos categóricos. El método consiste en utilizar funciones hash para producir un número fijo de características. El resultado es mayor velocidad y menor uso de memoria, a expensas de la capacidad de hacer una transformación inversa para recuperar el datos. Además, pueden darse colisiones si se establece un número pequeño de características de salida.\n",
    "\n",
    "Otra opción muy comúnmente utilizada es el **agrupamiento (binning)**, la idea clave es que muchos datos siguen el **principio de Pareto o regla del 80-20 (ley de los pocos vitales)** que establece que  la mayoría de los datos se concentrarán en pocos valores. Un ejemplo simple es la nacionalidad. Hay muchas naciones en el mundo, pero para modelar un fenómeno particular no se usan todas, si no que, se elígen las principales (las más comunes) y se coloca a las demás en la categoría Otros. \n",
    "\n",
    "Alternativamente, se utiliza también la codificación **Target Encoding** (o codificación media) que funciona como una solución eficaz para superar el problema de la alta cardinalidad. En esta codificación, las características categóricas se reemplazan con el valor de repetición promedio de cada categoría respectiva. Con esta técnica se maneja el problema de alta cardinalidad y se ordenan las categorías permitiendo una fácil extracción de la información y simplificación del modelo. El principal inconveniente de Target Encoding aparece cuando las categorías con pocas muestras (incluso solo una) se reemplazan por valores cercanos al objetivo deseado. Esto sesga el modelo y lo hace propenso a sobreajustarse. \n",
    "\n",
    "**Learned Embedding** es un método que proviene del **Deep Learning y el Procesamiento de Lenguaje Natural (NLP)**. El Learned Embedding utiliza vectores densos para realizar la codificación, la idea general es que la distancia entre los vectores densos tendrá significado para el vocabulario codificado. Por ejemplo, la distancia entre \"gato\" y \"perro\" será mucho menor que la distancia entre \"gato\" y \"teclado\".\n",
    "\n",
    "El objetivo de este documento es mostrar el uso de los métodos existentes para codificar datos categóricos y presentar un ejemplo de Learned Embedding. \n",
    "\n",
    "\n",
    "### Los datos usado en los ejemplos:\n",
    "\n",
    "En este ejemplo, se utilizará el conjunto de datos denominado \"Breast Cancer Data Set\" que se ha estudiado ampliamente en el aprendizaje automático desde la década de 1980. Más información en https://archive.ics.uci.edu/ml/datasets/Breast+Cancer\n",
    "\n",
    "El conjunto de datos es administrado por la Universidad de California en Irvine (University of California Irvine) y clasifica los datos de pacientes con cáncer de mama como recurrente o no recurrente del cáncer. Es un problema de clasificación binaria. Hay 286 ejemplos y nueve variables de entrada.  Una precisión de clasificación razonable en este conjunto de datos está entre el 68% y el 73% (Brownlee, 2020).\n",
    "\n",
    "**Información de los atributos (casi todos categóricos)**:\n",
    "\n",
    "1. Class: no-recurrence-events, recurrence-events\n",
    "2. age: 10-19, 20-29, 30-39, 40-49, 50-59, 60-69, 70-79, 80-89, 90-99.\n",
    "3. menopause: lt40, ge40, premeno.\n",
    "4. tumor-size: 0-4, 5-9, 10-14, 15-19, 20-24, 25-29, 30-34, 35-39, 40-44, 45-49, 50-54, 55-59.\n",
    "5. inv-nodes: 0-2, 3-5, 6-8, 9-11, 12-14, 15-17, 18-20, 21-23, 24-26, 27-29, 30-32, 33-35, 36-39.\n",
    "6. node-caps: yes, no.\n",
    "7. deg-malig: 1, 2, 3.\n",
    "8. breast: left, right.\n",
    "9. breast-quad: left-up, left-low, right-up, right-low, central.\n",
    "10. irradiat: yes, no.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b6576ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install keras\n",
    "#!pip install tensorflow\n",
    "#!pip install keras.layers\n",
    "#!pip install pydot\n",
    "#!pip install graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7d78b1a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-19 13:04:42.812877: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-06-19 13:04:42.948285: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-06-19 13:04:42.949059: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-06-19 13:04:43.742281: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=============== formato de los datos ==================\n",
      "         0          1        2      3      4    5        6           7      8  \\\n",
      "0  '40-49'  'premeno'  '15-19'  '0-2'  'yes'  '3'  'right'   'left_up'   'no'   \n",
      "1  '50-59'     'ge40'  '15-19'  '0-2'   'no'  '1'  'right'   'central'   'no'   \n",
      "2  '50-59'     'ge40'  '35-39'  '0-2'   'no'  '2'   'left'  'left_low'   'no'   \n",
      "3  '40-49'  'premeno'  '35-39'  '0-2'  'yes'  '3'  'right'  'left_low'  'yes'   \n",
      "4  '40-49'  'premeno'  '30-34'  '3-5'  'yes'  '2'   'left'  'right_up'   'no'   \n",
      "\n",
      "                        9  \n",
      "0     'recurrence-events'  \n",
      "1  'no-recurrence-events'  \n",
      "2     'recurrence-events'  \n",
      "3  'no-recurrence-events'  \n",
      "4     'recurrence-events'  \n",
      "=============== valores del dataset ==================\n",
      "[[\"'40-49'\" \"'premeno'\" \"'15-19'\" ... \"'left_up'\" \"'no'\"\n",
      "  \"'recurrence-events'\"]\n",
      " [\"'50-59'\" \"'ge40'\" \"'15-19'\" ... \"'central'\" \"'no'\"\n",
      "  \"'no-recurrence-events'\"]\n",
      " [\"'50-59'\" \"'ge40'\" \"'35-39'\" ... \"'left_low'\" \"'no'\"\n",
      "  \"'recurrence-events'\"]\n",
      " ...\n",
      " [\"'30-39'\" \"'premeno'\" \"'30-34'\" ... \"'right_up'\" \"'no'\"\n",
      "  \"'no-recurrence-events'\"]\n",
      " [\"'50-59'\" \"'premeno'\" \"'15-19'\" ... \"'left_low'\" \"'no'\"\n",
      "  \"'no-recurrence-events'\"]\n",
      " [\"'50-59'\" \"'ge40'\" \"'40-44'\" ... \"'right_up'\" \"'no'\"\n",
      "  \"'no-recurrence-events'\"]]\n",
      "=============== Y resultante ==================\n",
      "[\"'recurrence-events'\" \"'no-recurrence-events'\" \"'recurrence-events'\"\n",
      " \"'no-recurrence-events'\" \"'recurrence-events'\" \"'no-recurrence-events'\"\n",
      " \"'no-recurrence-events'\" \"'no-recurrence-events'\"\n",
      " \"'no-recurrence-events'\" \"'no-recurrence-events'\"\n",
      " \"'no-recurrence-events'\" \"'no-recurrence-events'\"\n",
      " \"'no-recurrence-events'\" \"'no-recurrence-events'\" \"'recurrence-events'\"\n",
      " \"'no-recurrence-events'\" \"'no-recurrence-events'\"\n",
      " \"'no-recurrence-events'\" \"'no-recurrence-events'\"\n",
      " \"'no-recurrence-events'\" \"'recurrence-events'\" \"'no-recurrence-events'\"\n",
      " \"'no-recurrence-events'\" \"'no-recurrence-events'\" \"'recurrence-events'\"\n",
      " \"'no-recurrence-events'\" \"'no-recurrence-events'\"\n",
      " \"'no-recurrence-events'\" \"'no-recurrence-events'\" \"'recurrence-events'\"\n",
      " \"'recurrence-events'\" \"'no-recurrence-events'\" \"'no-recurrence-events'\"\n",
      " \"'no-recurrence-events'\" \"'recurrence-events'\" \"'no-recurrence-events'\"\n",
      " \"'no-recurrence-events'\" \"'no-recurrence-events'\"\n",
      " \"'no-recurrence-events'\" \"'no-recurrence-events'\" \"'recurrence-events'\"\n",
      " \"'no-recurrence-events'\" \"'no-recurrence-events'\" \"'recurrence-events'\"\n",
      " \"'no-recurrence-events'\" \"'recurrence-events'\" \"'no-recurrence-events'\"\n",
      " \"'no-recurrence-events'\" \"'no-recurrence-events'\"\n",
      " \"'no-recurrence-events'\" \"'recurrence-events'\" \"'no-recurrence-events'\"\n",
      " \"'no-recurrence-events'\" \"'no-recurrence-events'\"\n",
      " \"'no-recurrence-events'\" \"'no-recurrence-events'\"\n",
      " \"'no-recurrence-events'\" \"'no-recurrence-events'\"\n",
      " \"'no-recurrence-events'\" \"'recurrence-events'\" \"'recurrence-events'\"\n",
      " \"'no-recurrence-events'\" \"'no-recurrence-events'\" \"'recurrence-events'\"\n",
      " \"'recurrence-events'\" \"'no-recurrence-events'\" \"'no-recurrence-events'\"\n",
      " \"'no-recurrence-events'\" \"'recurrence-events'\" \"'no-recurrence-events'\"\n",
      " \"'recurrence-events'\" \"'no-recurrence-events'\" \"'no-recurrence-events'\"\n",
      " \"'recurrence-events'\" \"'recurrence-events'\" \"'no-recurrence-events'\"\n",
      " \"'no-recurrence-events'\" \"'no-recurrence-events'\" \"'recurrence-events'\"\n",
      " \"'no-recurrence-events'\" \"'no-recurrence-events'\"\n",
      " \"'no-recurrence-events'\" \"'no-recurrence-events'\"\n",
      " \"'no-recurrence-events'\" \"'no-recurrence-events'\"\n",
      " \"'no-recurrence-events'\" \"'no-recurrence-events'\"\n",
      " \"'no-recurrence-events'\" \"'no-recurrence-events'\"\n",
      " \"'no-recurrence-events'\" \"'recurrence-events'\" \"'recurrence-events'\"\n",
      " \"'recurrence-events'\" \"'no-recurrence-events'\" \"'no-recurrence-events'\"\n",
      " \"'no-recurrence-events'\" \"'recurrence-events'\" \"'no-recurrence-events'\"\n",
      " \"'recurrence-events'\" \"'recurrence-events'\" \"'recurrence-events'\"\n",
      " \"'recurrence-events'\" \"'no-recurrence-events'\" \"'no-recurrence-events'\"\n",
      " \"'no-recurrence-events'\" \"'recurrence-events'\" \"'recurrence-events'\"\n",
      " \"'recurrence-events'\" \"'no-recurrence-events'\" \"'no-recurrence-events'\"\n",
      " \"'no-recurrence-events'\" \"'no-recurrence-events'\"\n",
      " \"'no-recurrence-events'\" \"'no-recurrence-events'\"\n",
      " \"'no-recurrence-events'\" \"'no-recurrence-events'\" \"'recurrence-events'\"\n",
      " \"'no-recurrence-events'\" \"'no-recurrence-events'\"\n",
      " \"'no-recurrence-events'\" \"'recurrence-events'\" \"'no-recurrence-events'\"\n",
      " \"'no-recurrence-events'\" \"'recurrence-events'\" \"'no-recurrence-events'\"\n",
      " \"'no-recurrence-events'\" \"'no-recurrence-events'\"\n",
      " \"'no-recurrence-events'\" \"'recurrence-events'\" \"'no-recurrence-events'\"\n",
      " \"'no-recurrence-events'\" \"'no-recurrence-events'\" \"'recurrence-events'\"\n",
      " \"'recurrence-events'\" \"'no-recurrence-events'\" \"'recurrence-events'\"\n",
      " \"'no-recurrence-events'\" \"'no-recurrence-events'\" \"'recurrence-events'\"\n",
      " \"'no-recurrence-events'\" \"'recurrence-events'\" \"'no-recurrence-events'\"\n",
      " \"'recurrence-events'\" \"'no-recurrence-events'\" \"'no-recurrence-events'\"\n",
      " \"'no-recurrence-events'\" \"'recurrence-events'\" \"'no-recurrence-events'\"\n",
      " \"'no-recurrence-events'\" \"'no-recurrence-events'\" \"'recurrence-events'\"\n",
      " \"'no-recurrence-events'\" \"'no-recurrence-events'\" \"'recurrence-events'\"\n",
      " \"'no-recurrence-events'\" \"'no-recurrence-events'\" \"'recurrence-events'\"\n",
      " \"'recurrence-events'\" \"'no-recurrence-events'\" \"'no-recurrence-events'\"\n",
      " \"'no-recurrence-events'\" \"'no-recurrence-events'\"\n",
      " \"'no-recurrence-events'\" \"'no-recurrence-events'\" \"'recurrence-events'\"\n",
      " \"'recurrence-events'\" \"'recurrence-events'\" \"'no-recurrence-events'\"\n",
      " \"'no-recurrence-events'\" \"'no-recurrence-events'\" \"'recurrence-events'\"\n",
      " \"'recurrence-events'\" \"'no-recurrence-events'\" \"'recurrence-events'\"\n",
      " \"'no-recurrence-events'\" \"'no-recurrence-events'\"\n",
      " \"'no-recurrence-events'\" \"'recurrence-events'\" \"'recurrence-events'\"\n",
      " \"'no-recurrence-events'\" \"'no-recurrence-events'\" \"'recurrence-events'\"\n",
      " \"'no-recurrence-events'\" \"'recurrence-events'\" \"'no-recurrence-events'\"\n",
      " \"'no-recurrence-events'\" \"'no-recurrence-events'\" \"'recurrence-events'\"\n",
      " \"'no-recurrence-events'\" \"'no-recurrence-events'\" \"'recurrence-events'\"\n",
      " \"'no-recurrence-events'\" \"'recurrence-events'\" \"'no-recurrence-events'\"\n",
      " \"'no-recurrence-events'\" \"'no-recurrence-events'\" \"'recurrence-events'\"\n",
      " \"'no-recurrence-events'\" \"'no-recurrence-events'\" \"'recurrence-events'\"\n",
      " \"'no-recurrence-events'\" \"'no-recurrence-events'\"\n",
      " \"'no-recurrence-events'\" \"'no-recurrence-events'\"\n",
      " \"'no-recurrence-events'\" \"'recurrence-events'\" \"'no-recurrence-events'\"\n",
      " \"'recurrence-events'\" \"'recurrence-events'\" \"'no-recurrence-events'\"\n",
      " \"'recurrence-events'\" \"'recurrence-events'\" \"'no-recurrence-events'\"\n",
      " \"'no-recurrence-events'\" \"'no-recurrence-events'\"\n",
      " \"'no-recurrence-events'\" \"'recurrence-events'\" \"'no-recurrence-events'\"\n",
      " \"'recurrence-events'\" \"'no-recurrence-events'\" \"'no-recurrence-events'\"\n",
      " \"'no-recurrence-events'\" \"'no-recurrence-events'\"\n",
      " \"'no-recurrence-events'\" \"'recurrence-events'\" \"'no-recurrence-events'\"\n",
      " \"'no-recurrence-events'\" \"'no-recurrence-events'\"\n",
      " \"'no-recurrence-events'\" \"'recurrence-events'\" \"'no-recurrence-events'\"\n",
      " \"'no-recurrence-events'\" \"'no-recurrence-events'\"\n",
      " \"'no-recurrence-events'\" \"'no-recurrence-events'\" \"'recurrence-events'\"\n",
      " \"'no-recurrence-events'\" \"'recurrence-events'\" \"'recurrence-events'\"\n",
      " \"'recurrence-events'\" \"'recurrence-events'\" \"'no-recurrence-events'\"\n",
      " \"'no-recurrence-events'\" \"'no-recurrence-events'\"\n",
      " \"'no-recurrence-events'\" \"'no-recurrence-events'\"\n",
      " \"'no-recurrence-events'\" \"'no-recurrence-events'\"\n",
      " \"'no-recurrence-events'\" \"'no-recurrence-events'\" \"'recurrence-events'\"\n",
      " \"'no-recurrence-events'\" \"'no-recurrence-events'\"\n",
      " \"'no-recurrence-events'\" \"'no-recurrence-events'\" \"'recurrence-events'\"\n",
      " \"'no-recurrence-events'\" \"'recurrence-events'\" \"'no-recurrence-events'\"\n",
      " \"'recurrence-events'\" \"'no-recurrence-events'\" \"'no-recurrence-events'\"\n",
      " \"'no-recurrence-events'\" \"'recurrence-events'\" \"'no-recurrence-events'\"\n",
      " \"'no-recurrence-events'\" \"'no-recurrence-events'\" \"'recurrence-events'\"\n",
      " \"'no-recurrence-events'\" \"'no-recurrence-events'\" \"'recurrence-events'\"\n",
      " \"'recurrence-events'\" \"'no-recurrence-events'\" \"'no-recurrence-events'\"\n",
      " \"'no-recurrence-events'\" \"'no-recurrence-events'\"\n",
      " \"'no-recurrence-events'\" \"'no-recurrence-events'\"\n",
      " \"'no-recurrence-events'\" \"'no-recurrence-events'\"\n",
      " \"'no-recurrence-events'\" \"'no-recurrence-events'\"\n",
      " \"'no-recurrence-events'\" \"'no-recurrence-events'\"\n",
      " \"'no-recurrence-events'\" \"'no-recurrence-events'\"]\n"
     ]
    }
   ],
   "source": [
    "# example of learned embedding encoding for a neural network\n",
    "from numpy import unique\n",
    "from pandas import read_csv\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# keras y tensorflow\n",
    "import tensorflow as tf\n",
    "from keras.models import Model\n",
    "from keras.layers import Input\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import concatenate\n",
    "from keras.utils import plot_model\n",
    "\n",
    "# torch \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "\n",
    "# load the dataset\n",
    "def load_dataset(filename):\n",
    "    # load the dataset as a pandas DataFrame\n",
    "    data = read_csv(filename, header=None)\n",
    "    print(\"=============== formato de los datos ==================\")\n",
    "    print(data.head(5))\n",
    "    \n",
    "    # retrieve numpy array\n",
    "    dataset = data.values\n",
    "\n",
    "    print(\"=============== valores del dataset ==================\")\n",
    "    print(dataset)\n",
    "\n",
    "    # split into input (X) and output (y) variables\n",
    "    X = dataset[:, :-1]\n",
    "    y = dataset[:,-1]\n",
    "    \n",
    "    # format all fields as string\n",
    "    X = X.astype(str)\n",
    "    \n",
    "    print(\"=============== Y resultante ==================\")\n",
    "    print(y)\n",
    "    \n",
    "    # reshape target to be a 2d array\n",
    "    y = y.reshape((len(y), 1))\n",
    "    return X, y\n",
    "\n",
    "# prepare input data\n",
    "def prepare_inputs(X_train, X_test):\n",
    "    X_train_enc, X_test_enc = list(), list()\n",
    "    # label encode each column\n",
    "    for i in range(X_train.shape[1]):\n",
    "        le = LabelEncoder()\n",
    "        le.fit(X_train[:, i])\n",
    "        # encode\n",
    "        train_enc = le.transform(X_train[:, i])\n",
    "        test_enc = le.transform(X_test[:, i])\n",
    "        # store\n",
    "        X_train_enc.append(train_enc)\n",
    "        X_test_enc.append(test_enc)\n",
    "    return X_train_enc, X_test_enc\n",
    "\n",
    "# prepare target\n",
    "def prepare_targets(y_train, y_test):\n",
    "    le = LabelEncoder()\n",
    "    le.fit(y_train)\n",
    "    y_train_enc = le.transform(y_train)\n",
    "    y_test_enc = le.transform(y_test)\n",
    "    return y_train_enc, y_test_enc\n",
    "\n",
    "# load the dataset\n",
    "X, y = load_dataset('../../Data/breast-cancer.csv')\n",
    "# split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=1)\n",
    "# prepare input data\n",
    "X_train_enc, X_test_enc = prepare_inputs(X_train, X_test)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8f61762c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=============== Cantidad de datos de entrenamiento y pruebas ==================\n",
      "Train [array([3, 1, 3, 1, 2, 2, 3, 3, 3, 1, 2, 2, 2, 2, 4, 2, 1, 4, 1, 4, 4, 3,\n",
      "       2, 3, 2, 4, 3, 2, 4, 2, 1, 2, 1, 4, 3, 1, 5, 3, 2, 2, 3, 3, 3, 2,\n",
      "       4, 3, 1, 4, 4, 5, 3, 2, 3, 3, 4, 2, 2, 3, 2, 2, 2, 4, 1, 2, 3, 3,\n",
      "       2, 2, 3, 3, 4, 2, 2, 3, 2, 1, 2, 1, 4, 2, 3, 1, 3, 3, 3, 4, 2, 4,\n",
      "       3, 2, 3, 3, 3, 3, 5, 4, 3, 2, 3, 2, 3, 2, 1, 3, 3, 3, 3, 4, 3, 1,\n",
      "       3, 4, 4, 1, 3, 2, 3, 4, 3, 3, 4, 4, 3, 3, 4, 2, 3, 2, 4, 1, 2, 3,\n",
      "       2, 1, 2, 1, 4, 4, 4, 3, 3, 2, 1, 2, 2, 3, 2, 3, 2, 3, 3, 3, 4, 1,\n",
      "       1, 2, 3, 3, 3, 2, 0, 2, 4, 3, 4, 2, 4, 2, 3, 3, 2, 2, 2, 5, 5, 3,\n",
      "       5, 1, 2, 2, 2, 2, 4, 4, 3, 2, 2, 3, 4, 4, 4]), array([0, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 0, 2, 0, 0, 2,\n",
      "       0, 2, 2, 0, 0, 2, 1, 2, 2, 2, 2, 0, 2, 2, 0, 0, 2, 2, 2, 2, 2, 2,\n",
      "       0, 0, 2, 0, 0, 0, 2, 2, 0, 2, 0, 2, 2, 0, 2, 2, 2, 0, 2, 2, 2, 0,\n",
      "       2, 2, 0, 0, 0, 2, 2, 0, 2, 2, 2, 2, 0, 2, 0, 2, 2, 2, 0, 0, 2, 0,\n",
      "       1, 2, 0, 0, 2, 0, 0, 0, 0, 2, 0, 2, 0, 2, 2, 0, 0, 0, 0, 0, 2, 2,\n",
      "       1, 0, 0, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 1, 2, 0, 2, 0, 2, 0, 0,\n",
      "       2, 2, 2, 2, 0, 0, 0, 0, 2, 0, 2, 0, 2, 0, 2, 1, 2, 0, 2, 0, 0, 2,\n",
      "       2, 2, 0, 0, 0, 2, 2, 2, 0, 2, 0, 2, 0, 2, 0, 0, 2, 2, 2, 0, 0, 2,\n",
      "       0, 2, 2, 2, 2, 2, 0, 0, 0, 2, 2, 2, 0, 0, 0]), array([ 4,  9, 10,  4,  3,  1,  3,  1,  4,  2,  5,  5,  2,  2,  5,  3,  4,\n",
      "        2,  5,  3,  5,  1,  5,  6,  4,  4,  6,  6,  5,  3,  5,  4,  6,  7,\n",
      "        2,  1,  7,  1,  9,  5,  5,  5,  3,  5,  1,  3,  0,  4,  9,  3,  2,\n",
      "        6,  4,  1,  2,  4,  4,  7,  5,  3,  7,  5,  7,  5,  1,  5,  5,  3,\n",
      "        7,  3,  3,  5,  7,  7,  5,  3,  4,  1,  5,  7,  4,  7,  4,  5,  9,\n",
      "        3,  1,  4,  3,  1,  5,  1,  2,  0,  0,  2,  6,  5,  7,  2,  5,  4,\n",
      "        7,  2,  5,  2,  4,  1,  4,  4,  3,  3,  3,  4,  3,  3,  4, 10,  4,\n",
      "        0,  4,  2,  1,  5,  1,  3,  5,  1,  5,  3,  7,  5,  3,  4,  5,  2,\n",
      "        3,  4,  5,  3,  4,  3,  3,  4,  6,  3,  3,  2,  0,  1,  2,  3,  5,\n",
      "        2,  5,  6,  6,  7,  5,  2,  6,  6,  5,  4,  1,  3,  4,  4,  2,  2,\n",
      "        1,  4,  7,  1,  7, 10,  2,  5,  5,  4,  4,  5,  4,  2,  2,  4,  4,\n",
      "        4,  1,  7,  8]), array([0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 2, 2, 0, 0, 0, 0, 0, 6, 4, 4, 4,\n",
      "       4, 0, 0, 0, 0, 0, 0, 5, 4, 0, 0, 4, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 6, 0, 0, 0, 0, 0, 0, 5, 4, 4, 0, 0, 0, 0, 0,\n",
      "       0, 0, 5, 0, 3, 0, 0, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 6, 0, 4, 0, 4, 0, 4, 0, 0, 5,\n",
      "       0, 0, 0, 5, 4, 4, 0, 0, 0, 0, 4, 0, 0, 5, 0, 4, 0, 0, 0, 4, 2, 6,\n",
      "       0, 4, 0, 0, 0, 0, 0, 4, 0, 0, 0, 1, 6, 0, 0, 0, 0, 0, 0, 4, 0, 5,\n",
      "       0, 0, 2, 0, 6, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 6,\n",
      "       6, 0, 0, 0, 0, 1, 4, 0, 0, 0, 0, 0, 0, 0, 5]), array([0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0,\n",
      "       0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
      "       0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 2, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
      "       0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       2, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1,\n",
      "       2, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 2,\n",
      "       0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1,\n",
      "       0, 1, 0, 0, 2, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
      "       2, 0, 0, 0, 0, 1, 2, 0, 1, 0, 0, 0, 0, 0, 1]), array([0, 1, 1, 0, 1, 1, 2, 1, 1, 0, 2, 2, 2, 2, 2, 0, 1, 1, 1, 1, 2, 0,\n",
      "       2, 1, 2, 1, 1, 2, 0, 1, 2, 2, 2, 1, 1, 0, 0, 0, 0, 2, 2, 0, 0, 1,\n",
      "       1, 1, 1, 2, 0, 2, 1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 2, 2, 1, 1, 0, 2,\n",
      "       0, 1, 2, 1, 2, 2, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 2, 1, 1, 1, 2,\n",
      "       0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 2, 1, 2, 1, 2, 0, 2, 1, 2, 1, 1, 2,\n",
      "       0, 2, 1, 1, 1, 1, 1, 1, 2, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 2,\n",
      "       0, 2, 0, 0, 0, 2, 1, 1, 0, 1, 2, 2, 1, 1, 1, 1, 2, 1, 0, 2, 0, 2,\n",
      "       0, 2, 2, 2, 2, 2, 1, 1, 2, 1, 0, 2, 2, 0, 0, 1, 1, 1, 0, 1, 0, 1,\n",
      "       0, 1, 0, 2, 1, 2, 0, 1, 1, 1, 0, 2, 0, 1, 2]), array([0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1,\n",
      "       0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1,\n",
      "       0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0,\n",
      "       1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0,\n",
      "       0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0,\n",
      "       0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0,\n",
      "       0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0,\n",
      "       1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1,\n",
      "       0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0]), array([3, 3, 2, 0, 4, 1, 2, 2, 1, 1, 4, 1, 1, 2, 1, 1, 1, 1, 2, 1, 1, 2,\n",
      "       1, 2, 4, 1, 2, 2, 1, 1, 2, 2, 1, 2, 1, 1, 4, 2, 1, 2, 2, 0, 1, 4,\n",
      "       1, 2, 0, 2, 0, 2, 3, 2, 1, 1, 2, 2, 3, 1, 2, 2, 2, 2, 4, 3, 1, 5,\n",
      "       2, 2, 1, 2, 1, 2, 1, 1, 1, 3, 1, 3, 4, 1, 2, 2, 1, 4, 4, 2, 2, 3,\n",
      "       1, 1, 1, 1, 1, 0, 3, 1, 1, 3, 2, 2, 3, 0, 4, 0, 1, 4, 2, 2, 4, 3,\n",
      "       2, 1, 2, 2, 1, 1, 1, 1, 4, 0, 4, 2, 1, 3, 4, 1, 1, 2, 2, 0, 2, 1,\n",
      "       3, 1, 1, 1, 1, 1, 1, 2, 1, 2, 0, 3, 4, 2, 3, 1, 0, 1, 1, 4, 2, 1,\n",
      "       2, 1, 1, 2, 2, 3, 4, 4, 0, 2, 2, 1, 2, 1, 0, 1, 2, 2, 2, 0, 2, 2,\n",
      "       1, 2, 4, 2, 1, 2, 2, 1, 0, 1, 3, 1, 1, 1, 0]), array([0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0,\n",
      "       0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1,\n",
      "       0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0,\n",
      "       0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1,\n",
      "       0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1,\n",
      "       0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
      "       0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0])]\n",
      "Test (95, 9) (95, 1)\n",
      "[[\"'50-59'\" \"'premeno'\" \"'25-29'\" \"'3-5'\" \"'yes'\" \"'3'\" \"'left'\"\n",
      "  \"'left_low'\" \"'yes'\"]\n",
      " [\"'50-59'\" \"'premeno'\" \"'30-34'\" \"'3-5'\" \"'yes'\" \"'2'\" \"'left'\"\n",
      "  \"'left_low'\" \"'yes'\"]\n",
      " [\"'50-59'\" \"'premeno'\" \"'40-44'\" \"'0-2'\" \"'no'\" \"'2'\" \"'left'\"\n",
      "  \"'left_up'\" \"'no'\"]\n",
      " [\"'40-49'\" \"'premeno'\" \"'20-24'\" \"'3-5'\" \"'yes'\" \"'2'\" \"'right'\"\n",
      "  \"'right_up'\" \"'yes'\"]\n",
      " [\"'30-39'\" \"'premeno'\" \"'0-4'\" \"'0-2'\" \"'no'\" \"'2'\" \"'right'\"\n",
      "  \"'central'\" \"'no'\"]\n",
      " [\"'30-39'\" \"'premeno'\" \"'35-39'\" \"'0-2'\" \"'no'\" \"'3'\" \"'left'\"\n",
      "  \"'left_low'\" \"'no'\"]\n",
      " [\"'40-49'\" \"'premeno'\" \"'30-34'\" \"'3-5'\" \"'no'\" \"'2'\" \"'right'\"\n",
      "  \"'left_up'\" \"'no'\"]\n",
      " [\"'30-39'\" \"'premeno'\" \"'20-24'\" \"'3-5'\" \"'yes'\" \"'2'\" \"'right'\"\n",
      "  \"'left_up'\" \"'yes'\"]\n",
      " [\"'60-69'\" \"'ge40'\" \"'15-19'\" \"'0-2'\" \"'no'\" \"'2'\" \"'right'\" \"'left_up'\"\n",
      "  \"'no'\"]\n",
      " [\"'50-59'\" \"'ge40'\" \"'30-34'\" \"'3-5'\" \"'no'\" \"'3'\" \"'right'\" \"'left_up'\"\n",
      "  \"'no'\"]\n",
      " [\"'50-59'\" \"'ge40'\" \"'20-24'\" \"'0-2'\" \"'no'\" \"'3'\" \"'left'\" \"'left_up'\"\n",
      "  \"'no'\"]\n",
      " [\"'40-49'\" \"'premeno'\" \"'25-29'\" \"'9-11'\" \"'yes'\" \"'3'\" \"'right'\"\n",
      "  \"'left_up'\" \"'no'\"]\n",
      " [\"'40-49'\" \"'premeno'\" \"'10-14'\" \"'0-2'\" \"'no'\" \"'1'\" \"'right'\"\n",
      "  \"'left_up'\" \"'no'\"]\n",
      " [\"'40-49'\" \"'premeno'\" \"'30-34'\" \"'0-2'\" \"'yes'\" \"'3'\" \"'right'\"\n",
      "  \"'right_up'\" \"'no'\"]\n",
      " [\"'50-59'\" \"'ge40'\" \"'30-34'\" \"'0-2'\" \"'no'\" \"'1'\" \"'right'\" \"'central'\"\n",
      "  \"'no'\"]\n",
      " [\"'40-49'\" \"'premeno'\" \"'20-24'\" \"'0-2'\" \"'no'\" \"'2'\" \"'left'\"\n",
      "  \"'left_low'\" \"'no'\"]\n",
      " [\"'50-59'\" \"'ge40'\" \"'30-34'\" \"'0-2'\" \"'no'\" \"'3'\" \"'right'\"\n",
      "  \"'left_low'\" \"'no'\"]\n",
      " [\"'60-69'\" \"'ge40'\" \"'15-19'\" \"'0-2'\" \"'no'\" \"'1'\" \"'right'\" \"'left_up'\"\n",
      "  \"'no'\"]\n",
      " [\"'30-39'\" \"'premeno'\" \"'30-34'\" \"'6-8'\" \"'yes'\" \"'2'\" \"'right'\"\n",
      "  \"'right_up'\" \"'no'\"]\n",
      " [\"'30-39'\" \"'premeno'\" \"'20-24'\" \"'0-2'\" \"'no'\" \"'3'\" \"'left'\"\n",
      "  \"'left_up'\" \"'yes'\"]\n",
      " [\"'40-49'\" \"'premeno'\" \"'25-29'\" \"'0-2'\" \"'no'\" \"'2'\" \"'right'\"\n",
      "  \"'left_up'\" \"'no'\"]\n",
      " [\"'50-59'\" \"'premeno'\" \"'25-29'\" \"'0-2'\" \"'no'\" \"'2'\" \"'right'\"\n",
      "  \"'right_low'\" \"'no'\"]\n",
      " [\"'30-39'\" \"'premeno'\" \"'35-39'\" \"'9-11'\" \"'yes'\" \"'3'\" \"'left'\"\n",
      "  \"'left_low'\" \"'no'\"]\n",
      " [\"'60-69'\" \"'ge40'\" \"'10-14'\" \"'6-8'\" \"'yes'\" \"'3'\" \"'left'\" \"'left_up'\"\n",
      "  \"'yes'\"]\n",
      " [\"'50-59'\" \"'ge40'\" \"'0-4'\" \"'0-2'\" \"'no'\" \"'1'\" \"'left'\" \"'left_low'\"\n",
      "  \"'no'\"]\n",
      " [\"'50-59'\" \"'ge40'\" \"'25-29'\" \"'6-8'\" \"'no'\" \"'3'\" \"'left'\" \"'left_low'\"\n",
      "  \"'yes'\"]\n",
      " [\"'50-59'\" \"'premeno'\" \"'25-29'\" \"'0-2'\" \"'no'\" \"'2'\" \"'left'\"\n",
      "  \"'right_up'\" \"'no'\"]\n",
      " [\"'60-69'\" \"'ge40'\" \"'30-34'\" \"'0-2'\" \"'no'\" \"'1'\" \"'right'\" \"'left_up'\"\n",
      "  \"'no'\"]\n",
      " [\"'60-69'\" \"'ge40'\" \"'30-34'\" \"'0-2'\" \"'no'\" \"'2'\" \"'left'\" \"'left_low'\"\n",
      "  \"'no'\"]\n",
      " [\"'30-39'\" \"'premeno'\" \"'30-34'\" \"'0-2'\" \"'no'\" \"'3'\" \"'left'\"\n",
      "  \"'left_low'\" \"'no'\"]\n",
      " [\"'40-49'\" \"'premeno'\" \"'20-24'\" \"'0-2'\" \"'no'\" \"'2'\" \"'right'\"\n",
      "  \"'left_up'\" \"'no'\"]\n",
      " [\"'40-49'\" \"'ge40'\" \"'20-24'\" \"'0-2'\" \"'no'\" \"'3'\" \"'left'\" \"'left_up'\"\n",
      "  \"'no'\"]\n",
      " [\"'50-59'\" \"'ge40'\" \"'35-39'\" \"'0-2'\" \"'no'\" \"'3'\" \"'left'\" \"'left_up'\"\n",
      "  \"'no'\"]\n",
      " [\"'50-59'\" \"'ge40'\" \"'20-24'\" \"'0-2'\" \"'no'\" \"'2'\" \"'right'\" \"'central'\"\n",
      "  \"'no'\"]\n",
      " [\"'60-69'\" \"'ge40'\" \"'50-54'\" \"'0-2'\" \"'no'\" \"'2'\" \"'right'\" \"'left_up'\"\n",
      "  \"'yes'\"]\n",
      " [\"'40-49'\" \"'premeno'\" \"'20-24'\" \"'0-2'\" \"'no'\" \"'2'\" \"'right'\"\n",
      "  \"'left_up'\" \"'no'\"]\n",
      " [\"'50-59'\" \"'lt40'\" \"'30-34'\" \"'0-2'\" \"'no'\" \"'3'\" \"'right'\" \"'left_up'\"\n",
      "  \"'no'\"]\n",
      " [\"'40-49'\" \"'premeno'\" \"'20-24'\" \"'0-2'\" \"'no'\" \"'2'\" \"'left'\"\n",
      "  \"'central'\" \"'no'\"]\n",
      " [\"'60-69'\" \"'ge40'\" \"'20-24'\" \"'0-2'\" \"'no'\" \"'1'\" \"'right'\" \"'left_up'\"\n",
      "  \"'no'\"]\n",
      " [\"'60-69'\" \"'ge40'\" \"'35-39'\" \"'6-8'\" \"'yes'\" \"'3'\" \"'left'\"\n",
      "  \"'left_low'\" \"'no'\"]\n",
      " [\"'40-49'\" \"'ge40'\" \"'30-34'\" \"'0-2'\" \"'no'\" \"'2'\" \"'left'\" \"'left_up'\"\n",
      "  \"'yes'\"]\n",
      " [\"'50-59'\" \"'ge40'\" \"'50-54'\" \"'0-2'\" \"'no'\" \"'1'\" \"'right'\"\n",
      "  \"'right_up'\" \"'no'\"]\n",
      " [\"'60-69'\" \"'ge40'\" \"'20-24'\" \"'0-2'\" \"'no'\" \"'1'\" \"'left'\" \"'left_low'\"\n",
      "  \"'no'\"]\n",
      " [\"'50-59'\" \"'ge40'\" \"'30-34'\" \"'0-2'\" \"'no'\" \"'1'\" \"'left'\" \"'left_up'\"\n",
      "  \"'no'\"]\n",
      " [\"'50-59'\" \"'ge40'\" \"'30-34'\" \"'0-2'\" \"'no'\" \"'1'\" \"'right'\"\n",
      "  \"'right_up'\" \"'no'\"]\n",
      " [\"'40-49'\" \"'premeno'\" \"'30-34'\" \"'3-5'\" \"'yes'\" \"'2'\" \"'left'\"\n",
      "  \"'right_up'\" \"'no'\"]\n",
      " [\"'60-69'\" \"'ge40'\" \"'15-19'\" \"'0-2'\" \"'no'\" \"'2'\" \"'right'\" \"'left_up'\"\n",
      "  \"'no'\"]\n",
      " [\"'60-69'\" \"'ge40'\" \"'50-54'\" \"'0-2'\" \"'no'\" \"'3'\" \"'right'\" \"'left_up'\"\n",
      "  \"'no'\"]\n",
      " [\"'30-39'\" \"'premeno'\" \"'30-34'\" \"'0-2'\" \"'no'\" \"'2'\" \"'left'\"\n",
      "  \"'left_up'\" \"'no'\"]\n",
      " [\"'30-39'\" \"'premeno'\" \"'40-44'\" \"'0-2'\" \"'no'\" \"'2'\" \"'left'\"\n",
      "  \"'left_low'\" \"'yes'\"]\n",
      " [\"'50-59'\" \"'ge40'\" \"'25-29'\" \"'0-2'\" \"'no'\" \"'1'\" \"'left'\" \"'left_low'\"\n",
      "  \"'no'\"]\n",
      " [\"'60-69'\" \"'ge40'\" \"'45-49'\" \"'0-2'\" \"'no'\" \"'1'\" \"'right'\"\n",
      "  \"'right_up'\" \"'yes'\"]\n",
      " [\"'60-69'\" \"'ge40'\" \"'25-29'\" \"'0-2'\" \"'no'\" \"'2'\" \"'right'\"\n",
      "  \"'left_low'\" \"'no'\"]\n",
      " [\"'50-59'\" \"'premeno'\" \"'25-29'\" \"'0-2'\" \"'no'\" \"'1'\" \"'right'\"\n",
      "  \"'left_up'\" \"'no'\"]\n",
      " [\"'50-59'\" \"'premeno'\" \"'25-29'\" \"'0-2'\" \"'no'\" \"'1'\" \"'right'\"\n",
      "  \"'left_up'\" \"'no'\"]\n",
      " [\"'40-49'\" \"'ge40'\" \"'25-29'\" \"'0-2'\" \"'no'\" \"'2'\" \"'left'\" \"'left_low'\"\n",
      "  \"'no'\"]\n",
      " [\"'60-69'\" \"'ge40'\" \"'15-19'\" \"'0-2'\" \"'no'\" \"'1'\" \"'left'\"\n",
      "  \"'right_low'\" \"'no'\"]\n",
      " [\"'50-59'\" \"'premeno'\" \"'25-29'\" \"'3-5'\" \"'no'\" \"'2'\" \"'right'\"\n",
      "  \"'left_up'\" \"'yes'\"]\n",
      " [\"'40-49'\" \"'premeno'\" \"'10-14'\" \"'0-2'\" \"'no'\" \"'1'\" \"'right'\"\n",
      "  \"'right_low'\" \"'no'\"]\n",
      " [\"'50-59'\" \"'ge40'\" \"'10-14'\" \"'0-2'\" \"'no'\" \"'1'\" \"'right'\" \"'left_up'\"\n",
      "  \"'no'\"]\n",
      " [\"'40-49'\" \"'ge40'\" \"'20-24'\" \"'0-2'\" \"'no'\" \"'3'\" \"'left'\" \"'left_low'\"\n",
      "  \"'no'\"]\n",
      " [\"'60-69'\" \"'ge40'\" \"'30-34'\" \"'3-5'\" \"'yes'\" \"'2'\" \"'left'\" \"'central'\"\n",
      "  \"'yes'\"]\n",
      " [\"'50-59'\" \"'ge40'\" \"'40-44'\" \"'0-2'\" \"'no'\" \"'3'\" \"'left'\" \"'right_up'\"\n",
      "  \"'no'\"]\n",
      " [\"'50-59'\" \"'ge40'\" \"'35-39'\" \"'0-2'\" \"'no'\" \"'3'\" \"'left'\" \"'left_low'\"\n",
      "  \"'no'\"]\n",
      " [\"'60-69'\" \"'ge40'\" \"'40-44'\" \"'3-5'\" \"'yes'\" \"'3'\" \"'right'\"\n",
      "  \"'left_low'\" \"'no'\"]\n",
      " [\"'50-59'\" \"'ge40'\" \"'30-34'\" \"'6-8'\" \"'yes'\" \"'3'\" \"'left'\"\n",
      "  \"'right_low'\" \"'no'\"]\n",
      " [\"'60-69'\" \"'ge40'\" \"'10-14'\" \"'0-2'\" \"'no'\" \"'1'\" \"'left'\" \"'left_up'\"\n",
      "  \"'no'\"]\n",
      " [\"'40-49'\" \"'premeno'\" \"'30-34'\" \"'0-2'\" \"'no'\" \"'1'\" \"'left'\"\n",
      "  \"'left_low'\" \"'yes'\"]\n",
      " [\"'50-59'\" \"'premeno'\" \"'30-34'\" \"'0-2'\" \"'no'\" \"'3'\" \"'left'\"\n",
      "  \"'left_low'\" \"'no'\"]\n",
      " [\"'40-49'\" \"'premeno'\" \"'15-19'\" \"'0-2'\" \"'no'\" \"'2'\" \"'left'\"\n",
      "  \"'left_low'\" \"'no'\"]\n",
      " [\"'30-39'\" \"'premeno'\" \"'20-24'\" \"'3-5'\" \"'yes'\" \"'2'\" \"'left'\"\n",
      "  \"'left_low'\" \"'no'\"]\n",
      " [\"'40-49'\" \"'premeno'\" \"'20-24'\" \"'0-2'\" \"'no'\" \"'1'\" \"'right'\"\n",
      "  \"'right_up'\" \"'no'\"]\n",
      " [\"'40-49'\" \"'premeno'\" \"'20-24'\" \"'0-2'\" \"'no'\" \"'2'\" \"'left'\"\n",
      "  \"'left_low'\" \"'no'\"]\n",
      " [\"'50-59'\" \"'ge40'\" \"'20-24'\" \"'0-2'\" \"'no'\" \"'3'\" \"'left'\" \"'left_up'\"\n",
      "  \"'no'\"]\n",
      " [\"'40-49'\" \"'premeno'\" \"'25-29'\" \"'0-2'\" \"'no'\" \"'2'\" \"'left'\"\n",
      "  \"'left_low'\" \"'yes'\"]\n",
      " [\"'40-49'\" \"'premeno'\" \"'35-39'\" \"'0-2'\" \"'no'\" \"'1'\" \"'left'\"\n",
      "  \"'left_low'\" \"'no'\"]\n",
      " [\"'40-49'\" \"'premeno'\" \"'50-54'\" \"'0-2'\" \"'no'\" \"'2'\" \"'right'\"\n",
      "  \"'left_low'\" \"'yes'\"]\n",
      " [\"'30-39'\" \"'premeno'\" \"'15-19'\" \"'0-2'\" \"'no'\" \"'1'\" \"'left'\"\n",
      "  \"'left_low'\" \"'no'\"]\n",
      " [\"'60-69'\" \"'ge40'\" \"'10-14'\" \"'0-2'\" \"'no'\" \"'1'\" \"'left'\" \"'left_low'\"\n",
      "  \"'no'\"]\n",
      " [\"'40-49'\" \"'premeno'\" \"'30-34'\" \"'0-2'\" \"'no'\" \"'3'\" \"'right'\"\n",
      "  \"'right_up'\" \"'no'\"]\n",
      " [\"'30-39'\" \"'lt40'\" \"'15-19'\" \"'0-2'\" \"'no'\" \"'3'\" \"'right'\" \"'left_up'\"\n",
      "  \"'no'\"]\n",
      " [\"'50-59'\" \"'ge40'\" \"'40-44'\" \"'6-8'\" \"'yes'\" \"'3'\" \"'left'\"\n",
      "  \"'left_low'\" \"'yes'\"]\n",
      " [\"'60-69'\" \"'ge40'\" \"'30-34'\" \"'6-8'\" \"'yes'\" \"'2'\" \"'right'\"\n",
      "  \"'right_up'\" \"'no'\"]\n",
      " [\"'50-59'\" \"'premeno'\" \"'35-39'\" \"'15-17'\" \"'yes'\" \"'3'\" \"'right'\"\n",
      "  \"'right_up'\" \"'no'\"]\n",
      " [\"'40-49'\" \"'premeno'\" \"'50-54'\" \"'0-2'\" \"'no'\" \"'2'\" \"'left'\"\n",
      "  \"'left_low'\" \"'no'\"]\n",
      " [\"'40-49'\" \"'premeno'\" \"'35-39'\" \"'0-2'\" \"'no'\" \"'1'\" \"'right'\"\n",
      "  \"'left_up'\" \"'no'\"]\n",
      " [\"'50-59'\" \"'ge40'\" \"'20-24'\" \"'0-2'\" \"'no'\" \"'1'\" \"'right'\"\n",
      "  \"'left_low'\" \"'no'\"]\n",
      " [\"'40-49'\" \"'premeno'\" \"'45-49'\" \"'0-2'\" \"'no'\" \"'2'\" \"'left'\"\n",
      "  \"'left_low'\" \"'yes'\"]\n",
      " [\"'40-49'\" \"'premeno'\" \"'0-4'\" \"'0-2'\" \"'no'\" \"'2'\" \"'right'\"\n",
      "  \"'right_low'\" \"'no'\"]\n",
      " [\"'40-49'\" \"'premeno'\" \"'25-29'\" \"'0-2'\" \"'no'\" \"'1'\" \"'left'\"\n",
      "  \"'right_low'\" \"'no'\"]\n",
      " [\"'40-49'\" \"'ge40'\" \"'20-24'\" \"'3-5'\" \"'no'\" \"'3'\" \"'right'\"\n",
      "  \"'left_low'\" \"'yes'\"]\n",
      " [\"'60-69'\" \"'ge40'\" \"'25-29'\" \"'3-5'\" 'nan' \"'1'\" \"'right'\" \"'left_low'\"\n",
      "  \"'yes'\"]\n",
      " [\"'30-39'\" \"'premeno'\" \"'25-29'\" \"'0-2'\" \"'no'\" \"'2'\" \"'left'\"\n",
      "  \"'left_low'\" \"'no'\"]\n",
      " [\"'50-59'\" \"'ge40'\" \"'25-29'\" \"'15-17'\" \"'yes'\" \"'3'\" \"'right'\"\n",
      "  \"'left_up'\" \"'no'\"]\n",
      " [\"'40-49'\" \"'premeno'\" \"'10-14'\" \"'0-2'\" \"'no'\" \"'2'\" \"'right'\"\n",
      "  \"'left_low'\" \"'no'\"]]\n"
     ]
    }
   ],
   "source": [
    "# summarize\n",
    "print(\"=============== Cantidad de datos de entrenamiento y pruebas ==================\")\n",
    "print('Train', X_train_enc)\n",
    "print('Test', X_test.shape, y_test.shape)\n",
    "\n",
    "print(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "089ff6da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mmora/anaconda3/envs/ml_env/lib/python3.9/site-packages/sklearn/preprocessing/_label.py:98: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/mmora/anaconda3/envs/ml_env/lib/python3.9/site-packages/sklearn/preprocessing/_label.py:133: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "# prepare output data\n",
    "y_train_enc, y_test_enc = prepare_targets(y_train, y_test)\n",
    "\n",
    "# make output 3d\n",
    "y_train_enc = y_train_enc.reshape((len(y_train_enc), 1, 1))\n",
    "y_test_enc = y_test_enc.reshape((len(y_test_enc), 1, 1))\n",
    "\n",
    "#print(y_train_enc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f70a00b5",
   "metadata": {},
   "source": [
    "## Demostración de uso de la capa de Embbeding (sin entrenar)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9750f371",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Words must be encoded to pass them to the network\n",
    "x = torch.tensor([[8,6,22,99,98,73,55,6],\n",
    "                 [43,65,67,32,22,68,6,2],\n",
    "                 [5,77,9,44,80,22,67,98]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "937fa53c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.8242, -0.3252,  0.3507,  0.3066, -0.8471, -0.4772, -0.0740,\n",
      "          -1.1890, -1.1237,  0.2101],\n",
      "         [-0.9664, -0.4777, -1.0152,  0.8259,  0.1611, -0.6200,  1.7658,\n",
      "           0.1584,  1.3820, -0.0477],\n",
      "         [ 0.6277,  0.2875,  0.2370,  1.8001, -0.9229,  0.8834,  0.6671,\n",
      "           0.2033, -0.4008,  1.0454],\n",
      "         [-0.0455, -1.3620, -1.2963,  1.0329, -0.0978,  0.3903, -0.8915,\n",
      "           0.2849, -1.2662, -1.0164],\n",
      "         [ 2.4928,  0.9330,  0.5080,  1.7356, -0.9419,  0.1307, -0.3615,\n",
      "           0.1445,  1.1244,  0.6139],\n",
      "         [ 1.1331, -1.2763,  1.1042,  1.2849, -1.1114, -0.5103, -0.0690,\n",
      "           0.4763,  1.7924, -0.7065],\n",
      "         [ 1.1208,  0.0163,  0.1834,  1.5753, -1.6584,  1.0253,  0.1199,\n",
      "           1.4505,  0.7030,  0.1735],\n",
      "         [-0.9664, -0.4777, -1.0152,  0.8259,  0.1611, -0.6200,  1.7658,\n",
      "           0.1584,  1.3820, -0.0477]],\n",
      "\n",
      "        [[ 0.5239,  1.5741,  0.3629,  0.1099,  0.8605,  0.4105, -0.1820,\n",
      "          -0.2295,  0.4657,  0.8737],\n",
      "         [-0.4251,  0.4806, -0.9103,  1.1995, -0.8325, -1.1115,  1.1590,\n",
      "           2.4023,  0.5070, -1.4065],\n",
      "         [-0.3069, -0.0957, -0.2787,  0.0951, -0.9145,  1.5746, -0.5010,\n",
      "           1.0612,  0.4050,  0.5788],\n",
      "         [-0.3336,  0.1884, -1.3347, -0.1175,  0.5147, -0.1637,  1.7347,\n",
      "           0.1882,  0.7961, -1.6449],\n",
      "         [ 0.6277,  0.2875,  0.2370,  1.8001, -0.9229,  0.8834,  0.6671,\n",
      "           0.2033, -0.4008,  1.0454],\n",
      "         [ 0.0531, -0.5128,  1.2869,  0.1210,  1.0567, -0.7422,  2.2766,\n",
      "           0.0327, -0.4973,  1.2531],\n",
      "         [-0.9664, -0.4777, -1.0152,  0.8259,  0.1611, -0.6200,  1.7658,\n",
      "           0.1584,  1.3820, -0.0477],\n",
      "         [ 0.3328, -0.0977,  0.7470,  0.7771,  1.6488, -0.5908,  0.0263,\n",
      "          -2.0319,  1.3201, -0.0839]],\n",
      "\n",
      "        [[-1.5461,  1.4760,  0.7588, -0.0030, -1.1356, -0.2527, -0.2995,\n",
      "          -1.0385, -1.1395,  0.0469],\n",
      "         [-0.0293,  0.8487,  0.7977, -0.5434,  0.4098,  0.9772, -0.2306,\n",
      "          -0.2220, -0.1822, -0.1356],\n",
      "         [ 1.3291, -1.1610, -1.1556,  0.3445,  1.0310, -1.2170,  1.1803,\n",
      "           1.0457,  0.2182, -0.0129],\n",
      "         [-0.8860, -1.5055,  0.1175, -0.4696, -0.5797,  1.2304,  1.4562,\n",
      "           0.3452, -1.5211, -1.4588],\n",
      "         [ 1.8235,  1.5035,  0.2671, -0.3524, -0.5065,  0.9906, -0.5311,\n",
      "           2.0435,  1.5196,  0.9347],\n",
      "         [ 0.6277,  0.2875,  0.2370,  1.8001, -0.9229,  0.8834,  0.6671,\n",
      "           0.2033, -0.4008,  1.0454],\n",
      "         [-0.3069, -0.0957, -0.2787,  0.0951, -0.9145,  1.5746, -0.5010,\n",
      "           1.0612,  0.4050,  0.5788],\n",
      "         [ 2.4928,  0.9330,  0.5080,  1.7356, -0.9419,  0.1307, -0.3615,\n",
      "           0.1445,  1.1244,  0.6139]]], grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# vectorization of words\n",
    "\n",
    "vocab_size = 100 \n",
    "# size of the vector representing the words\n",
    "embedding_dim = 10  \n",
    "embedding_layer = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "# Embedding layer output\n",
    "out1 = embedding_layer(x)\n",
    "\n",
    "print(out1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "94be7a1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.2174, -0.3359,  0.6195,  ...,  1.1784,  0.2482,  0.2195],\n",
      "         [-1.2203,  0.7760, -1.2649,  ..., -2.1388,  1.3165,  0.1988],\n",
      "         [ 0.2174, -0.3359,  0.6195,  ...,  1.1784,  0.2482,  0.2195],\n",
      "         ...,\n",
      "         [ 1.4419,  0.7008, -0.0742,  ..., -0.1825, -0.4501, -1.1709],\n",
      "         [ 1.4419,  0.7008, -0.0742,  ..., -0.1825, -0.4501, -1.1709],\n",
      "         [ 1.4419,  0.7008, -0.0742,  ..., -0.1825, -0.4501, -1.1709]],\n",
      "\n",
      "        [[ 0.1311, -1.0711,  1.0482,  ..., -1.6498,  0.1316,  0.5189],\n",
      "         [ 0.3328, -0.0977,  0.7470,  ..., -2.0319,  1.3201, -0.0839],\n",
      "         [ 0.3328, -0.0977,  0.7470,  ..., -2.0319,  1.3201, -0.0839],\n",
      "         ...,\n",
      "         [ 0.1311, -1.0711,  1.0482,  ..., -1.6498,  0.1316,  0.5189],\n",
      "         [ 0.1311, -1.0711,  1.0482,  ..., -1.6498,  0.1316,  0.5189],\n",
      "         [ 0.1311, -1.0711,  1.0482,  ..., -1.6498,  0.1316,  0.5189]],\n",
      "\n",
      "        [[ 1.4419,  0.7008, -0.0742,  ..., -0.1825, -0.4501, -1.1709],\n",
      "         [ 1.3291, -1.1610, -1.1556,  ...,  1.0457,  0.2182, -0.0129],\n",
      "         [ 0.3671,  0.6117, -0.6769,  ...,  1.1999, -1.3132,  0.0916],\n",
      "         ...,\n",
      "         [-1.2203,  0.7760, -1.2649,  ..., -2.1388,  1.3165,  0.1988],\n",
      "         [-0.3453,  0.0890, -0.5380,  ..., -0.7816, -0.0440,  0.9442],\n",
      "         [-0.8242, -0.3252,  0.3507,  ..., -1.1890, -1.1237,  0.2101]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.1311, -1.0711,  1.0482,  ..., -1.6498,  0.1316,  0.5189],\n",
      "         [ 0.1311, -1.0711,  1.0482,  ..., -1.6498,  0.1316,  0.5189],\n",
      "         [-1.2203,  0.7760, -1.2649,  ..., -2.1388,  1.3165,  0.1988],\n",
      "         ...,\n",
      "         [-1.2203,  0.7760, -1.2649,  ..., -2.1388,  1.3165,  0.1988],\n",
      "         [-1.2203,  0.7760, -1.2649,  ..., -2.1388,  1.3165,  0.1988],\n",
      "         [ 0.1311, -1.0711,  1.0482,  ..., -1.6498,  0.1316,  0.5189]],\n",
      "\n",
      "        [[ 0.2174, -0.3359,  0.6195,  ...,  1.1784,  0.2482,  0.2195],\n",
      "         [ 0.2174, -0.3359,  0.6195,  ...,  1.1784,  0.2482,  0.2195],\n",
      "         [ 0.3328, -0.0977,  0.7470,  ..., -2.0319,  1.3201, -0.0839],\n",
      "         ...,\n",
      "         [-1.2203,  0.7760, -1.2649,  ..., -2.1388,  1.3165,  0.1988],\n",
      "         [-1.2203,  0.7760, -1.2649,  ..., -2.1388,  1.3165,  0.1988],\n",
      "         [ 0.1311, -1.0711,  1.0482,  ..., -1.6498,  0.1316,  0.5189]],\n",
      "\n",
      "        [[ 0.1311, -1.0711,  1.0482,  ..., -1.6498,  0.1316,  0.5189],\n",
      "         [ 0.1311, -1.0711,  1.0482,  ..., -1.6498,  0.1316,  0.5189],\n",
      "         [-1.2203,  0.7760, -1.2649,  ..., -2.1388,  1.3165,  0.1988],\n",
      "         ...,\n",
      "         [ 0.1311, -1.0711,  1.0482,  ..., -1.6498,  0.1316,  0.5189],\n",
      "         [ 0.1311, -1.0711,  1.0482,  ..., -1.6498,  0.1316,  0.5189],\n",
      "         [ 0.1311, -1.0711,  1.0482,  ..., -1.6498,  0.1316,  0.5189]]],\n",
      "       grad_fn=<EmbeddingBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_36662/1667751672.py:2: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:230.)\n",
      "  out1 = embedding_layer(torch.tensor(X_train_enc))\n"
     ]
    }
   ],
   "source": [
    "# For the sample data\n",
    "out1 = embedding_layer(torch.tensor(X_train_enc))\n",
    "print(out1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8bdd82c",
   "metadata": {},
   "source": [
    "## Ejemplo con Keras (API muy simple construida sobre Tensorflow)\n",
    "\n",
    "Uso de la capa de Embbeding pero luego de entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "629ad6a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "12/12 - 1s - loss: 0.6795 - accuracy: 0.7173 - 1s/epoch - 106ms/step\n",
      "Epoch 2/20\n",
      "12/12 - 0s - loss: 0.6580 - accuracy: 0.7277 - 13ms/epoch - 1ms/step\n",
      "Epoch 3/20\n",
      "12/12 - 0s - loss: 0.6297 - accuracy: 0.7277 - 14ms/epoch - 1ms/step\n",
      "Epoch 4/20\n",
      "12/12 - 0s - loss: 0.6007 - accuracy: 0.7277 - 13ms/epoch - 1ms/step\n",
      "Epoch 5/20\n",
      "12/12 - 0s - loss: 0.5740 - accuracy: 0.7277 - 13ms/epoch - 1ms/step\n",
      "Epoch 6/20\n",
      "12/12 - 0s - loss: 0.5564 - accuracy: 0.7277 - 13ms/epoch - 1ms/step\n",
      "Epoch 7/20\n",
      "12/12 - 0s - loss: 0.5449 - accuracy: 0.7277 - 13ms/epoch - 1ms/step\n",
      "Epoch 8/20\n",
      "12/12 - 0s - loss: 0.5343 - accuracy: 0.7277 - 13ms/epoch - 1ms/step\n",
      "Epoch 9/20\n",
      "12/12 - 0s - loss: 0.5266 - accuracy: 0.7435 - 12ms/epoch - 1ms/step\n",
      "Epoch 10/20\n",
      "12/12 - 0s - loss: 0.5173 - accuracy: 0.7539 - 13ms/epoch - 1ms/step\n",
      "Epoch 11/20\n",
      "12/12 - 0s - loss: 0.5090 - accuracy: 0.7592 - 13ms/epoch - 1ms/step\n",
      "Epoch 12/20\n",
      "12/12 - 0s - loss: 0.5028 - accuracy: 0.7592 - 13ms/epoch - 1ms/step\n",
      "Epoch 13/20\n",
      "12/12 - 0s - loss: 0.4977 - accuracy: 0.7644 - 13ms/epoch - 1ms/step\n",
      "Epoch 14/20\n",
      "12/12 - 0s - loss: 0.4921 - accuracy: 0.7801 - 12ms/epoch - 997us/step\n",
      "Epoch 15/20\n",
      "12/12 - 0s - loss: 0.4871 - accuracy: 0.7853 - 12ms/epoch - 1ms/step\n",
      "Epoch 16/20\n",
      "12/12 - 0s - loss: 0.4831 - accuracy: 0.7853 - 13ms/epoch - 1ms/step\n",
      "Epoch 17/20\n",
      "12/12 - 0s - loss: 0.4776 - accuracy: 0.7958 - 13ms/epoch - 1ms/step\n",
      "Epoch 18/20\n",
      "12/12 - 0s - loss: 0.4737 - accuracy: 0.7958 - 13ms/epoch - 1ms/step\n",
      "Epoch 19/20\n",
      "12/12 - 0s - loss: 0.4697 - accuracy: 0.7958 - 13ms/epoch - 1ms/step\n",
      "Epoch 20/20\n",
      "12/12 - 0s - loss: 0.4665 - accuracy: 0.8010 - 13ms/epoch - 1ms/step\n",
      "Accuracy: 72.63\n"
     ]
    }
   ],
   "source": [
    "# A MultiLayer Perceptron (MLP) neural network is definde with one hidden layer with 10 nodes, \n",
    "# and one node in the output layer for making binary classifications.\n",
    "\n",
    "# prepare each input head\n",
    "in_layers = list()\n",
    "em_layers = list()\n",
    "\n",
    "# for each characteristic\n",
    "for i in range(len(X_train_enc)):\n",
    "    # calculate the number of unique inputs\n",
    "    n_labels = len(unique(X_train_enc[i]))\n",
    "    # define input layer\n",
    "    in_layer = Input(shape=(1,))\n",
    "\n",
    "    # define embedding layer\n",
    "    # The embedding layer is trained in the network training process.\n",
    "    em_layer = Embedding(n_labels, 10)(in_layer)\n",
    "    # store layers\n",
    "    in_layers.append(in_layer)\n",
    "    em_layers.append(em_layer)\n",
    "# concat all embeddings\n",
    "merge = concatenate(em_layers)\n",
    "dense = Dense(10, activation='relu', kernel_initializer='he_normal')(merge)\n",
    "output = Dense(1, activation='sigmoid')(dense)\n",
    "model = Model(inputs=in_layers, outputs=output)\n",
    "# compile the keras model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# fit the keras model on the dataset\n",
    "model.fit(X_train_enc, y_train_enc, epochs=20, batch_size=16, verbose=2)\n",
    "\n",
    "# evaluate the keras model\n",
    "_, accuracy = model.evaluate(X_test_enc, y_test_enc, verbose=0)\n",
    "print('Accuracy: %.2f' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23ccb162",
   "metadata": {},
   "source": [
    "### Referencias y más ejemplos\n",
    "\n",
    "[1] Sarkar, D. (2018). Categorical Data.  Recuperado de https://towardsdatascience.com/understanding-feature-engineering-part-2-categorical-data-f54324193e63\n",
    "    \n",
    "    \n",
    "[2] Here’s All you Need to Know About Encoding Categorical Data (with Python code). Recuperado de  https://www.analyticsvidhya.com/blog/2020/08/types-of-categorical-data-encoding/\n",
    "\n",
    "[4] Scikit-learn (2016). Category Encoders.Recuperado de http://contrib.scikit-learn.org/category_encoders/index.html\n",
    "\n",
    "[5] Dhasade, G (2020). Ways To Handle Categorical Data With Implementation. Recuperado de https://towardsdatascience.com/ways-to-handle-categorical-data-before-train-ml-models-with-implementation-ffc213dc84ec\n",
    "\n",
    "Ali, M. (2023). Handling Machine Learning Categorical Data with Python Tutorial. Recuperado de https://www.datacamp.com/tutorial/categorical-data\n",
    "\n",
    "Mougan, C., Masip,D., Nin, J. & Pujol, O. (2021). Quantile Encoder: Tackling High Cardinality Categorical Features in Regression Problems. Recuperado de https://link.springer.com/chapter/10.1007/978-3-030-85529-1_14\n",
    "\n",
    "Carey, G. (2003). Coding Categorical Variables. Recuperado de http://psych.colorado.edu/~carey/Courses/PSYC5741/handouts/Coding%20Categorical%20Variables%202006-03-03.pdf\n",
    "\n",
    "Brownlee, J. (2020). 3 Ways to Encode Categorical Variables for Deep Learning. Recuperado de https://machinelearningmastery.com/how-to-prepare-categorical-data-for-deep-learning-in-python/\n",
    "\n",
    "Serger, C. (2018). An investigation of categorical variable encoding techniques in machine learning: binary versus one-hot and feature hashing. Recuperado de https://www.diva-portal.org/smash/get/diva2:1259073/FULLTEXT01.pdf\n",
    "\n",
    "Tabular Modeling Deep Dive. Recuperado de https://github.com/fastai/fastbook/blob/master/09_tabular.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a216ae5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
