{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7y2MGMnOtLWR"
   },
   "source": [
    "# Instituto Tecnológico de Costa Rica (ITCR)\n",
    "## Aprendizaje automático\n",
    "\n",
    "\n",
    "# Selección de características\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "Autores: Saúl Calderón, Juan Esquivel, Luis-Alexander Calvo-Valverde, María Auxiliadora Mora"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7KYJ_CTEmXfD"
   },
   "source": [
    "En general, la gran cantidad de datos existentes, en algunos casos **datos masivos**, por ejemplo, los generados con sensores (ej. para capturar temperatura, calidad del aire, humedad, pH del suelo, entre otros) nos **da la posiblidad de calcular miles de características o «features» posibles** para un conjunto de datos. Sin embargo, crear un **arreglo de características $\\vec{x}\\in\\mathbb{R}^{D}$ con un D muy alto, genera problemas** en cuanto al tiempo de entrenamiento, el cual crece de forma exponencial por la maldición de la dimensionalidad, sumado al riesgo de sobre ajuste al agregar más características al arreglo. Además, algunas de esas característcias pueden ser útiles, otras podrían ser completamente inútiles. Entonces **los mecanismos para selección de características resultan muy necesarios**. \n",
    "\n",
    "Los métodos de selección de características tienen como **objetivo reducir las dimensiones de las muestras, minimizando la pérdida de información, y posibilitando la efectividad de los modelos a construir sobre tales datos**. \n",
    "\n",
    "A continuación se discuten **tres técnicas básicas de selección de características** que son parte de los métodos supervisados: \n",
    "\n",
    "- Métodos de filtrado \n",
    "- Métodos de envoltura  \n",
    "- Métodos empotrados \n",
    "\n",
    "La siguiente figura (Brownlee, 2019) muestra las técnicas básicas:\n",
    "\n",
    "![](../imagenes/metodos_seleccion_caracteristicas.png)\n",
    "*Figura que muestra algunas técnicas básicas de selección de características y ejemplos de algoritmos como el Recursive feature elimination (RFE).*\n",
    "\n",
    "## Métodos de filtrado\n",
    "\n",
    "Los métodos de filtrado **analizan la relación entre las características y la variable de salida o a estimar**. Por ejemplo, si el objetivo del modelo es estimar la temperatura $y$ con variable aleatoria $Y$, y para ello se contempla la construcción de un modelo basado en el vector de características $\\vec{x}=\\left[x,v\\right]$, con $x$ correspondiente a la medición de la humedad y $v$ a la iluminación, y sus variables aleatorias correspondientes $X$ y $V$. A continuación se detalla el método de información mutua.\n",
    "\n",
    "\n",
    "### Método de Información Mutua\n",
    "\n",
    "La información mutua entre dos variables aleatorias, por ejemplo Y y X **mide la dependencia entre tales variables aleatorias**. Para realizarlo, si se dispone de la función de densidad de probabilidad de ambas variables aleatorias discretas $p\\left[Y=y\\right]=p\\left[y\\right]$ y $p\\left[X=x\\right]=p\\left[x\\right]$, la información mutua de ambas variables aleatorias puede expresarse de la siguiente forma: \n",
    "\n",
    "$$I\\left[X;Y\\right]=\\sum_{y\\in Y}\\sum_{x\\in X}p\\left[x,y\\right]\\log\\left(\\frac{p\\left[x,y\\right]}{p\\left[x\\right]p\\left[y\\right]}\\right)$$\n",
    "\n",
    "de forma que: \n",
    "\n",
    "1. Si $Y$ y $X$ son independientes, no hay información de $Y$ que se pueda obtener conociendo $X$ ni viceversa, por lo que entonces la Información mutua \n",
    "$I\\left[X;Y\\right]=0$. Esto pues $\\log{\\left(\\frac{p\\left[x,y\\right]}{p\\left[x\\right]p\\left[y\\right]}\\right)}=\\log{1}=0$\n",
    "\n",
    "2. Si existe una función determinística que permita calcular $y$ con $x$, entonces la información mutua resulta en $I\\left[X;Y\\right]=1$. \n",
    "\n",
    "Dos beneficios de usar el método de información mutua como selector de características, son:\n",
    "\n",
    "- Es un método neutral, lo que significa que la solución se puede aplicar a varios tipos de algoritmos de ML.\n",
    "- La solución es rápida."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejemplo 1\n",
    "Prueba de tres modelos de clasificación utilizando subconjuntos distintos de características seleccionadas con los resultados del método de información mutua.\n",
    "\n",
    "Conjuntos de datos:\n",
    "- 1 Todo el conjunto de datos (nombre con _1).\n",
    "- 2 Seleccionando las variables con score > 0.2 (_2)\n",
    "- 3 Seleccionando las variables con score <= 0.2 (_3)\n",
    "\n",
    "Datos a utilizar: Datos de cáncer de mama de Wisconsin. El conjunto de datos de cáncer de mama es un conjunto de datos utilizado para clasificación binaria clásico.\n",
    "\n",
    "Composición:\n",
    "- Classes: 2\n",
    "- Samples per class: 212(Malignant),357(Benign)\n",
    "- Samples total: 569\n",
    "- Dimensionality: 30\n",
    "- Features: real, positive\n",
    "\n",
    "Algunas características:\n",
    "- radius (mean of distances from center to points on the perimeter)        \n",
    "- texture (standard deviation of gray-scale values)       \n",
    "- perimeter        \n",
    "- area        \n",
    "- smoothness (local variation in radius lengths)        \n",
    "- compactness (perimeter^2 / area - 1.0)        \n",
    "- concavity (severity of concave portions of the contour)       \n",
    "- concave points (number of concave portions of the contour)       \n",
    "- symmetry\n",
    "- fractal dimension (\"coastline approximation\"). Multiple fields.   \n",
    "- class:\n",
    "       - WDBC-Malignant                \n",
    "       - WDBC-Benign\n",
    "\n",
    "\n",
    "Datos disponibles como parte de los conjuntos de datos del scikit-learn en:\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_breast_cancer.html\n",
    "\n",
    "El conjunto de datos original y su documentación están disponible en https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+(Diagnostic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo 1\n",
    "\n",
    "from sklearn.datasets import load_breast_cancer \n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.model_selection import train_test_split \n",
    "import numpy as np\n",
    "from sklearn.feature_selection import SelectPercentile \n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "\n",
    "# cancer is of type Bunch (Dictionary-like object)\n",
    "cancer = load_breast_cancer ()\n",
    "X = cancer['data']\n",
    "y = cancer['target']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de características (569, 30)\n"
     ]
    }
   ],
   "source": [
    "print(\"Número de características\", X.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.36665582 0.09695648 0.40333271 0.35922432 0.08215737 0.21205567\n",
      " 0.3733539  0.44121908 0.06436303 0.00651898 0.24536058 0.00134916\n",
      " 0.27563103 0.339812   0.0146169  0.07563582 0.11845151 0.12691562\n",
      " 0.01363194 0.04144752 0.45500854 0.12071894 0.47510987 0.4632881\n",
      " 0.10723397 0.22553527 0.31515669 0.43781613 0.09553499 0.06840785]\n"
     ]
    }
   ],
   "source": [
    "# Compute MI (mutual information) score\n",
    "\n",
    "transformer = MinMaxScaler().fit(X)\n",
    "X = transformer.transform(X)\n",
    "#print( X[0:5] )\n",
    "\n",
    "# Vector with mutual information scores\n",
    "mi_score = mutual_info_classif(X,y)\n",
    "print(mi_score)\n",
    "\n",
    "# Since there are 30 characteristics, the algorithm generates \n",
    "# a vector with scores assigned to each one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datos de entrenamiento con todas las característica (426, 30)\n"
     ]
    }
   ],
   "source": [
    "# Set 1: train and test datasets (using all data). \n",
    "X_train_1,X_test_1,y_train_1,y_test_1 = train_test_split(X,y,random_state=0,stratify=y)\n",
    "\n",
    "print(\"Datos de entrenamiento con todas las característica\", X_train_1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0  2  3  5  6  7 10 12 13 20 22 23 25 26 27]\n",
      "Datos de entrenamiento con características con peso de información mutua > 0.2 (426, 15)\n"
     ]
    }
   ],
   "source": [
    "# Set 2: with features having MI scores > 0.2\n",
    "\n",
    "# Characteristics with mi_score > 0.2\n",
    "mi_score_selected_index = np.where(mi_score >0.2)[0]\n",
    "print(mi_score_selected_index)\n",
    "\n",
    "# X_2 data with features having MI scores > 0.2\n",
    "X_2 = X[:,mi_score_selected_index]\n",
    "\n",
    "X_train_2,X_test_2,y_train,y_test = train_test_split(X_2,y,random_state=0,stratify=y)\n",
    "\n",
    "print(\"Datos de entrenamiento con características con peso de información mutua > 0.2\", X_train_2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1  4  8  9 11 14 15 16 17 18 19 21 24 28 29]\n",
      "Datos de entrenamiento con características con peso de información mutua < 0.2 (426, 15)\n"
     ]
    }
   ],
   "source": [
    "# Dataset 3 with features have MI (mutual information) scores less than 0.2\n",
    "mi_score_selected_index = np.where(mi_score <= 0.2)[0]\n",
    "print(mi_score_selected_index)\n",
    "\n",
    "X_3 = X[:,mi_score_selected_index]\n",
    "X_train_3,X_test_3,y_train,y_test = train_test_split(X_3,y,random_state=0,stratify=y)\n",
    "\n",
    "print(\"Datos de entrenamiento con características con peso de información mutua < 0.2\", X_train_3.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score_1:0.9090909090909091\n",
      " score_2:0.916083916083916\n",
      " score_3:0.8321678321678322\n"
     ]
    }
   ],
   "source": [
    "# Test classifiers, one for each dataset with different columns.\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier \n",
    "model_1 = DecisionTreeClassifier().fit(X_train_1,y_train)\n",
    "model_2 = DecisionTreeClassifier().fit(X_train_2,y_train)\n",
    "model_3 = DecisionTreeClassifier().fit(X_train_3,y_train)\n",
    "\n",
    "# Return the mean accuracy on the given test data and labels.\n",
    "score_1 = model_1.score(X_test_1,y_test)\n",
    "score_2 = model_2.score(X_test_2,y_test)\n",
    "score_3 = model_3.score(X_test_3,y_test)\n",
    "print(f\"score_1:{score_1}\\n score_2:{score_2}\\n score_3:{score_3}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selector de características de Scikit-Learn\n",
    "\n",
    "Scikit-Learn proporciona selectores de características para no tener que calcular manualmente las puntuaciones de Información Mutua para seleccionar las características a utilizar. Por ejemplo, se puede seleccionar el 50% de características con mayor puntuación, otros selectores se usan de forma similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score_4:0.9230769230769231\n"
     ]
    }
   ],
   "source": [
    "selector = SelectPercentile(percentile=50) # select features with top 50% \n",
    "selector.fit(X,y)\n",
    "X_4 = selector.transform(X)\n",
    "\n",
    "X_train_4,X_test_4,y_train,y_test = train_test_split(X_4,y,random_state=0,stratify=y)\n",
    "\n",
    "model_4 = DecisionTreeClassifier().fit(X_train_4,y_train)\n",
    "score_4 = model_4.score(X_test_4,y_test)\n",
    "\n",
    "print(f\"score_4:{score_4}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusión \n",
    "\n",
    "- El conjunto de datos 2 con 15 características con información mutua (MI) > 0.2 alcanza una precisión mayor que 0.92 tan buena como el conjunto de datos 1, que incluye todas las características. Mientras que score_3 es tiene una precisión de 0.83, que es el resultado de 15 características que tienen una puntuación MI <= 0.2.\n",
    "\n",
    "- Es posible con el Método de información mutua, seleccionar las características que más aportan al ejercicio de clasificación en este conjunto de datos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejemplo 2  usando los datos de las especies de iris.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensiones del conjunto de datos (150, 4)\n",
      "Algunos registros [[5.1 3.5 1.4 0.2]\n",
      " [4.9 3.  1.4 0.2]\n",
      " [4.7 3.2 1.3 0.2]\n",
      " [4.6 3.1 1.5 0.2]\n",
      " [5.  3.6 1.4 0.2]]\n",
      "Pesos de información mutua [0.48889652 0.27416791 0.99603254 0.97979058]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "irisData = load_iris()\n",
    "\n",
    "# Cargado de datos \n",
    "X, y = load_iris(return_X_y=True)\n",
    "print(\"Dimensiones del conjunto de datos\",  X.shape )\n",
    "print(\"Algunos registros\", X[0:5] )\n",
    "\n",
    "transformer = MinMaxScaler().fit(X)\n",
    "X = transformer.transform(X)\n",
    "\n",
    "\n",
    "X_new = mutual_info_classif(X, y, discrete_features='auto', n_neighbors=3, copy=True, random_state=None)\n",
    "print(\"Pesos de información mutua\",  X_new )\n",
    "# Nota: Este método requiere X e y, y analiza la información mutua entre cada uno de los\n",
    "# atributos en X con y. print( X_new ) representa esa información, a más valor, más información\n",
    "# aporta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'data': array([[5.1, 3.5, 1.4, 0.2],\n",
      "       [4.9, 3. , 1.4, 0.2],\n",
      "       [4.7, 3.2, 1.3, 0.2],\n",
      "       [4.6, 3.1, 1.5, 0.2],\n",
      "       [5. , 3.6, 1.4, 0.2],\n",
      "       [5.4, 3.9, 1.7, 0.4],\n",
      "       [4.6, 3.4, 1.4, 0.3],\n",
      "       [5. , 3.4, 1.5, 0.2],\n",
      "       [4.4, 2.9, 1.4, 0.2],\n",
      "       [4.9, 3.1, 1.5, 0.1],\n",
      "       [5.4, 3.7, 1.5, 0.2],\n",
      "       [4.8, 3.4, 1.6, 0.2],\n",
      "       [4.8, 3. , 1.4, 0.1],\n",
      "       [4.3, 3. , 1.1, 0.1],\n",
      "       [5.8, 4. , 1.2, 0.2],\n",
      "       [5.7, 4.4, 1.5, 0.4],\n",
      "       [5.4, 3.9, 1.3, 0.4],\n",
      "       [5.1, 3.5, 1.4, 0.3],\n",
      "       [5.7, 3.8, 1.7, 0.3],\n",
      "       [5.1, 3.8, 1.5, 0.3],\n",
      "       [5.4, 3.4, 1.7, 0.2],\n",
      "       [5.1, 3.7, 1.5, 0.4],\n",
      "       [4.6, 3.6, 1. , 0.2],\n",
      "       [5.1, 3.3, 1.7, 0.5],\n",
      "       [4.8, 3.4, 1.9, 0.2],\n",
      "       [5. , 3. , 1.6, 0.2],\n",
      "       [5. , 3.4, 1.6, 0.4],\n",
      "       [5.2, 3.5, 1.5, 0.2],\n",
      "       [5.2, 3.4, 1.4, 0.2],\n",
      "       [4.7, 3.2, 1.6, 0.2],\n",
      "       [4.8, 3.1, 1.6, 0.2],\n",
      "       [5.4, 3.4, 1.5, 0.4],\n",
      "       [5.2, 4.1, 1.5, 0.1],\n",
      "       [5.5, 4.2, 1.4, 0.2],\n",
      "       [4.9, 3.1, 1.5, 0.2],\n",
      "       [5. , 3.2, 1.2, 0.2],\n",
      "       [5.5, 3.5, 1.3, 0.2],\n",
      "       [4.9, 3.6, 1.4, 0.1],\n",
      "       [4.4, 3. , 1.3, 0.2],\n",
      "       [5.1, 3.4, 1.5, 0.2],\n",
      "       [5. , 3.5, 1.3, 0.3],\n",
      "       [4.5, 2.3, 1.3, 0.3],\n",
      "       [4.4, 3.2, 1.3, 0.2],\n",
      "       [5. , 3.5, 1.6, 0.6],\n",
      "       [5.1, 3.8, 1.9, 0.4],\n",
      "       [4.8, 3. , 1.4, 0.3],\n",
      "       [5.1, 3.8, 1.6, 0.2],\n",
      "       [4.6, 3.2, 1.4, 0.2],\n",
      "       [5.3, 3.7, 1.5, 0.2],\n",
      "       [5. , 3.3, 1.4, 0.2],\n",
      "       [7. , 3.2, 4.7, 1.4],\n",
      "       [6.4, 3.2, 4.5, 1.5],\n",
      "       [6.9, 3.1, 4.9, 1.5],\n",
      "       [5.5, 2.3, 4. , 1.3],\n",
      "       [6.5, 2.8, 4.6, 1.5],\n",
      "       [5.7, 2.8, 4.5, 1.3],\n",
      "       [6.3, 3.3, 4.7, 1.6],\n",
      "       [4.9, 2.4, 3.3, 1. ],\n",
      "       [6.6, 2.9, 4.6, 1.3],\n",
      "       [5.2, 2.7, 3.9, 1.4],\n",
      "       [5. , 2. , 3.5, 1. ],\n",
      "       [5.9, 3. , 4.2, 1.5],\n",
      "       [6. , 2.2, 4. , 1. ],\n",
      "       [6.1, 2.9, 4.7, 1.4],\n",
      "       [5.6, 2.9, 3.6, 1.3],\n",
      "       [6.7, 3.1, 4.4, 1.4],\n",
      "       [5.6, 3. , 4.5, 1.5],\n",
      "       [5.8, 2.7, 4.1, 1. ],\n",
      "       [6.2, 2.2, 4.5, 1.5],\n",
      "       [5.6, 2.5, 3.9, 1.1],\n",
      "       [5.9, 3.2, 4.8, 1.8],\n",
      "       [6.1, 2.8, 4. , 1.3],\n",
      "       [6.3, 2.5, 4.9, 1.5],\n",
      "       [6.1, 2.8, 4.7, 1.2],\n",
      "       [6.4, 2.9, 4.3, 1.3],\n",
      "       [6.6, 3. , 4.4, 1.4],\n",
      "       [6.8, 2.8, 4.8, 1.4],\n",
      "       [6.7, 3. , 5. , 1.7],\n",
      "       [6. , 2.9, 4.5, 1.5],\n",
      "       [5.7, 2.6, 3.5, 1. ],\n",
      "       [5.5, 2.4, 3.8, 1.1],\n",
      "       [5.5, 2.4, 3.7, 1. ],\n",
      "       [5.8, 2.7, 3.9, 1.2],\n",
      "       [6. , 2.7, 5.1, 1.6],\n",
      "       [5.4, 3. , 4.5, 1.5],\n",
      "       [6. , 3.4, 4.5, 1.6],\n",
      "       [6.7, 3.1, 4.7, 1.5],\n",
      "       [6.3, 2.3, 4.4, 1.3],\n",
      "       [5.6, 3. , 4.1, 1.3],\n",
      "       [5.5, 2.5, 4. , 1.3],\n",
      "       [5.5, 2.6, 4.4, 1.2],\n",
      "       [6.1, 3. , 4.6, 1.4],\n",
      "       [5.8, 2.6, 4. , 1.2],\n",
      "       [5. , 2.3, 3.3, 1. ],\n",
      "       [5.6, 2.7, 4.2, 1.3],\n",
      "       [5.7, 3. , 4.2, 1.2],\n",
      "       [5.7, 2.9, 4.2, 1.3],\n",
      "       [6.2, 2.9, 4.3, 1.3],\n",
      "       [5.1, 2.5, 3. , 1.1],\n",
      "       [5.7, 2.8, 4.1, 1.3],\n",
      "       [6.3, 3.3, 6. , 2.5],\n",
      "       [5.8, 2.7, 5.1, 1.9],\n",
      "       [7.1, 3. , 5.9, 2.1],\n",
      "       [6.3, 2.9, 5.6, 1.8],\n",
      "       [6.5, 3. , 5.8, 2.2],\n",
      "       [7.6, 3. , 6.6, 2.1],\n",
      "       [4.9, 2.5, 4.5, 1.7],\n",
      "       [7.3, 2.9, 6.3, 1.8],\n",
      "       [6.7, 2.5, 5.8, 1.8],\n",
      "       [7.2, 3.6, 6.1, 2.5],\n",
      "       [6.5, 3.2, 5.1, 2. ],\n",
      "       [6.4, 2.7, 5.3, 1.9],\n",
      "       [6.8, 3. , 5.5, 2.1],\n",
      "       [5.7, 2.5, 5. , 2. ],\n",
      "       [5.8, 2.8, 5.1, 2.4],\n",
      "       [6.4, 3.2, 5.3, 2.3],\n",
      "       [6.5, 3. , 5.5, 1.8],\n",
      "       [7.7, 3.8, 6.7, 2.2],\n",
      "       [7.7, 2.6, 6.9, 2.3],\n",
      "       [6. , 2.2, 5. , 1.5],\n",
      "       [6.9, 3.2, 5.7, 2.3],\n",
      "       [5.6, 2.8, 4.9, 2. ],\n",
      "       [7.7, 2.8, 6.7, 2. ],\n",
      "       [6.3, 2.7, 4.9, 1.8],\n",
      "       [6.7, 3.3, 5.7, 2.1],\n",
      "       [7.2, 3.2, 6. , 1.8],\n",
      "       [6.2, 2.8, 4.8, 1.8],\n",
      "       [6.1, 3. , 4.9, 1.8],\n",
      "       [6.4, 2.8, 5.6, 2.1],\n",
      "       [7.2, 3. , 5.8, 1.6],\n",
      "       [7.4, 2.8, 6.1, 1.9],\n",
      "       [7.9, 3.8, 6.4, 2. ],\n",
      "       [6.4, 2.8, 5.6, 2.2],\n",
      "       [6.3, 2.8, 5.1, 1.5],\n",
      "       [6.1, 2.6, 5.6, 1.4],\n",
      "       [7.7, 3. , 6.1, 2.3],\n",
      "       [6.3, 3.4, 5.6, 2.4],\n",
      "       [6.4, 3.1, 5.5, 1.8],\n",
      "       [6. , 3. , 4.8, 1.8],\n",
      "       [6.9, 3.1, 5.4, 2.1],\n",
      "       [6.7, 3.1, 5.6, 2.4],\n",
      "       [6.9, 3.1, 5.1, 2.3],\n",
      "       [5.8, 2.7, 5.1, 1.9],\n",
      "       [6.8, 3.2, 5.9, 2.3],\n",
      "       [6.7, 3.3, 5.7, 2.5],\n",
      "       [6.7, 3. , 5.2, 2.3],\n",
      "       [6.3, 2.5, 5. , 1.9],\n",
      "       [6.5, 3. , 5.2, 2. ],\n",
      "       [6.2, 3.4, 5.4, 2.3],\n",
      "       [5.9, 3. , 5.1, 1.8]]), 'target': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]), 'frame': None, 'target_names': array(['setosa', 'versicolor', 'virginica'], dtype='<U10'), 'DESCR': '.. _iris_dataset:\\n\\nIris plants dataset\\n--------------------\\n\\n**Data Set Characteristics:**\\n\\n    :Number of Instances: 150 (50 in each of three classes)\\n    :Number of Attributes: 4 numeric, predictive attributes and the class\\n    :Attribute Information:\\n        - sepal length in cm\\n        - sepal width in cm\\n        - petal length in cm\\n        - petal width in cm\\n        - class:\\n                - Iris-Setosa\\n                - Iris-Versicolour\\n                - Iris-Virginica\\n                \\n    :Summary Statistics:\\n\\n    ============== ==== ==== ======= ===== ====================\\n                    Min  Max   Mean    SD   Class Correlation\\n    ============== ==== ==== ======= ===== ====================\\n    sepal length:   4.3  7.9   5.84   0.83    0.7826\\n    sepal width:    2.0  4.4   3.05   0.43   -0.4194\\n    petal length:   1.0  6.9   3.76   1.76    0.9490  (high!)\\n    petal width:    0.1  2.5   1.20   0.76    0.9565  (high!)\\n    ============== ==== ==== ======= ===== ====================\\n\\n    :Missing Attribute Values: None\\n    :Class Distribution: 33.3% for each of 3 classes.\\n    :Creator: R.A. Fisher\\n    :Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\\n    :Date: July, 1988\\n\\nThe famous Iris database, first used by Sir R.A. Fisher. The dataset is taken\\nfrom Fisher\\'s paper. Note that it\\'s the same as in R, but not as in the UCI\\nMachine Learning Repository, which has two wrong data points.\\n\\nThis is perhaps the best known database to be found in the\\npattern recognition literature.  Fisher\\'s paper is a classic in the field and\\nis referenced frequently to this day.  (See Duda & Hart, for example.)  The\\ndata set contains 3 classes of 50 instances each, where each class refers to a\\ntype of iris plant.  One class is linearly separable from the other 2; the\\nlatter are NOT linearly separable from each other.\\n\\n.. topic:: References\\n\\n   - Fisher, R.A. \"The use of multiple measurements in taxonomic problems\"\\n     Annual Eugenics, 7, Part II, 179-188 (1936); also in \"Contributions to\\n     Mathematical Statistics\" (John Wiley, NY, 1950).\\n   - Duda, R.O., & Hart, P.E. (1973) Pattern Classification and Scene Analysis.\\n     (Q327.D83) John Wiley & Sons.  ISBN 0-471-22361-1.  See page 218.\\n   - Dasarathy, B.V. (1980) \"Nosing Around the Neighborhood: A New System\\n     Structure and Classification Rule for Recognition in Partially Exposed\\n     Environments\".  IEEE Transactions on Pattern Analysis and Machine\\n     Intelligence, Vol. PAMI-2, No. 1, 67-71.\\n   - Gates, G.W. (1972) \"The Reduced Nearest Neighbor Rule\".  IEEE Transactions\\n     on Information Theory, May 1972, 431-433.\\n   - See also: 1988 MLC Proceedings, 54-64.  Cheeseman et al\"s AUTOCLASS II\\n     conceptual clustering system finds 3 classes in the data.\\n   - Many, many more ...', 'feature_names': ['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)'], 'filename': 'iris.csv', 'data_module': 'sklearn.datasets.data'}\n"
     ]
    }
   ],
   "source": [
    "print(irisData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Método de selección por Umbral de varianza (VarianceThreshold)\n",
    "\n",
    "Selector de características que elimina todas las funciones de baja varianza.\n",
    "\n",
    "Este algoritmo de selección analiza solo las características (X), no los resultados deseados (y), y por lo tanto puede usarse para el aprendizaje no supervisado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150, 4)\n",
      "[[5.1 3.5 1.4 0.2]\n",
      " [4.9 3.  1.4 0.2]\n",
      " [4.7 3.2 1.3 0.2]\n",
      " [4.6 3.1 1.5 0.2]\n",
      " [5.  3.6 1.4 0.2]]\n",
      "[[0.22222222 0.625      0.06779661 0.04166667]\n",
      " [0.16666667 0.41666667 0.06779661 0.04166667]\n",
      " [0.11111111 0.5        0.05084746 0.04166667]\n",
      " [0.08333333 0.45833333 0.08474576 0.04166667]\n",
      " [0.19444444 0.66666667 0.06779661 0.04166667]]\n",
      "varianzas:  [0.05255573 0.03276265 0.08892567 0.10019668]\n",
      "[[0.04166667]\n",
      " [0.04166667]\n",
      " [0.04166667]\n",
      " [0.04166667]\n",
      " [0.04166667]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "X, y = load_iris(return_X_y=True)\n",
    "print( X.shape )\n",
    "print( X[0:5] )\n",
    "\n",
    "transformer = MinMaxScaler().fit(X)\n",
    "X = transformer.transform(X)\n",
    "print( X[0:5] )\n",
    "\n",
    "selector = VarianceThreshold(threshold=.1)\n",
    "X_new = selector.fit_transform(X)\n",
    "print(\"varianzas: \", selector.variances_ )\n",
    "print( X_new[0:5] )\n",
    "\n",
    "# Este método analiza las varianzas en X, ojo que no usa y.  Y lo que imprime al final en\n",
    "# selector.variances_ son las varianzas de cada atributo. En X_new quedan los atributos\n",
    "# con varianzas mayores al umbral.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es común utilizar medidas estadísticas de tipo correlación entre las variables de entrada y salida como base para la selección de características de filtro. Debido a esto, la elección de medidas estadísticas depende en gran medida de los tipos de datos de las variables. La siguiente figura (Brownlee, 2019) muestra algunos algoritmos estadísticos utilizados en el selección de característica. \n",
    "\n",
    "![](../imagenes/Metodos_de_filtro_estadisticos.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Métodos de envoltura\n",
    "\n",
    "Los métodos de envoltura **generan modelos con subconjuntos de características y miden los rendimientos de los modelos** construídos con tales subconjuntos. \n",
    "\n",
    "### Búsqueda hacia adelante\n",
    "\n",
    "Este método evalúa en la primer etapa $n$ modelos construídos con las $n$ características específicas, para escoger el modelo con menor error con la característica $x_{i}$. En la segunda etapa, se combina tal característica $x_{i}$ con todas las restantes $n-1$ características para construír $n-1$ modelos, y escoger las mejores características $x_{i}$ y $x_{j}$. Tal secuencia de pasos se realiza de forma sucesiva para encontrar el conjunto de características más significativo, con un enfoque bottom-up.\n",
    "![picture](../imagenes/Seleccion_caracteristicas_baa.png)\n",
    "\n",
    "\n",
    "### Eliminación recursiva de características\n",
    "\n",
    "La eliminación recursiva de características inicia con el modelo construído a partir de las $n$ características, y en la próxima iteración prueba los modelos al remover una característica, usando un enfoque top-down. \n",
    "\n",
    "![picture](../imagenes/Seleccion_caracteristicas_recElim.png)\n",
    "\n",
    "\n",
    "## Métodos empotrados\n",
    "Existen además los métodos empotrados los cuales echan mano de métodos de aprendizaje automático y regresión los cuales reducen y resaltan las dimensiones más importantes, como por ejemplo, la regresión LASSO, la cual se discutirá más adelante. \n",
    "\n",
    "## Ejemplos\n",
    "En el siguiente link se aprecian varios métodos de selección de características en scikit-learn:\n",
    "\n",
    "https://scikit-learn.org/stable/modules/feature_selection.html#feature-selection\n",
    "\n",
    "SelectKBest\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectKBest.html#sklearn.feature_selection.SelectKBest\n",
    "\n",
    "chi2\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.chi2.html#sklearn.feature_selection.chi2\n",
    "\n",
    "Brownlee, J. (2019). How to Choose a Feature Selection Method For Machine Learning. Recuperado de https://machinelearningmastery.com/feature-selection-with-real-and-categorical-data/#:~:text=Feature%20selection%20is%20the%20process,the%20performance%20of%20the%20model.\n",
    "\n",
    "Zhu, A. (2021). Select Features for Machine Learning Model with Mutual Information. Recuperado de: https://towardsdatascience.com/select-features-for-machine-learning-model-with-mutual-information-534fe387d5c8\n",
    "\n",
    "Manual de Scikit-Learn v1.2.2 (n.d.). sklearn.feature_selection.mutual_info_classif. Recuperado de https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.mutual_info_classif.html#sklearn.feature_selection.mutual_info_classif\n",
    "\n",
    "Manual de Scikit-Learn v1.2.2 (n.d.). sklearn.feature_selection.VarianceThreshold. Recuperado de https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.VarianceThreshold.html#sklearn.feature_selection.VarianceThreshold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 225
    },
    "colab_type": "code",
    "id": "lXeiw7OSIg66",
    "outputId": "de6f46d6-895b-414b-8513-71f2f5c0b279"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150, 4)\n",
      "[[5.1 3.5 1.4 0.2]\n",
      " [4.9 3.  1.4 0.2]\n",
      " [4.7 3.2 1.3 0.2]\n",
      " [4.6 3.1 1.5 0.2]\n",
      " [5.  3.6 1.4 0.2]]\n",
      "(150, 2)\n",
      "[[1.4 0.2]\n",
      " [1.4 0.2]\n",
      " [1.3 0.2]\n",
      " [1.5 0.2]\n",
      " [1.4 0.2]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "X, y = load_iris(return_X_y=True)\n",
    "print( X.shape )\n",
    "print( X[0:5] )\n",
    "X_new = SelectKBest(chi2, k=2).fit_transform(X, y)\n",
    "print( X_new.shape )\n",
    "print( X_new[0:5] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Z8huDbwL22AU"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "ML_LACV_Selección_de_características.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
