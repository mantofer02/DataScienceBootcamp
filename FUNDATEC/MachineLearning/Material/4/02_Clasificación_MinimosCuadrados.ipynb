{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sPD8sqQXo3_P"
   },
   "source": [
    "## Escuela de Ingeniería en Computación, ITCR \n",
    "\n",
    "## Aprendizaje automático\n",
    "\n",
    "\n",
    "### Clasificación con modelos lineales\n",
    "\n",
    "---\n",
    "\n",
    "Autor: Saúl Calderón, Juan Esquivel, Luis-Alexander Calvo-Valverde"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "i0cR7LtcO6Ju"
   },
   "source": [
    "# Clasificación con modelos lineales\n",
    "En este documento se abordará el problema de\n",
    "clasificación utilizando únicamente modelos lineales. De esta forma, el estudiante podrá estudiar su comportamiento y limitaciones. Los modelos a estudiar implementan el **enfoque discriminativo de clasificación** (una muestra pertenece o no pertenece a una clase), prescindiendo de una definición probabilística de pertenencia, la cual separa las etapas de inferencia y de toma de decisiones. Los modelos lineales de clasificación suponen que los datos son linealmente separables, es decir, es posible trazar una superficie de decisión lineal que clasifique correctamente todas las muestras."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OP-hrfaCEcat"
   },
   "source": [
    "## Modelos lineales para clasificación en dos clases\n",
    "\n",
    "Anteriormente se exploró el problema de regresión lineal donde un\n",
    "conjunto de datos a la entrada y salida de un fenómeno $\\mathcal{D}=\\{\\vec{x},\\vec{t}\\}$\n",
    "se utilizó para aproximar un modelo específico $y(x,\\vec{w})$.\n",
    "Con tal modelo es posible predecir la salida del fenómeno estudiado\n",
    "frente a nuevas muestras $\\vec{x}'$. \n",
    "\n",
    "\n",
    "El objetivo de la clasificación consiste en tomar una muestra de dimensión\n",
    "$D$, $\\vec{m'}=[m_{1},\\ldots,m_{D}]^{t}$ y asignarle una etiqueta\n",
    "$k$ de las $k=1,\\ldots,K$ clases posibles. El espacio de las entradas\n",
    "de dimensión $D$ es dividido en **regiones de decisión**, cuyos\n",
    "límites se denominan superficies de decisión o límites de decisión.\n",
    "Para los clasificadores con modelos lineales, las superficies de decisión\n",
    "se definen como una función lineal respecto a la muestra a evaluar\n",
    "$\\vec{m}$, tal función es refererida como **función de activación**\n",
    "$f$: \n",
    "\n",
    "\\begin{equation}\n",
    "y(\\vec{m})=f\\left(\\vec{w'}^{T}\\vec{m}'\\mathbf{+}w_{0}\\right)=f\\left(y(\\vec{m})\\right)\n",
    "\\end{equation}\n",
    "\n",
    "con lo que la posición de la muestra $\\vec{m'}$ respecto al hiperplano\n",
    "definido por $y(\\vec{m})$ define la clasificación (etiqueta $k$)\n",
    "de la muestra $\\vec{m'}$. La dimensionalidad del vector de pesos,\n",
    "en este caso, es la misma que los datos, por lo que $\\vec{w'}\\in\\mathbb{R}^{D}$.\n",
    "La función $y(\\vec{m'})$ es lineal respecto a $\\vec{m'}$.\n",
    "\n",
    "Haciendo la analogía respecto al problema de regresión, las muestras\n",
    "de entrada $\\vec{x}$ son comparables con el conjunto de muestras\n",
    "de entrada $M'=\\left\\{ \\vec{m'}_{1},\\vec{m'}_{2},\\ldots,\\vec{m'}_{N}\\right\\} $\n",
    "para construir el modelo de clasificación, y por cada muestra a la\n",
    "salida real del fenómeno $t$ es comparable con la etiqueta a asignar\n",
    "por muestra $\\vec{q}$. El problema de la clasificación se puede enfocar\n",
    "entonces como la **discretización del problema de ajuste de\n",
    "curvas**, donde la salida $t$ pasa a tener un número finito de valores\n",
    "$K$. Por ejemplo, si el objetivo es clasificar en $K=2$ clases el\n",
    "conjunto de muestras $M'$, $\\vec{q}\\in\\{0,1\\}$. Si $K>2$, la codificación\n",
    "de la etiqueta puede implementarse usando un esquema 1-de-*K* el cual define a\n",
    "\n",
    "\\begin{equation}\n",
    "\\vec{q}=\\left[q_{0},q_{1},\\ldots,q_{K}\\right],\n",
    "\\end{equation}\n",
    "\n",
    "poniendo en uno el valor $q_{j}$ de la etiqueta $j$ a representar\n",
    "y en cero los demás como un vector de largo $K$.\n",
    "\n",
    "Por ejemplo, si la muestra $\\vec{m}_{i}$ corresponde a la clase $k=3$\n",
    "de $K=5$, se tiene que: \n",
    "\n",
    "\\begin{equation}\n",
    "\\vec{q}=\\left[0,0,1,0,0\\right],\n",
    "\\end{equation}\n",
    " \n",
    "\n",
    "Para el problema de clasificación en 2 clases, la función lineal discriminante\n",
    "más sencilla es dada por:\n",
    "\n",
    "\\begin{equation}\n",
    "y(\\vec{m})=\\vec{w}'^{T}\\vec{m'}+w_{0}\n",
    "\\end{equation}\n",
    "\n",
    "donde $\\vec{w}$ es el vector de pesos, y $w_{0}$ **es el sesgo\n",
    "o su negativo es también llamado umbral**. Un vector de entrada $\\vec{m}$\n",
    "es asignado a la clase $\\mathcal{C}_{1}$ si $y\\left(\\vec{m'}\\right)\\geq0$\n",
    "y a la clase $\\mathcal{C}_{2}$ de otro modo. Lo anterior se resume\n",
    "como:\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{array}[t]{ccc}\n",
    "y\\left(\\vec{m'}\\right)\\geq0 & \\vec{m'}\\in\\mathcal{C}_{1} & k=0\\\\\n",
    "y\\left(\\vec{m}'\\right)<0 & \\vec{m'}\\in\\mathcal{C}_{2} & k=1,\n",
    "\\end{array}\n",
    "\\end{equation}\n",
    "\n",
    "donde $k$ corresponde a la etiqueta de la clase. Como se puede observar\n",
    "en la siguiente Figura, el vector de pesos $\\vec{w}'$ define la orientación del límite de decisión, y el negativo del sesgo $w_{0}$, la distancia respecto al origen. La superficie de decisión con $D=2$ está definida por $y\\left(\\vec{m'}\\right)=\\vec{w}'^{T}\\vec{m'}+w_{0}$,\n",
    "con $\\mathbf{\\mbox{x}}=\\vec{m'}$ a partir de las ecuaciones de proyección\n",
    "y del hecho de que para los puntos en la superficie de decisión $y(\\mathbf{x})=0$\n",
    "\n",
    "![](https://drive.google.com/uc?export=view&id=1DThUicIRQbntw4XqejcKIhOx3MSRhf3N)\n",
    "\n",
    "\n",
    "Tomando en cuenta como **los dos puntos** $\\vec{m}_{1}$ y $\\vec{m}_{2}$\n",
    "están sobre la superficie de decisión, por lo que entonces\n",
    "\n",
    "\\begin{equation}\n",
    "y\\left(\\vec{m'}_{1}\\right)=y\\left(\\vec{m'}_{2}\\right)=0\\Rightarrow\\vec{w}'^{T}\\vec{m'_{1}}+w_{0}-\\vec{w}'^{T}\\vec{m'_{2}}-w_{0}=0\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "\\Rightarrow\\vec{w}^{T}\\left(\\vec{m'_{1}}-\\vec{m'_{2}}\\right)=0\n",
    "\\end{equation}\n",
    "\n",
    "lo cual significa que el vector $\\vec{w}$ es perpendicular a todos\n",
    "los vectores paralelos o sobre la superficie de decisión, y además\n",
    "define la orientación de tal superficie, **como lo muestra el vector verde en la Figura.** \n",
    "\n",
    "De forma similar, si $\\vec{m'}_{1}$ es un punto sobre la superficie\n",
    "de decisión, se tiene que:\n",
    "\n",
    "\\begin{equation}\n",
    "y\\left(\\vec{m'}_{1}\\right)=\\vec{w}'^{T}\\vec{m'}_{1}+w_{0}=0\n",
    "\\end{equation}\n",
    "\n",
    "Para **simplificar la notación**, se incluye el sesgo, con un\n",
    "valor neutro $m_{o}=1$, definiendo entonces $\\vec{w}=\\left[w_{0},\\vec{w}\\right]^{T}$\n",
    "y $\\vec{m}=\\left(m_{0},\\vec{m'}\\right)$. Por ello, $\\vec{w},\\vec{m}\\in\\mathbb{R}^{D+1}$.\n",
    "Es necesario entonces aumentar la dimensionalidad de las entradas\n",
    "a $D+1$ para utilizar esta notación. Así entonces la expresión simplificada,\n",
    "la ecuación se escribe como sigue:\n",
    "\n",
    "\\begin{equation}\n",
    "y\\left(\\vec{m}\\right)=\\vec{w}^{T}\\vec{m}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oAyeSJ2xOgLa"
   },
   "source": [
    "### Mínimos cuadrados para la clasificación en dos clases\n",
    "\n",
    "Anteriormente se analizó el enfoque de mínimos cuadrados para el ajuste\n",
    "de curvas, el cual proponía una solución analítica cerrada a partir\n",
    "de la derivada de la función de error. Para el problema de clasificación\n",
    "en dos clases, la minimización del error cuadrático se hace respecto\n",
    "a los $N$ pares ordenados de entrenamiento $\\mathcal{D}=\\{M,T\\}$,\n",
    "con $M=\\{\\vec{m}_{1},\\ldots,\\vec{m}_{N}\\}$ y $T=\\{t_{1},\\ldots,t_{N}\\}$,\n",
    "pues recordemos que $t_{j}\\in\\{0,1\\}$ en el caso de la clasificación\n",
    "por dos clases $K=2$.\n",
    "\n",
    "\n",
    "Generalizando para la matriz de muestras $M\\in\\mathbb{R}^{N\\times\\left(D+1\\right)}$\n",
    "y $\\vec{w}\\in\\mathbb{R}^{\\left(D+1\\right)\\times1}$, donde los valores\n",
    "de cada muestra $\\vec{m}_{j}$ se representan en la fila $j$ de tal\n",
    "matriz a la ecuación en la sección anterior, se obtiene:\n",
    "\n",
    "\\begin{equation}\n",
    "y\\left(M\\right)=M\\,\\vec{w}.\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "\\Rightarrow y\\left(M\\right)=\\begin{bmatrix}- & \\vec{m}_{1} & -\\\\\n",
    "\\vdots & \\vdots & \\vdots\\\\\n",
    "- & \\vec{m}_{N} & -\n",
    "\\end{bmatrix}\\begin{bmatrix}w_{0}\\\\\n",
    "\\vdots\\\\\n",
    "w_{D}\n",
    "\\end{bmatrix}.\n",
    "\\end{equation}\n",
    "\n",
    "Así, respecto a la matriz con todos los valores conocidos de pertenencia\n",
    "de clase $\\vec{t}=\\begin{bmatrix}t_{1} & \\ldots & t_{N}\\end{bmatrix}^{T}$,\n",
    "con $t_{j}\\in\\{0,1\\}$, el error cuadrático en forma matricial se\n",
    "expresa como sigue:\n",
    "\n",
    "\\begin{equation}\n",
    "E\\left(\\vec{w}\\right)=\\frac{1}{2}\\left\\Vert M\\,\\vec{w}-\\vec{t}\\right\\Vert ^{2}=\\frac{1}{2}\\left\\{ \\left(M\\,\\vec{w}-\\vec{t}\\right)^{T}\\left(M\\,\\vec{w}-\\vec{t}\\right)\\right\\} \n",
    "\\end{equation}\n",
    "\n",
    "La función gradiente respecto al vector de pesos $\\vec{w}$ de esta\n",
    "función de error igualada a cero, viene dada por (según\n",
    "lo demostrado para el caso de la regresión):\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{\\partial E(\\vec{w})}{\\partial\\vec{w}}=M^{T}M\\,\\vec{w}-M^{T}\\vec{t}=0\n",
    "\\end{equation}\n",
    "\n",
    "y despejando $\\vec{w}$ para obtener el vector de pesos que logra el error mínimo:\n",
    "\n",
    "\\begin{equation}\n",
    "\\vec{w}=\\left(M^{T}M\\right)^{-1}M^{T}\\vec{t}\n",
    "\\end{equation}\n",
    "\n",
    "donde, si no existe la inversa de $\\left(M^{T}M\\right)^{-1}$,\n",
    "los tres factores se reemplazan por la pseudo inversa $\\left(M^{T}M\\right)^{-1}M^{T}=M^{+}$.\n",
    "Así, la clasificación de una nueva muestra $\\vec{h}\\in\\mathbb{R}^{D+1}$\n",
    "viene dada por:\n",
    "\n",
    "\\begin{equation}\n",
    "y\\left(\\vec{h}\\right)=\\vec{h}\\:\\left(\\left(M^{T}M\\right)^{-1}M^{T}\\vec{t}\\right).\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "Para simplificar la clasificación en dos clases, la superficie de\n",
    "decisión queda definida por $y\\left(\\vec{m}\\right)=w_{0}$, por lo\n",
    "que si $y\\left(\\vec{m}\\right)\\geq w_{0}$, $\\vec{m}\\in\\mathcal{C}_{1}$,\n",
    "de lo contrario $\\vec{m}\\in\\mathcal{C}_{2}$.\n",
    "\n",
    "\n",
    "Para el caso específico de la clasificación en un espacio $\\mathbb{R}^{2}$,\n",
    "($D=2$) , se tiene que cada muestra está dada por $\\vec{m}=\\begin{bmatrix}m_{0}=1 & m_{1} & m_{2}\\end{bmatrix}^{T}$\n",
    "y el modelo está compuesto por los pesos $\\vec{w}=\\begin{bmatrix}w_{0} & w_{1} & w_{2}\\end{bmatrix}^{T}$,\n",
    "por lo que entonces el modelo está dado por: \n",
    "\n",
    "\\begin{equation}\n",
    "y\\left(\\vec{m}\\right)=w_{0}+w_{1}m_{1}+w_{2}m_{2}\n",
    "\\end{equation}\n",
    "\n",
    "lo cual es un plano en $\\mathbb{R}^{2}$, para lo cual es necesario\n",
    "graficar la curva de nivel, para lo cual se toma el valor en $y\\left(\\vec{m}\\right)=0$,\n",
    "con lo que:\n",
    "\n",
    "\\begin{equation}\n",
    "-w_{0}=w_{1}m_{1}+w_{2}m_{2}\n",
    "\\end{equation}\n",
    "\n",
    "y para la graficación de la curva, es necesario entonces despejar\n",
    "alguna de las variables: \n",
    "\\begin{equation}\n",
    "\\frac{-w_{0}-w_{1}m_{1}}{w_{2}}=m_{2}\n",
    "\\end{equation}\n",
    "\n",
    "La clasificación por mínimos cuadrados presenta una **importante\n",
    "debilidad**: al castigar la distancia euclidiana de las muestras respecto\n",
    "al modelo, la exactitud de tal modelo es perjudicada cuando se presentan\n",
    "sesgos en las muestras, es decir, muestras muy desviadas del comportamiento\n",
    "usual de la clase. Incluso si estos sesgos son *muy correctos*\n",
    "(muestras muy alejadas de los montículos principales), el modelo es\n",
    "afectado, como se observa en la Figura.\n",
    "\n",
    "![](https://drive.google.com/uc?export=view&id=13I0hNftkuYkFs0r95L-eoq53TeF4P6IJ)\n",
    "\n",
    "Ej: https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html#sklearn.linear_model.LinearRegression\n",
    "\n",
    "### Recordemos la fórmula de mínimos cuadrados\n",
    "El problema de los mínimos cuadrados se define para dada la matriz $A\\in\\mathbb{R}^{m\\times n}$ y el vector $\\vec{b}\\in\\mathbb{R}^{m\\times1},$\n",
    "encontrar el vector $\\vec{x}\\in\\mathbb{R}^{n\\times1}$ más cercano al **espacio de columnas** de la matriz $A$, el cual recordamos es denotado como $\\mathcal{C}\\left(A\\right)$ y corresponde al espacio generado por las columnas de la matriz $A$, combinadas linealmente por los componentes $x_{i}$ del vector $\\vec{x}$: \n",
    "\n",
    "\\begin{equation}\n",
    "\\mathcal{C}\\left(A\\right)=\\left\\{ \\vec{v}\\in\\mathbb{R}^{m}:\\vec{v}=A\\,\\vec{x},\\;\\vec{x}\\in\\mathbb{R}^{n},\\:A\\in\\mathbb{R}^{m\\times n}\\right\\} \n",
    "\\end{equation}\n",
    "\n",
    "Asumiendo que $A$ es de rango completo y que $n<m$ se tiene que la proyección del vector $\\vec{b}\\in\\mathbb{R}^{m\\times1}$ **al cuadrado para simplificar su minimización** en el espacio de columnas\n",
    "de la matriz $A$ está dado por: \n",
    "\n",
    "\\begin{equation}\n",
    "\\textrm{proy}\\left(\\vec{b};A\\right)=\\textrm{argmin}_{\\vec{v}\\in\\mathcal{C}\\left(A\\right)}\\left\\Vert \\vec{v}-\\vec{b}\\right\\Vert _{2}^{2}=\\textrm{argmin}_{\\vec{x}}\\left(A\\:\\vec{x}-\\vec{b}\\right)\\cdotp\\left(A\\:\\vec{x}-\\vec{b}\\right)\n",
    "\\end{equation}\n",
    "\n",
    "Entonces se calculará **el gradiente de tal producto y se igualará a cero**, para encontrar su punto mínimo y despejando.  \n",
    "\n",
    "\\begin{equation}\n",
    "\\Rightarrow\\vec{x}=A^{+}\\,\\vec{b}\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "### Datos utilizados en el ejemplo\n",
    "Los datos se extrajeron de imágenes que se tomaron de especímenes \n",
    "similares a billetes genuinos y falsificados. La herramienta\n",
    "Wavelet Transform se utilizó para extraer características de las imágenes. Más información en https://archive.ics.uci.edu/ml/datasets/banknote+authentication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X:  1372\n",
      "[[ 3.6216   8.6661  -2.8073  -0.44699]\n",
      " [ 4.5459   8.1674  -2.4586  -1.4621 ]\n",
      " [ 3.866   -2.6383   1.9242   0.10645]\n",
      " [ 3.4566   9.5228  -4.0112  -3.5944 ]]\n",
      "y:  1372\n",
      "[0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn import linear_model\n",
    "from numpy import genfromtxt\n",
    "\n",
    "\n",
    "my_data = genfromtxt('../../Data/data_banknote_authentication.csv', delimiter=';',filling_values=0.0,skip_header=1)\n",
    "X = my_data[:,0:-1]\n",
    "cantidad_muestras = len(my_data)\n",
    "y = my_data[:,-1]\n",
    "print(\"X: \", len(X))\n",
    "print(X[0:4])\n",
    "print(\"y: \", len(y))\n",
    "print(y[0:4])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RobustScaler()\n",
      "X:  1372\n",
      "[[ 1.          0.68025618  0.74464159 -0.72018678  0.04973186]\n",
      " [ 1.          0.88143259  0.68612813 -0.64684149 -0.31174108]\n",
      " [ 1.          0.7334505  -0.58172613  0.27503326  0.24680763]\n",
      " [ 1.          0.64434348  0.84515991 -0.97341417 -1.07103687]]\n",
      "y:  1372\n",
      "[0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import RobustScaler\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.RobustScaler.html#sklearn.preprocessing.RobustScaler\n",
    "\n",
    "transformer = RobustScaler().fit(X)\n",
    "print( RobustScaler() )\n",
    "X = transformer.transform(X)\n",
    "bias = np.ones((cantidad_muestras,1))\n",
    "X = np.append(bias,X, axis=1)\n",
    "\n",
    "print(\"X: \", len(X))\n",
    "print(X[0:4])\n",
    "print(\"y: \", len(y))\n",
    "print(y[0:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:  960\n",
      "Test:  412\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=42)   \n",
    "print(\"Train: \", len(X_train))\n",
    "print(\"Test: \", len(X_test))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "OLSR\n",
      "Atributos X:  6860\n",
      "Score:  0.8646952440209418\n",
      "Coeficientes:  [ 0.         -0.64967539 -0.66692304 -0.47970371 -0.0097394 ]\n",
      "Intersección:  0.4775155514191671\n",
      "Predicción\n",
      "[ 0.09768026  0.51292752  0.44837712 -0.1923766   0.04452557  0.09503343\n",
      "  0.05940325 -0.05336919  0.044103   -0.03861586]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mmora/anaconda3/envs/ml_env/lib/python3.9/site-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\")\n",
    "print( \"OLSR\")\n",
    "reg = linear_model.LinearRegression(normalize=False).fit(X_train, y_train)\n",
    "print(\"Atributos X: \", X.size)\n",
    "print( \"Score: \", reg.score(X, y) )\n",
    "print( \"Coeficientes: \", reg.coef_ )\n",
    "print( \"Intersección: \", reg.intercept_ )\n",
    "print(\"Predicción\")\n",
    "result = reg.predict(X_test)\n",
    "print(result[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.09768026  0.51292752  0.44837712 -0.1923766   0.04452557]\n",
      "[0. 0. 0. 0. 0.]\n",
      "[1, 1, 1, 0, 1, 1, 1, 0, 1, 0]\n",
      "0.6747572815533981\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html#sklearn.metrics.accuracy_score\n",
    "\n",
    "result_new = []\n",
    "print( result[0:5])\n",
    "print( y_test[0:5])\n",
    "for x in result:\n",
    "    if x >= 0:\n",
    "        result_new.append( 1 )\n",
    "    else:\n",
    "        result_new.append( 0 )\n",
    "print(result_new[0:10])\n",
    "print(accuracy_score(result_new, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "ML_LACV_ClasificaciónLineal.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
