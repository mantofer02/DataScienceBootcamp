{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b07b5b6f",
   "metadata": {},
   "source": [
    "## Escuela de Ingeniería en Computación, ITCR \n",
    "\n",
    "## Aprendizaje automático\n",
    "\n",
    "### Multilayer Perceptron (MLP) utilizando PyTorch\n",
    "\n",
    "\n",
    "**Profesora: María Auxiliadora Mora**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3250815",
   "metadata": {},
   "source": [
    "## Introducción\n",
    "\n",
    "Otros algoritmos que pueden ser utilizados para realizar regresión no lineal se listan a continuación:\n",
    "\n",
    "- Multivariate Adaptive Regression Splines (Regresión Spline Adaptativa Multivariante)\n",
    "- Multilayer Perceptron (Perceptrón Multicapa)\n",
    "\n",
    "\n",
    "\n",
    "### Multivariate Adaptive Regression Splines\n",
    "\n",
    "Splines de regresión adaptativa multivariante (MARS) es un método de regresión que modela múltiples no linealidades en los datos mediante funciones bisagra (funciones con una torcedura en ellas)(Friedman 1991).\n",
    "\n",
    "El algoritmo implica encontrar un conjunto de funciones lineales simples que, en conjunto, den como resultado el mejor rendimiento predictivo. De esta forma, MARS puede lograr un buen desempeño en problemas de regresión complejos con muchas variables de entrada y relaciones no lineales complejas.\n",
    "\n",
    "Por ejemplo, en la siguiente figura las líneas azules representan los valores predichos (y) en función de x utilizando diferentes algoritmos de regresión. (A) El enfoque de regresión lineal tradicional no captura ninguna no linealidad. (B) Polinomio de grado 2, (C) Polinomio de grado 3, (D) Función escalón que divide x en seis niveles categóricos.\n",
    "\n",
    "![](../imagenes/nonlinear-comparisons-1.png)\n",
    "\n",
    "Utilizando MARS, la siguiente figura muestra ejemplos de splines de regresión ajustados de uno (A), dos (B), tres (C) y cuatro (D) nudos. \n",
    "\n",
    "![](../imagenes/examples-of-multiple-knots-1.png)\n",
    "\n",
    "\n",
    "Imágenes de https://bradleyboehmke.github.io/HOML/mars.html\n",
    "\n",
    "### Multilayer Perceptron (MLP)\n",
    "Un Perceptrón Multicapa es un algoritmo de aprendizaje supervisado que aprende una función $f(x): \\mathbb{R}^{m}-> \\mathbb{R}^{o}$ por medio de ajustar los parámetros del modelo usando un conjunto de datos, donde m es el número de dimensiones para la entrada y o las dimensiones de la salida.\n",
    "\n",
    "Dado un conjunto de características y un objetivo, el perceptrón multicapa puede aprender un aproximador de función no lineal para realizar **clasificación o regresión**.  La figura siguiente muestra un MLP de una capa oculta con salida escalar.\n",
    "\n",
    "![](../imagenes/MLP.png)\n",
    "\n",
    "Perceptrón de una capa (Imagen scikit-learn.org)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbca5d75",
   "metadata": {},
   "source": [
    "## MLP Ejemplo\n",
    "\n",
    "El objetivo de esta sección es implementar un ejemplo básico de regresión utilizando la biblioteca de PyTorch para introducir a los estudiantes en el uso de redes neuronales.\n",
    "\n",
    "Los datos del ejemplo documentan características de viviendas de una región de Boston. El objetivo del modelo es predecir el valor de las soluciones de vivienda en función de los atributos de la propiedad [5]. \n",
    "\n",
    "El ejemplo está basado en [3] y utiliza otras funcionalidades descritas en la lista de referencias.\n",
    "\n",
    "El proceso de construir un modelo incluye las siguientes etapas: \n",
    "- Carga de datos\n",
    "- Revisión, limpeza de datos, selección de características a utilizar\n",
    "- Definición del modelo\n",
    "- Entrenamiento\n",
    "- Evaluación de modelos\n",
    "- Uso del modelo seleccionado\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7fad15fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install prettytable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f4e5eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# General libraries\n",
    "from numpy import vstack\n",
    "from numpy import sqrt\n",
    "from numpy import array\n",
    "from pandas import read_csv, notnull\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from prettytable import PrettyTable\n",
    "\n",
    "# Form Scikit-Learn\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Anomalies detection\n",
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "# From Pytorch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import random_split\n",
    "from torch import Tensor\n",
    "from torch.nn import Linear\n",
    "from torch.nn import Sigmoid\n",
    "from torch.nn import Module\n",
    "from torch.optim import SGD\n",
    "from torch.nn import MSELoss\n",
    "from torch.nn.init import xavier_uniform_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f78b52ec",
   "metadata": {},
   "source": [
    "### Carga, revisión, limpeza de datos y selección de características a utilizar \n",
    "\n",
    "El código para procesar muestras puede ser difícil de mantener. Idealmente, se desea que el conjunto de datos a utilizar esté desacoplado del código de entrenamiento del modelo para logar una mejor legibilidad y modularidad. \n",
    "\n",
    "PyTorch proporciona dos primitivas para cargado y uso de datos durante el entrenamiento y evaluación de modelos: torch.utils.data.DataLoader y torch.utils.data.Dataset. Estas clase  permiten manejar conjuntos de datos precargados y disponibles en Internet y datos locales. Dataset almacena las muestras y sus etiquetas correspondientes, y DataLoader envuelve un iterador alrededor de Dataset para permitir un fácil acceso a las muestras.\n",
    "\n",
    "El conjunto de datos a utilizar es \"The Boston Housing Dataset\" disponible en https://www.kaggle.com/code/prasadperera/the-boston-housing-dataset.  El conjunto de datos de viviendas de Boston se deriva de la información recopilada por el Servicio del Censo de los EE. UU. sobre viviendas en el área de Boston MA. A continuación se describen las columnas del conjunto de datos:\n",
    "\n",
    "  - CRIM - per capita crime rate by town\n",
    "  - ZN - proportion of residential land zoned for lots over 25,000 sq.ft.\n",
    "  - INDUS - proportion of non-retail business acres per town.\n",
    "  - CHAS - Charles River dummy variable (1 if tract bounds river; 0 otherwise)\n",
    "  - NOX - nitric oxides concentration (parts per 10 million)\n",
    "  - RM - average number of rooms per dwelling\n",
    "  - AGE - proportion of owner-occupied units built prior to 1940\n",
    "  - DIS - weighted distances to five Boston employment centres\n",
    "  - RAD - index of accessibility to radial highways\n",
    "  - TAX - full-value property-tax rate per $ $10,000$\n",
    "  - PTRATIO - pupil-teacher ratio by town\n",
    "  - B:  1000(Bk - 0.63)^2  where Bk is the proportion of blacks by town \n",
    "  - LSTAT: percentage of lower status of the population\n",
    "  - MEDV - Median value of owner-occupied homes in dolars 1000's\n",
    "\n",
    "Un ejemplo claro del racismo existente en algunos conjuntos de datos (observen la columna B)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5a0aa1a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        CRIM    ZN  INDUS  CHAS    NOX     RM   AGE     DIS  RAD    TAX  \\\n",
      "0    0.00632  18.0   2.31     0  0.538  6.575  65.2  4.0900    1  296.0   \n",
      "1    0.02731   0.0   7.07     0  0.469  6.421  78.9  4.9671    2  242.0   \n",
      "2    0.02729   0.0   7.07     0  0.469  7.185  61.1  4.9671    2  242.0   \n",
      "3    0.03237   0.0   2.18     0  0.458  6.998  45.8  6.0622    3  222.0   \n",
      "4    0.06905   0.0   2.18     0  0.458  7.147  54.2  6.0622    3  222.0   \n",
      "..       ...   ...    ...   ...    ...    ...   ...     ...  ...    ...   \n",
      "501  0.06263   0.0  11.93     0  0.573  6.593  69.1  2.4786    1  273.0   \n",
      "502  0.04527   0.0  11.93     0  0.573  6.120  76.7  2.2875    1  273.0   \n",
      "503  0.06076   0.0  11.93     0  0.573  6.976  91.0  2.1675    1  273.0   \n",
      "504  0.10959   0.0  11.93     0  0.573  6.794  89.3  2.3889    1  273.0   \n",
      "505  0.04741   0.0  11.93     0  0.573  6.030  80.8  2.5050    1  273.0   \n",
      "\n",
      "     PTRATIO       B  LSTAT  MEDV  \n",
      "0       15.3  396.90   4.98  24.0  \n",
      "1       17.8  396.90   9.14  21.6  \n",
      "2       17.8  392.83   4.03  34.7  \n",
      "3       18.7  394.63   2.94  33.4  \n",
      "4       18.7  396.90   5.33  36.2  \n",
      "..       ...     ...    ...   ...  \n",
      "501     21.0  391.99   9.67  22.4  \n",
      "502     21.0  396.90   9.08  20.6  \n",
      "503     21.0  396.90   5.64  23.9  \n",
      "504     21.0  393.45   6.48  22.0  \n",
      "505     21.0  396.90   7.88  11.9  \n",
      "\n",
      "[506 rows x 14 columns]\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Tamaño del dataset (506, 14)\n",
      "      CRIM    ZN  INDUS  CHAS    NOX     RM   AGE     DIS  RAD    TAX  \\\n",
      "0  0.00632  18.0   2.31     0  0.538  6.575  65.2  4.0900    1  296.0   \n",
      "1  0.02731   0.0   7.07     0  0.469  6.421  78.9  4.9671    2  242.0   \n",
      "2  0.02729   0.0   7.07     0  0.469  7.185  61.1  4.9671    2  242.0   \n",
      "3  0.03237   0.0   2.18     0  0.458  6.998  45.8  6.0622    3  222.0   \n",
      "4  0.06905   0.0   2.18     0  0.458  7.147  54.2  6.0622    3  222.0   \n",
      "\n",
      "   PTRATIO       B  LSTAT  MEDV  \n",
      "0     15.3  396.90   4.98  24.0  \n",
      "1     17.8  396.90   9.14  21.6  \n",
      "2     17.8  392.83   4.03  34.7  \n",
      "3     18.7  394.63   2.94  33.4  \n",
      "4     18.7  396.90   5.33  36.2  \n"
     ]
    }
   ],
   "source": [
    "# Explore data\n",
    "path = '../../Data/housing.csv'\n",
    "column_names = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT', 'MEDV']\n",
    "data = read_csv(path, header=None, names=column_names)\n",
    "print(data)\n",
    "print(type(data))\n",
    "print(\"Tamaño del dataset\", data.shape)\n",
    "print(data.head(5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "30d46e87",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# utility functions\n",
    "\n",
    "# dataset definition\n",
    "class CSVDataset(Dataset):\n",
    "    \"\"\" \n",
    "    load and preprocess the dataset. Extends the functionality of the class Dataset.\n",
    "    \"\"\"\n",
    "    def __init__(self, df):\n",
    "        \"\"\"\n",
    "        The __init__ function is run once when instantiating the Dataset object. \n",
    "        :param: df a dataframe with the data to be preprocess. \n",
    "        \"\"\"  \n",
    "        # store the inputs and outputs\n",
    "        self.X = df.values[:, :-1].astype('float32')\n",
    "        self.y = df.values[:, -1].astype('float32')\n",
    "        \n",
    "        #print(\"Tipo de datos\", type (self.X))\n",
    "        #print(self.X)\n",
    "        \n",
    "        # Scale the data\n",
    "        self.transformer = MinMaxScaler().fit(self.X)\n",
    "        self.X = self.transformer.transform(self.X)\n",
    "        \n",
    "        # ensure target has the right shape\n",
    "        self.y = self.y.reshape((len(self.y), 1))\n",
    "\n",
    "    # number of rows in the dataset\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        The __len__ function returns the number of samples in our dataset.\n",
    "        return: the length of the ndarray X.\n",
    "        \"\"\"\n",
    "        return len(self.X)\n",
    "\n",
    "    # get a row at an index\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        The __getitem__ function loads and returns \n",
    "        a sample from the dataset at the given index idx. \n",
    "        :param idx: index.\n",
    "        return: a list with the value in X[idx] and y[idx].\n",
    "        \"\"\"\n",
    "        return [self.X[idx], self.y[idx]]\n",
    "\n",
    "    # get indexes for train and test rows\n",
    "    def get_splits(self, n_test=0.33):\n",
    "        \"\"\"\n",
    "        Split the dataset into training and test data.\n",
    "        :param n_test: training data percentage\n",
    "        return the training and test data in four vectors (X_train, y_train, X_test, y_test).\n",
    "        \"\"\"\n",
    "        # determine sizes\n",
    "        test_size = round(n_test * len(self.X))\n",
    "        train_size = len(self.X) - test_size\n",
    "        # calculate the split\n",
    "        return random_split(self, [train_size, test_size])\n",
    "    \n",
    "    def scale_data(self, data_lst):\n",
    "        \"\"\"\n",
    "        Scale a data array.\n",
    "        :param data_array: records.\n",
    "        return the scaled data using the transform instance of MinMaxScaler.\n",
    "        \"\"\"\n",
    "        data_array = np.array(data_lst)\n",
    "        data_array = data_array.reshape(1, -1)\n",
    "        data_array = self.transformer.transform(data_array)\n",
    "        return(data_array)\n",
    "\n",
    "    \n",
    "def prepare_data(boston_df):\n",
    "    \"\"\"\n",
    "    Prepare the dataset.\n",
    "    :param path: path and name of the file .\n",
    "    \"\"\"\n",
    "    # load the dataset\n",
    "    dataset = CSVDataset(boston_df)\n",
    "    #print(\"dataset.X.len\", dataset.__len__())\n",
    "    #print(\"dataset.X\", dataset.X)\n",
    "    #print(\"dataset.y\", dataset.y)\n",
    "\n",
    "    # calculate split\n",
    "    train, test = dataset.get_splits()\n",
    "    # prepare data loaders\n",
    "    train_dl = DataLoader(train, batch_size=32, shuffle=True)\n",
    "    test_dl = DataLoader(test, batch_size=10, shuffle=False)\n",
    "    return train_dl, test_dl, dataset\n",
    "\n",
    "\n",
    "def evaluate_model(test_dl, model):\n",
    "    \"\"\"\n",
    "    Evaluates the model performance using Mean Squared Error (MSE).\n",
    "    :param: test_dt, test data.\n",
    "    :param: model, model to evaluate.\n",
    "    \"\"\"\n",
    "    predictions, actuals = list(), list()\n",
    "    for i, (inputs, targets) in enumerate(test_dl):\n",
    "        # evaluate the model on the test set\n",
    "        yhat = model(inputs)\n",
    "        # retrieve numpy array\n",
    "        yhat = yhat.detach().numpy()\n",
    "        actual = targets.numpy()\n",
    "        actual = actual.reshape((len(actual), 1))\n",
    "        # store\n",
    "        predictions.append(yhat)\n",
    "        actuals.append(actual)\n",
    "    # vstack: Stack arrays in sequence vertically (row wise).    \n",
    "    predictions, actuals = vstack(predictions), vstack(actuals)\n",
    "    # calculate the mse and mae\n",
    "    mse = mean_squared_error(actuals, predictions)\n",
    "    mae = mean_absolute_error(actuals, predictions)\n",
    "    return mse, mae\n",
    "\n",
    "\n",
    "def predict(row, model):\n",
    "    \"\"\"\n",
    "    Make a class prediction for one row of data\n",
    "    :param: row, data that will be used for prediction.\n",
    "    :param: model, the model to apply to the data. \n",
    "    \"\"\"\n",
    "    # scale the row\n",
    "    row = dataset.scale_data(row)\n",
    "    \n",
    "    # convert row to a tensor.\n",
    "    # Creating a tensor from a list of numpy.ndarrays is extremely slow \n",
    "    # then converting the list to a single numpy.ndarray is recomended.\n",
    "    row = Tensor(np.array([row]))\n",
    "    # make prediction\n",
    "    yhat = model(row)\n",
    "    # retrieve numpy array\n",
    "    yhat = yhat.detach().numpy()\n",
    "    return yhat\n",
    "\n",
    "def count_parameters(model):\n",
    "    \"\"\"\n",
    "    Display in a table the model parameters. The model must be a PyTorch DNN model.\n",
    "    :param: row, data that will be used for prediction.\n",
    "    :param: model, the model to apply to the data. \n",
    "    \"\"\"\n",
    "    table = PrettyTable([\"Mod name\", \"Parameters Listed\"])\n",
    "    t_params = 0\n",
    "    for name, parameter in model.named_parameters():\n",
    "        if not parameter.requires_grad: continue\n",
    "        param = parameter.numel()\n",
    "        table.add_row([name, param])\n",
    "        t_params+=param\n",
    "    print(table)\n",
    "    print(f\"Sum of trained paramters: {t_params}\")\n",
    "    return t_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c0e091b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model definition\n",
    "class MLP(Module):\n",
    "    \"\"\"\n",
    "    Class that implements the perceptron, it extends the nn.Module class.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_inputs, n_output, n_layer_1, n_layer_2):\n",
    "        \"\"\"\n",
    "        Defines the model's structure.\n",
    "        :param: n_inputs, amount of input data.\n",
    "        :param: n_output, amount of result elements. \n",
    "        :param: n_layer_1, number of neurons in layer 1.\n",
    "        :param: n_layer_2, number of neurons in layer 2.\n",
    "        \"\"\"\n",
    "        super(MLP, self).__init__()\n",
    "\n",
    "        # input to first hidden layer\n",
    "        self.hidden1 = Linear(n_inputs, n_layer_1)\n",
    "\n",
    "        # Initialization: Weights are scaled using a uniform distribution.\n",
    "        xavier_uniform_(self.hidden1.weight)\n",
    "        self.act1 = Sigmoid()\n",
    "\n",
    "        # second hidden layer\n",
    "        self.hidden2 = Linear(n_layer_1, n_layer_2)\n",
    "        xavier_uniform_(self.hidden2.weight)\n",
    "        self.act2 = Sigmoid()\n",
    "\n",
    "        # third hidden layer and output\n",
    "        self.hidden3 = Linear(n_layer_2, n_output)\n",
    "        xavier_uniform_(self.hidden3.weight)\n",
    "\n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        Forward run of the network using the data in X.\n",
    "        \"\"\"\n",
    "        # input to first hidden layer\n",
    "        X = self.hidden1(X)\n",
    "        X = self.act1(X)\n",
    "         # second hidden layer\n",
    "        X = self.hidden2(X)\n",
    "        X = self.act2(X)\n",
    "        # third hidden layer and output\n",
    "        X = self.hidden3(X)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "600f6108",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_model(train_dl, model):\n",
    "    \"\"\"\n",
    "    Train the model using the train data loader (train_dl).\n",
    "    :param: train_dl, training data accessed via a dataloader.\n",
    "    :param: model to be trained.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define the optimization parameters\n",
    "    # Mean Squared Error (MSE)\n",
    "    criterion = MSELoss()\n",
    "    # Stochastic gradient descent (SGD)\n",
    "    optimizer = SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "    # enumerate epochs\n",
    "    for epoch in range(100):\n",
    "        # enumerate mini batches\n",
    "        for i, (inputs, targets) in enumerate(train_dl):\n",
    "            # clear the gradients\n",
    "            optimizer.zero_grad()\n",
    "            # compute the model output\n",
    "            yhat = model(inputs)\n",
    "            # calculate loss\n",
    "            loss = criterion(yhat, targets)\n",
    "            # credit assignment\n",
    "            loss.backward()\n",
    "            # update model weights\n",
    "            optimizer.step()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "98abd469",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cantidad de datos de entrenamiento y prueba 339 167\n"
     ]
    }
   ],
   "source": [
    "# prepare the data\n",
    "boston_df = read_csv(path, header=None)\n",
    "\n",
    "train_dl, test_dl, dataset = prepare_data(boston_df)\n",
    "print(\"Cantidad de datos de entrenamiento y prueba\", len(train_dl.dataset), len(test_dl.dataset))\n",
    "\n",
    "# define the network\n",
    "n_inputs = data.shape[1] -1 \n",
    "n_layer_1 = 10\n",
    "n_layer_2 = 8\n",
    "n_output = 1\n",
    "model = MLP( n_inputs, n_output, n_layer_1, n_layer_2 )\n",
    "\n",
    "# train the model\n",
    "model = train_model(train_dl, model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "497dba3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamaño del dataset (incluyendo el target) (506, 14)\n",
      "==============================================\n",
      "Cantidad de parámetros del modelo\n",
      "+----------------+-------------------+\n",
      "|    Mod name    | Parameters Listed |\n",
      "+----------------+-------------------+\n",
      "| hidden1.weight |        130        |\n",
      "|  hidden1.bias  |         10        |\n",
      "| hidden2.weight |         80        |\n",
      "|  hidden2.bias  |         8         |\n",
      "| hidden3.weight |         8         |\n",
      "|  hidden3.bias  |         1         |\n",
      "+----------------+-------------------+\n",
      "Sum of trained paramters: 237\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "237"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To explore the model parameters\n",
    "print(\"Tamaño del dataset (incluyendo el target)\", data.shape)\n",
    "print(\"==============================================\")\n",
    "print(\"Cantidad de parámetros del modelo\")\n",
    "count_parameters(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f29cede",
   "metadata": {},
   "source": [
    "### Evaluación del modelo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "05c36591",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 11.391, RMSE: 3.375\n",
      "MAE: 2.510\n",
      "Predicted: 28.022\n"
     ]
    }
   ],
   "source": [
    "# evaluate the model using the Mean Squared Error (MSE)\n",
    "mse, mae = evaluate_model(test_dl, model)\n",
    "print('MSE: %.3f, RMSE: %.3f' % (mse, sqrt(mse)))\n",
    "print('MAE: %.3f' % (mae))\n",
    "\n",
    "\n",
    "# make a single prediction\n",
    "row = [0.00632,18.00,2.310,0,0.5380,6.5750,65.20,4.0900,1,296.0,15.30,396.90,4.98]\n",
    "yhat = predict(row, model)\n",
    "print('Predicted: %.3f' % yhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff8f8be",
   "metadata": {},
   "source": [
    "El MSE calcula una medida cuantitativa de **qué tan bien puede predecir el modelo la variable objetivo**. Un valor de MSE más bajo indica que el modelo está haciendo predicciones más precisas, mientras que un valor de MSE más alto indica que el modelo está haciendo predicciones menos precisas.\n",
    "\n",
    "Es importante tener en cuenta que el **MSE es sensible a los valores atípicos**, ya que elevar al cuadrado la diferencia entre los valores predichos y reales puede amplificar el efecto de los valores atípicos. Por lo tanto, es importante tener en cuenta otras métricas de rendimiento, como el error absoluto medio (MAE), si el conjunto de datos contiene valores atípicos.\n",
    "\n",
    "En resumen, MSE es una métrica de rendimiento ampliamente utilizada en el aprendizaje automático para problemas de regresión."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "799ee79b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.utils.data.dataloader.DataLoader object at 0x7f5119354f70>\n",
      "0\n",
      "Tamaño de las características torch.Size([10, 13])\n",
      "Features tensor([[3.2794e-02, 0.0000e+00, 7.0088e-01, 0.0000e+00, 4.5267e-01, 4.8668e-01,\n",
      "         9.2791e-01, 1.0492e-01, 1.7391e-01, 4.1221e-01, 2.2340e-01, 6.0477e-01,\n",
      "         2.2296e-01],\n",
      "        [6.1785e-04, 2.0000e-01, 1.0521e-01, 1.0000e+00, 1.1914e-01, 7.8253e-01,\n",
      "         4.8198e-01, 3.7122e-01, 1.7391e-01, 5.5344e-02, 2.4468e-01, 9.5000e-01,\n",
      "         3.5320e-02],\n",
      "        [3.0797e-05, 9.0000e-01, 9.2009e-02, 0.0000e+00, 3.0864e-02, 6.7580e-01,\n",
      "         1.8435e-01, 5.6177e-01, 0.0000e+00, 1.8702e-01, 2.8723e-01, 9.9450e-01,\n",
      "         1.6887e-01],\n",
      "        [1.3080e-03, 0.0000e+00, 2.3644e-01, 0.0000e+00, 1.2963e-01, 4.8055e-01,\n",
      "         3.8208e-01, 4.1751e-01, 8.6957e-02, 8.7786e-02, 5.6383e-01, 9.8106e-01,\n",
      "         2.1578e-01],\n",
      "        [1.0319e-01, 0.0000e+00, 6.4663e-01, 0.0000e+00, 6.4815e-01, 3.7842e-01,\n",
      "         1.0000e+00, 4.0993e-02, 1.0000e+00, 9.1412e-01, 8.0851e-01, 1.0000e+00,\n",
      "         6.0348e-01],\n",
      "        [2.6447e-04, 0.0000e+00, 6.3050e-02, 0.0000e+00, 1.5021e-01, 5.4972e-01,\n",
      "         5.7467e-01, 4.4854e-01, 8.6957e-02, 6.6794e-02, 6.4894e-01, 9.9299e-01,\n",
      "         9.6026e-02],\n",
      "        [1.3628e-03, 3.0000e-01, 1.6386e-01, 0.0000e+00, 8.8477e-02, 5.4263e-01,\n",
      "         5.0463e-02, 5.3705e-01, 2.1739e-01, 2.1565e-01, 4.2553e-01, 9.4405e-01,\n",
      "         9.5475e-02],\n",
      "        [2.3570e-04, 0.0000e+00, 2.4230e-01, 0.0000e+00, 1.7284e-01, 6.9439e-01,\n",
      "         5.9938e-01, 3.4896e-01, 4.3478e-02, 1.0496e-01, 5.5319e-01, 9.8974e-01,\n",
      "         6.3466e-02],\n",
      "        [9.7674e-05, 9.0000e-01, 2.7493e-02, 1.0000e+00, 3.2922e-02, 8.3579e-01,\n",
      "         2.2554e-01, 4.3243e-01, 0.0000e+00, 2.0992e-02, 1.0638e-01, 9.9652e-01,\n",
      "         3.9459e-02],\n",
      "        [1.3682e-02, 0.0000e+00, 7.0088e-01, 0.0000e+00, 4.5267e-01, 6.4802e-01,\n",
      "         9.7322e-01, 6.7992e-02, 1.7391e-01, 4.1221e-01, 2.2340e-01, 9.1560e-01,\n",
      "         7.8918e-02]])\n",
      "Targets tensor([[25.0000],\n",
      "        [46.0000],\n",
      "        [32.2000],\n",
      "        [21.2000],\n",
      "        [11.3000],\n",
      "        [28.7000],\n",
      "        [23.7000],\n",
      "        [34.7000],\n",
      "        [50.0000],\n",
      "        [41.3000]])\n",
      "1\n",
      "Tamaño de las características torch.Size([10, 13])\n",
      "Features tensor([[2.8302e-04, 9.5000e-01, 3.7023e-02, 0.0000e+00, 3.7037e-02, 6.5415e-01,\n",
      "         1.2770e-01, 5.9324e-01, 8.6957e-02, 4.1031e-01, 4.6809e-01, 1.0000e+00,\n",
      "         7.8091e-02],\n",
      "        [8.2567e-04, 4.0000e-01, 2.1811e-01, 0.0000e+00, 1.2757e-01, 5.5969e-01,\n",
      "         3.0072e-01, 2.7378e-01, 1.3043e-01, 1.2786e-01, 5.3191e-01, 1.0000e+00,\n",
      "         1.5066e-01],\n",
      "        [7.8861e-02, 0.0000e+00, 6.4663e-01, 0.0000e+00, 6.8519e-01, 4.6848e-01,\n",
      "         9.5160e-01, 6.7746e-02, 1.0000e+00, 9.1412e-01, 8.0851e-01, 8.0604e-01,\n",
      "         3.8549e-01],\n",
      "        [9.3764e-03, 0.0000e+00, 2.8152e-01, 0.0000e+00, 3.1481e-01, 3.9050e-01,\n",
      "         8.5273e-01, 3.0236e-01, 1.3043e-01, 2.2901e-01, 8.9362e-01, 7.6428e-01,\n",
      "         4.0784e-01],\n",
      "        [6.3618e-02, 0.0000e+00, 6.4663e-01, 0.0000e+00, 7.3045e-01, 5.0929e-01,\n",
      "         1.0000e+00, 7.9586e-02, 1.0000e+00, 9.1412e-01, 8.0851e-01, 9.9695e-01,\n",
      "         4.1004e-01],\n",
      "        [1.2131e-03, 0.0000e+00, 4.9230e-01, 1.0000e+00, 3.3951e-01, 5.3880e-01,\n",
      "         9.2173e-01, 2.0312e-01, 1.7391e-01, 1.6985e-01, 4.0426e-01, 9.9203e-01,\n",
      "         2.4200e-01],\n",
      "        [1.7134e-03, 0.0000e+00, 3.7940e-01, 0.0000e+00, 5.7613e-02, 4.5986e-01,\n",
      "         1.5036e-01, 3.7808e-01, 1.3043e-01, 2.2519e-01, 7.0213e-01, 9.4967e-01,\n",
      "         2.2489e-01],\n",
      "        [1.2220e-03, 0.0000e+00, 8.9076e-02, 0.0000e+00, 1.2346e-01, 4.9856e-01,\n",
      "         6.8692e-01, 2.1512e-01, 4.3478e-02, 1.6985e-01, 5.7447e-01, 9.8722e-01,\n",
      "         2.6518e-01],\n",
      "        [1.1139e-01, 0.0000e+00, 6.4663e-01, 0.0000e+00, 6.3374e-01, 4.3897e-01,\n",
      "         7.7137e-01, 3.3719e-02, 1.0000e+00, 9.1412e-01, 8.0851e-01, 8.5188e-01,\n",
      "         7.7925e-01],\n",
      "        [3.7627e-03, 0.0000e+00, 2.5367e-01, 0.0000e+00, 2.2222e-01, 5.4685e-01,\n",
      "         3.8311e-01, 3.2659e-01, 1.7391e-01, 1.9084e-01, 7.4468e-01, 1.0000e+00,\n",
      "         1.2114e-01]])\n",
      "Targets tensor([[34.9000],\n",
      "        [29.1000],\n",
      "        [14.2000],\n",
      "        [13.9000],\n",
      "        [18.4000],\n",
      "        [23.0000],\n",
      "        [21.7000],\n",
      "        [21.4000],\n",
      "        [ 6.3000],\n",
      "        [25.0000]])\n",
      "2\n",
      "Tamaño de las características torch.Size([10, 13])\n",
      "Features tensor([[7.3867e-03, 2.0000e-01, 1.2867e-01, 0.0000e+00, 5.3909e-01, 7.2274e-01,\n",
      "         1.0000e+00, 6.9565e-02, 1.7391e-01, 1.4695e-01, 4.2553e-02, 9.6568e-01,\n",
      "         1.6722e-01],\n",
      "        [5.7356e-04, 0.0000e+00, 1.4773e-01, 0.0000e+00, 1.3169e-01, 5.8804e-01,\n",
      "         5.4789e-01, 3.0082e-01, 8.6957e-02, 1.1450e-01, 6.2766e-01, 9.8840e-01,\n",
      "         1.3245e-01],\n",
      "        [3.1202e-03, 0.0000e+00, 2.5367e-01, 0.0000e+00, 2.2222e-01, 4.1138e-01,\n",
      "         7.3532e-01, 3.2659e-01, 1.7391e-01, 1.9084e-01, 7.4468e-01, 9.8545e-01,\n",
      "         2.7621e-01],\n",
      "        [5.4016e-02, 0.0000e+00, 6.4663e-01, 0.0000e+00, 6.7490e-01, 6.0165e-01,\n",
      "         8.9701e-01, 1.3348e-01, 1.0000e+00, 9.1412e-01, 8.0851e-01, 6.4277e-01,\n",
      "         4.0535e-01],\n",
      "        [4.1194e-04, 5.2500e-01, 1.7815e-01, 0.0000e+00, 4.1152e-02, 5.7559e-01,\n",
      "         2.0597e-01, 5.6267e-01, 2.1739e-01, 2.0229e-01, 4.2553e-01, 9.3651e-01,\n",
      "         2.1468e-01],\n",
      "        [1.0686e-01, 0.0000e+00, 6.4663e-01, 0.0000e+00, 6.7490e-01, 6.0682e-01,\n",
      "         9.3924e-01, 1.2426e-01, 1.0000e+00, 9.1412e-01, 8.0851e-01, 1.6037e-02,\n",
      "         4.6854e-01],\n",
      "        [7.0001e-04, 0.0000e+00, 8.9076e-02, 0.0000e+00, 1.2346e-01, 7.3865e-01,\n",
      "         6.1380e-01, 2.1512e-01, 4.3478e-02, 1.6985e-01, 5.7447e-01, 1.0000e+00,\n",
      "         1.2307e-01],\n",
      "        [1.4242e-02, 0.0000e+00, 7.0088e-01, 1.0000e+00, 4.5267e-01, 5.1523e-01,\n",
      "         9.2379e-01, 6.0817e-02, 1.7391e-01, 4.1221e-01, 2.2340e-01, 8.5380e-01,\n",
      "         1.0403e-01],\n",
      "        [8.1825e-05, 7.5000e-01, 1.2977e-01, 0.0000e+00, 5.1440e-02, 4.4587e-01,\n",
      "         4.6035e-01, 5.6289e-01, 8.6957e-02, 5.3817e-01, 9.0426e-01, 1.0000e+00,\n",
      "         3.6065e-01],\n",
      "        [4.5640e-03, 0.0000e+00, 2.1041e-01, 0.0000e+00, 2.4486e-01, 6.9017e-01,\n",
      "         7.9300e-01, 1.8970e-01, 3.0435e-01, 2.2901e-01, 5.1064e-01, 9.3741e-01,\n",
      "         1.2776e-01]])\n",
      "Targets tensor([[36.0000],\n",
      "        [26.6000],\n",
      "        [18.5000],\n",
      "        [16.4000],\n",
      "        [24.8000],\n",
      "        [14.9000],\n",
      "        [33.2000],\n",
      "        [27.0000],\n",
      "        [18.9000],\n",
      "        [31.6000]])\n",
      "3\n",
      "Tamaño de las características torch.Size([10, 13])\n",
      "Features tensor([[3.7512e-03, 0.0000e+00, 7.8556e-01, 0.0000e+00, 4.9177e-01, 5.5509e-01,\n",
      "         9.8867e-01, 8.9925e-02, 1.3043e-01, 4.7710e-01, 9.1489e-01, 9.9531e-01,\n",
      "         2.9994e-01],\n",
      "        [2.4778e-01, 0.0000e+00, 6.4663e-01, 0.0000e+00, 7.3045e-01, 4.3246e-01,\n",
      "         9.2173e-01, 6.6983e-02, 1.0000e+00, 9.1412e-01, 8.0851e-01, 9.8626e-01,\n",
      "         5.6236e-01],\n",
      "        [1.4929e-03, 0.0000e+00, 1.3160e-01, 0.0000e+00, 2.5720e-01, 3.8532e-01,\n",
      "         8.8157e-01, 1.3336e-01, 1.7391e-01, 2.0802e-01, 4.2553e-01, 1.0000e+00,\n",
      "         3.5762e-01],\n",
      "        [5.2731e-03, 0.0000e+00, 3.4604e-01, 0.0000e+00, 3.2716e-01, 4.8898e-01,\n",
      "         5.7570e-01, 2.6119e-01, 1.3043e-01, 2.2328e-01, 6.1702e-01, 9.9831e-01,\n",
      "         3.0353e-01],\n",
      "        [3.1986e-03, 0.0000e+00, 7.8556e-01, 0.0000e+00, 4.9177e-01, 5.0067e-01,\n",
      "         9.3409e-01, 4.3858e-02, 1.3043e-01, 4.7710e-01, 9.1489e-01, 9.7776e-01,\n",
      "         6.1893e-01],\n",
      "        [7.0605e-02, 0.0000e+00, 6.4663e-01, 0.0000e+00, 7.3045e-01, 5.3267e-01,\n",
      "         9.6292e-01, 8.5697e-02, 1.0000e+00, 9.1412e-01, 8.0851e-01, 8.0107e-01,\n",
      "         4.4316e-01],\n",
      "        [3.1840e-03, 0.0000e+00, 3.3834e-01, 0.0000e+00, 4.1152e-01, 3.5045e-01,\n",
      "         7.2091e-01, 1.5177e-01, 2.1739e-01, 3.8931e-01, 7.0213e-01, 1.0000e+00,\n",
      "         5.3560e-01],\n",
      "        [3.1853e-04, 3.5000e-01, 2.0528e-01, 0.0000e+00, 1.0885e-01, 4.7327e-01,\n",
      "         2.1009e-01, 5.0115e-01, 0.0000e+00, 2.2328e-01, 4.5745e-01, 9.1263e-01,\n",
      "         1.6832e-01],\n",
      "        [2.7367e-03, 0.0000e+00, 7.8556e-01, 0.0000e+00, 4.9177e-01, 4.3993e-01,\n",
      "         9.8146e-01, 4.9014e-02, 1.3043e-01, 4.7710e-01, 9.1489e-01, 9.8775e-01,\n",
      "         5.4056e-01],\n",
      "        [3.6361e-03, 0.0000e+00, 7.8556e-01, 0.0000e+00, 4.9177e-01, 4.3322e-01,\n",
      "         9.5263e-01, 1.2188e-01, 1.3043e-01, 4.7710e-01, 9.1489e-01, 9.7930e-01,\n",
      "         3.6700e-01]])\n",
      "Targets tensor([[19.2000],\n",
      "        [10.5000],\n",
      "        [23.1000],\n",
      "        [21.0000],\n",
      "        [14.0000],\n",
      "        [14.9000],\n",
      "        [19.7000],\n",
      "        [19.4000],\n",
      "        [13.3000],\n",
      "        [18.4000]])\n",
      "4\n",
      "Tamaño de las características torch.Size([10, 13])\n",
      "Features tensor([[3.8561e-03, 0.0000e+00, 3.4604e-01, 0.0000e+00, 3.2716e-01, 4.6197e-01,\n",
      "         7.6004e-01, 1.7941e-01, 1.3043e-01, 2.2328e-01, 6.1702e-01, 9.9834e-01,\n",
      "         2.2737e-01],\n",
      "        [1.2170e-03, 2.0000e-01, 2.3827e-01, 0.0000e+00, 1.6255e-01, 5.7042e-01,\n",
      "         5.7467e-01, 2.5352e-01, 8.6957e-02, 6.8702e-02, 6.3830e-01, 9.9511e-01,\n",
      "         1.6556e-01],\n",
      "        [7.4729e-02, 0.0000e+00, 6.4663e-01, 0.0000e+00, 6.7490e-01, 5.2807e-01,\n",
      "         8.2492e-01, 1.4593e-01, 1.0000e+00, 9.1412e-01, 8.0851e-01, 1.0000e+00,\n",
      "         3.3830e-01],\n",
      "        [5.4984e-02, 0.0000e+00, 6.4663e-01, 0.0000e+00, 5.0617e-01, 2.6998e-01,\n",
      "         1.0000e+00, 1.8451e-02, 1.0000e+00, 9.1412e-01, 8.0851e-01, 9.4609e-01,\n",
      "         4.2219e-02],\n",
      "        [1.7052e-01, 0.0000e+00, 6.4663e-01, 0.0000e+00, 7.3045e-01, 4.9646e-01,\n",
      "         1.0000e+00, 7.1347e-02, 1.0000e+00, 9.1412e-01, 8.0851e-01, 2.2694e-02,\n",
      "         6.8212e-01],\n",
      "        [5.2405e-02, 0.0000e+00, 6.4663e-01, 0.0000e+00, 6.7490e-01, 4.6273e-01,\n",
      "         8.7539e-01, 1.3195e-01, 1.0000e+00, 9.1412e-01, 8.0851e-01, 2.5619e-02,\n",
      "         4.7682e-01],\n",
      "        [1.4561e-03, 0.0000e+00, 3.7133e-01, 1.0000e+00, 2.1399e-01, 4.7959e-01,\n",
      "         5.7878e-01, 2.8277e-01, 1.3043e-01, 1.7176e-01, 6.3830e-01, 9.6071e-01,\n",
      "         3.5679e-01],\n",
      "        [1.9700e-03, 0.0000e+00, 2.5367e-01, 0.0000e+00, 2.2222e-01, 5.3938e-01,\n",
      "         5.2935e-01, 3.1016e-01, 1.7391e-01, 1.9084e-01, 7.4468e-01, 1.0000e+00,\n",
      "         1.4183e-01],\n",
      "        [2.4509e-03, 0.0000e+00, 3.3834e-01, 0.0000e+00, 4.1152e-01, 4.7250e-01,\n",
      "         7.9094e-01, 1.2445e-01, 2.1739e-01, 3.8931e-01, 7.0213e-01, 1.0000e+00,\n",
      "         3.4768e-01],\n",
      "        [4.1643e-04, 2.1000e-01, 1.8988e-01, 0.0000e+00, 1.1111e-01, 4.8937e-01,\n",
      "         6.1895e-01, 5.1697e-01, 1.3043e-01, 1.0687e-01, 4.4681e-01, 9.9261e-01,\n",
      "         2.1247e-01]])\n",
      "Targets tensor([[20.3000],\n",
      "        [24.4000],\n",
      "        [19.5000],\n",
      "        [50.0000],\n",
      "        [ 8.7000],\n",
      "        [12.7000],\n",
      "        [24.4000],\n",
      "        [23.1000],\n",
      "        [16.8000],\n",
      "        [20.5000]])\n",
      "5\n",
      "Tamaño de las características torch.Size([10, 13])\n",
      "Features tensor([[3.6406e-04, 5.2500e-01, 1.7815e-01, 0.0000e+00, 4.1152e-02, 5.0738e-01,\n",
      "         2.9248e-01, 5.6267e-01, 2.1739e-01, 2.0229e-01, 4.2553e-01, 1.0000e+00,\n",
      "         1.4928e-01],\n",
      "        [1.0529e-03, 3.4000e-01, 2.0638e-01, 0.0000e+00, 9.8765e-02, 6.5549e-01,\n",
      "         1.5242e-01, 3.9667e-01, 2.6087e-01, 2.7099e-01, 3.7234e-01, 9.8369e-01,\n",
      "         8.6369e-02],\n",
      "        [3.3528e-04, 8.0000e-01, 1.6459e-01, 0.0000e+00, 5.3498e-02, 5.8804e-01,\n",
      "         2.1112e-01, 3.6257e-01, 1.3043e-01, 1.1069e-01, 7.0213e-01, 1.0000e+00,\n",
      "         8.1954e-02],\n",
      "        [1.6747e-02, 0.0000e+00, 7.0088e-01, 0.0000e+00, 1.0000e+00, 3.5313e-01,\n",
      "         1.0000e+00, 4.2012e-02, 1.7391e-01, 4.1221e-01, 2.2340e-01, 8.6056e-01,\n",
      "         3.1871e-01],\n",
      "        [9.0474e-02, 0.0000e+00, 6.4663e-01, 0.0000e+00, 4.0947e-01, 3.5754e-01,\n",
      "         9.5263e-01, 1.1823e-01, 1.0000e+00, 9.1412e-01, 8.0851e-01, 8.8824e-01,\n",
      "         4.5281e-01],\n",
      "        [3.2775e-04, 8.0000e-01, 1.1657e-01, 0.0000e+00, 1.4403e-02, 4.4357e-01,\n",
      "         1.6684e-01, 7.3573e-01, 0.0000e+00, 2.4427e-01, 4.0426e-01, 9.9566e-01,\n",
      "         2.0751e-01],\n",
      "        [5.6356e-04, 0.0000e+00, 4.5345e-01, 0.0000e+00, 1.0700e-01, 5.1178e-01,\n",
      "         5.2317e-01, 3.5324e-01, 1.7391e-01, 4.0267e-01, 6.4894e-01, 9.7352e-01,\n",
      "         2.9277e-01],\n",
      "        [1.7510e-03, 2.0000e-01, 2.3827e-01, 0.0000e+00, 1.6255e-01, 5.1332e-01,\n",
      "         1.3800e-01, 3.0003e-01, 8.6957e-02, 6.8702e-02, 6.3830e-01, 1.0000e+00,\n",
      "         1.3411e-01],\n",
      "        [2.8144e-01, 0.0000e+00, 6.4663e-01, 0.0000e+00, 6.3374e-01, 4.6484e-01,\n",
      "         1.0000e+00, 4.1757e-02, 1.0000e+00, 9.1412e-01, 8.0851e-01, 1.0000e+00,\n",
      "         6.9095e-01],\n",
      "        [7.6334e-01, 0.0000e+00, 6.4663e-01, 0.0000e+00, 6.3374e-01, 4.0659e-01,\n",
      "         1.0000e+00, 2.6898e-02, 1.0000e+00, 9.1412e-01, 8.0851e-01, 9.6992e-01,\n",
      "         5.8637e-01]])\n",
      "Targets tensor([[23.2000],\n",
      "        [33.1000],\n",
      "        [27.9000],\n",
      "        [19.6000],\n",
      "        [13.8000],\n",
      "        [20.9000],\n",
      "        [21.2000],\n",
      "        [25.2000],\n",
      "        [ 5.6000],\n",
      "        [ 5.0000]])\n",
      "6\n",
      "Tamaño de las características torch.Size([10, 13])\n",
      "Features tensor([[5.2152e-05, 5.5000e-01, 6.5616e-02, 0.0000e+00, 8.2305e-03, 5.5413e-01,\n",
      "         2.9866e-01, 5.6177e-01, 0.0000e+00, 2.1565e-01, 2.8723e-01, 9.9450e-01,\n",
      "         1.7936e-01],\n",
      "        [2.8703e-03, 0.0000e+00, 3.4604e-01, 0.0000e+00, 3.2716e-01, 4.7174e-01,\n",
      "         9.0113e-01, 1.5499e-01, 1.3043e-01, 2.2328e-01, 6.1702e-01, 9.9849e-01,\n",
      "         2.7566e-01],\n",
      "        [3.5487e-02, 0.0000e+00, 6.4663e-01, 0.0000e+00, 5.5556e-01, 4.2115e-01,\n",
      "         4.6653e-01, 1.7613e-01, 1.0000e+00, 9.1412e-01, 8.0851e-01, 8.4240e-01,\n",
      "         3.4216e-01],\n",
      "        [1.1217e-03, 3.0000e-01, 1.6386e-01, 0.0000e+00, 8.8477e-02, 4.8553e-01,\n",
      "         6.4058e-01, 4.7345e-01, 2.1739e-01, 2.1565e-01, 4.2553e-01, 9.9425e-01,\n",
      "         2.9443e-01],\n",
      "        [2.5785e-02, 0.0000e+00, 7.0088e-01, 0.0000e+00, 4.5267e-01, 5.2845e-01,\n",
      "         9.5984e-01, 8.8243e-02, 1.7391e-01, 4.1221e-01, 2.2340e-01, 7.4832e-01,\n",
      "         2.5855e-01],\n",
      "        [1.5156e-02, 0.0000e+00, 2.8152e-01, 0.0000e+00, 3.1481e-01, 4.8113e-01,\n",
      "         1.0000e+00, 2.7693e-01, 1.3043e-01, 2.2901e-01, 8.9362e-01, 9.4914e-01,\n",
      "         3.1209e-01],\n",
      "        [2.7824e-03, 0.0000e+00, 2.3644e-01, 0.0000e+00, 1.2963e-01, 3.5217e-01,\n",
      "         9.5160e-01, 4.3107e-01, 8.6957e-02, 8.7786e-02, 5.6383e-01, 1.0000e+00,\n",
      "         8.0243e-01],\n",
      "        [3.1834e-03, 0.0000e+00, 3.7133e-01, 0.0000e+00, 2.1399e-01, 3.5467e-01,\n",
      "         7.1061e-02, 2.2351e-01, 1.3043e-01, 1.7176e-01, 6.3830e-01, 8.7904e-01,\n",
      "         7.6766e-01],\n",
      "        [2.0727e-03, 2.2000e-01, 1.9795e-01, 0.0000e+00, 9.4650e-02, 6.0491e-01,\n",
      "         1.5036e-01, 6.0898e-01, 2.6087e-01, 2.7290e-01, 6.9149e-01, 9.9203e-01,\n",
      "         1.3328e-01],\n",
      "        [1.2478e-01, 0.0000e+00, 6.4663e-01, 0.0000e+00, 5.8230e-01, 2.5771e-01,\n",
      "         1.0000e+00, 4.0557e-03, 1.0000e+00, 9.1412e-01, 8.0851e-01, 1.0000e+00,\n",
      "         9.1170e-01]])\n",
      "Targets tensor([[22.0000],\n",
      "        [19.4000],\n",
      "        [19.9000],\n",
      "        [20.1000],\n",
      "        [23.8000],\n",
      "        [14.5000],\n",
      "        [14.4000],\n",
      "        [23.7000],\n",
      "        [26.2000],\n",
      "        [13.8000]])\n",
      "7\n",
      "Tamaño de las características torch.Size([10, 13])\n",
      "Features tensor([[1.0923e-01, 0.0000e+00, 6.4663e-01, 0.0000e+00, 7.3045e-01, 5.4512e-01,\n",
      "         9.7116e-01, 8.5069e-02, 1.0000e+00, 9.1412e-01, 8.0851e-01, 9.7241e-01,\n",
      "         4.9089e-01],\n",
      "        [3.5062e-03, 0.0000e+00, 3.4604e-01, 0.0000e+00, 3.2716e-01, 4.5085e-01,\n",
      "         8.2698e-01, 2.6089e-01, 1.3043e-01, 2.2328e-01, 6.1702e-01, 9.8437e-01,\n",
      "         4.5806e-01],\n",
      "        [9.8340e-03, 0.0000e+00, 7.8556e-01, 0.0000e+00, 4.9177e-01, 3.9778e-01,\n",
      "         9.4542e-01, 7.7322e-02, 1.3043e-01, 4.7710e-01, 9.1489e-01, 1.0000e+00,\n",
      "         4.5833e-01],\n",
      "        [1.0242e-03, 0.0000e+00, 2.0161e-01, 0.0000e+00, 2.3457e-01, 4.3687e-01,\n",
      "         6.0247e-01, 2.0445e-01, 1.7391e-01, 1.7557e-01, 7.0213e-01, 9.5123e-01,\n",
      "         2.6711e-01],\n",
      "        [4.7061e-04, 8.0000e-01, 1.1657e-01, 0.0000e+00, 1.4403e-02, 4.8802e-01,\n",
      "         2.9969e-01, 7.3573e-01, 0.0000e+00, 2.4427e-01, 4.0426e-01, 9.8989e-01,\n",
      "         1.3355e-01],\n",
      "        [5.4682e-02, 0.0000e+00, 6.4663e-01, 0.0000e+00, 4.7119e-01, 5.6007e-01,\n",
      "         9.3409e-01, 1.0691e-01, 1.0000e+00, 9.1412e-01, 8.0851e-01, 9.9826e-01,\n",
      "         4.6772e-01],\n",
      "        [8.7839e-04, 0.0000e+00, 1.3160e-01, 0.0000e+00, 2.5720e-01, 4.4031e-01,\n",
      "         6.7765e-01, 1.4298e-01, 1.7391e-01, 2.0802e-01, 4.2553e-01, 9.9075e-01,\n",
      "         2.1827e-01],\n",
      "        [2.7613e-03, 0.0000e+00, 3.7133e-01, 0.0000e+00, 2.1399e-01, 4.2575e-01,\n",
      "         7.1885e-01, 2.9329e-01, 1.3043e-01, 1.7176e-01, 6.3830e-01, 9.8116e-01,\n",
      "         4.5061e-01],\n",
      "        [1.3614e-03, 0.0000e+00, 2.3644e-01, 0.0000e+00, 1.2963e-01, 6.1487e-01,\n",
      "         0.0000e+00, 4.1751e-01, 8.6957e-02, 8.7786e-02, 5.6383e-01, 9.7103e-01,\n",
      "         8.5817e-02],\n",
      "        [2.3592e-04, 0.0000e+00, 2.4230e-01, 0.0000e+00, 1.7284e-01, 5.4800e-01,\n",
      "         7.8270e-01, 3.4896e-01, 4.3478e-02, 1.0496e-01, 5.5319e-01, 1.0000e+00,\n",
      "         2.0447e-01]])\n",
      "Targets tensor([[17.1000],\n",
      "        [17.8000],\n",
      "        [14.3000],\n",
      "        [20.0000],\n",
      "        [21.9000],\n",
      "        [16.7000],\n",
      "        [22.6000],\n",
      "        [22.5000],\n",
      "        [26.6000],\n",
      "        [21.6000]])\n",
      "8\n",
      "Tamaño de las características torch.Size([10, 13])\n",
      "Features tensor([[5.5862e-04, 0.0000e+00, 7.3314e-02, 0.0000e+00, 2.1193e-01, 8.1816e-01,\n",
      "         5.2214e-01, 1.8820e-01, 8.6957e-02, 1.1450e-02, 5.5319e-01, 9.8923e-01,\n",
      "         7.5055e-02],\n",
      "        [6.0041e-03, 2.0000e-01, 1.2867e-01, 0.0000e+00, 3.9095e-01, 7.4899e-01,\n",
      "         5.1184e-01, 1.5844e-01, 1.7391e-01, 1.4695e-01, 4.2553e-02, 9.8336e-01,\n",
      "         3.9459e-02],\n",
      "        [9.0705e-05, 6.0000e-01, 9.0543e-02, 0.0000e+00, 3.2922e-02, 5.8306e-01,\n",
      "         1.6375e-01, 4.6286e-01, 0.0000e+00, 1.4885e-01, 3.1915e-01, 9.4906e-01,\n",
      "         7.3124e-02],\n",
      "        [2.7789e-03, 0.0000e+00, 3.4604e-01, 0.0000e+00, 3.2716e-01, 4.1081e-01,\n",
      "         7.7034e-01, 2.5602e-01, 1.3043e-01, 2.2328e-01, 6.1702e-01, 9.9879e-01,\n",
      "         2.6959e-01],\n",
      "        [9.4819e-04, 4.5000e-01, 1.0924e-01, 0.0000e+00, 1.0700e-01, 6.4955e-01,\n",
      "         1.9156e-01, 4.8652e-01, 1.7391e-01, 4.0267e-01, 2.7660e-01, 9.5154e-01,\n",
      "         9.2991e-02],\n",
      "        [1.5084e-03, 0.0000e+00, 3.7133e-01, 0.0000e+00, 2.1399e-01, 5.3918e-01,\n",
      "         3.0278e-01, 2.5605e-01, 1.3043e-01, 1.7176e-01, 6.3830e-01, 9.7204e-01,\n",
      "         2.1109e-01],\n",
      "        [1.3535e-01, 0.0000e+00, 6.4663e-01, 0.0000e+00, 4.7119e-01, 3.9988e-01,\n",
      "         8.7230e-01, 7.4712e-02, 1.0000e+00, 9.1412e-01, 8.0851e-01, 7.3435e-01,\n",
      "         3.4134e-01],\n",
      "        [2.8417e-03, 0.0000e+00, 7.8556e-01, 0.0000e+00, 4.9177e-01, 4.0851e-01,\n",
      "         9.5881e-01, 5.9899e-02, 1.3043e-01, 4.7710e-01, 9.1489e-01, 9.8792e-01,\n",
      "         4.2660e-01],\n",
      "        [2.0784e-01, 0.0000e+00, 6.4663e-01, 0.0000e+00, 5.8230e-01, 1.1056e-01,\n",
      "         1.0000e+00, 6.7291e-04, 1.0000e+00, 9.1412e-01, 8.0851e-01, 1.0000e+00,\n",
      "         1.0000e+00],\n",
      "        [9.0278e-04, 4.5000e-01, 1.0924e-01, 0.0000e+00, 1.0700e-01, 6.9304e-01,\n",
      "         2.4099e-01, 4.8652e-01, 1.7391e-01, 4.0267e-01, 2.7660e-01, 9.8384e-01,\n",
      "         3.1457e-02]])\n",
      "Targets tensor([[50.0000],\n",
      "        [43.5000],\n",
      "        [29.1000],\n",
      "        [16.2000],\n",
      "        [37.0000],\n",
      "        [28.1000],\n",
      "        [20.8000],\n",
      "        [16.2000],\n",
      "        [13.8000],\n",
      "        [36.4000]])\n",
      "9\n",
      "Tamaño de las características torch.Size([10, 13])\n",
      "Features tensor([[1.0778e-01, 0.0000e+00, 6.4663e-01, 0.0000e+00, 6.3374e-01, 5.4474e-01,\n",
      "         1.0000e+00, 4.6322e-02, 1.0000e+00, 9.1412e-01, 8.0851e-01, 9.4758e-01,\n",
      "         5.1269e-01],\n",
      "        [5.6818e-03, 0.0000e+00, 2.1041e-01, 0.0000e+00, 2.5103e-01, 7.2753e-01,\n",
      "         7.0752e-01, 2.7448e-01, 3.0435e-01, 2.2901e-01, 5.1064e-01, 9.8278e-01,\n",
      "         8.2781e-02],\n",
      "        [4.8882e-04, 2.1000e-01, 1.8988e-01, 0.0000e+00, 1.1111e-01, 4.6695e-01,\n",
      "         1.9053e-01, 5.1697e-01, 1.3043e-01, 1.0687e-01, 4.4681e-01, 1.0000e+00,\n",
      "         1.8488e-01],\n",
      "        [1.4079e-03, 0.0000e+00, 3.5007e-01, 0.0000e+00, 3.3333e-01, 5.0105e-01,\n",
      "         7.1679e-01, 1.4554e-01, 2.1739e-01, 4.6756e-01, 5.5319e-01, 9.9092e-01,\n",
      "         2.8449e-01],\n",
      "        [1.3999e-02, 0.0000e+00, 2.8152e-01, 0.0000e+00, 3.1481e-01, 3.8494e-01,\n",
      "         9.8043e-01, 2.4264e-01, 1.3043e-01, 2.2901e-01, 8.9362e-01, 9.4874e-01,\n",
      "         5.3228e-01],\n",
      "        [8.9512e-03, 0.0000e+00, 2.8152e-01, 0.0000e+00, 3.1481e-01, 3.6310e-01,\n",
      "         3.4706e-01, 2.4251e-01, 1.3043e-01, 2.2901e-01, 8.9362e-01, 7.2790e-01,\n",
      "         2.7483e-01],\n",
      "        [6.1189e-04, 0.0000e+00, 4.2045e-01, 0.0000e+00, 3.8683e-01, 6.5434e-01,\n",
      "         9.0731e-01, 9.4381e-02, 0.0000e+00, 1.6412e-01, 8.9362e-01, 1.0000e+00,\n",
      "         1.0789e-01],\n",
      "        [2.9551e-02, 0.0000e+00, 3.4604e-01, 0.0000e+00, 3.2716e-01, 2.7055e-01,\n",
      "         3.5942e-01, 1.2638e-01, 1.3043e-01, 2.2328e-01, 6.1702e-01, 8.8287e-01,\n",
      "         3.0105e-01],\n",
      "        [1.5994e-01, 0.0000e+00, 6.4663e-01, 0.0000e+00, 6.3374e-01, 5.3305e-01,\n",
      "         1.0000e+00, 4.0420e-02, 1.0000e+00, 9.1412e-01, 8.0851e-01, 1.0000e+00,\n",
      "         5.1297e-01],\n",
      "        [1.8557e-03, 0.0000e+00, 2.3644e-01, 0.0000e+00, 1.2963e-01, 4.0640e-01,\n",
      "         3.1823e-01, 3.6108e-01, 8.6957e-02, 8.7786e-02, 5.6383e-01, 1.0000e+00,\n",
      "         2.3400e-01]])\n",
      "Targets tensor([[12.1000],\n",
      "        [31.5000],\n",
      "        [23.4000],\n",
      "        [21.2000],\n",
      "        [13.6000],\n",
      "        [20.2000],\n",
      "        [23.9000],\n",
      "        [16.1000],\n",
      "        [ 7.2000],\n",
      "        [19.3000]])\n",
      "10\n",
      "Tamaño de las características torch.Size([10, 13])\n",
      "Features tensor([[2.1136e-01, 0.0000e+00, 6.4663e-01, 0.0000e+00, 4.3621e-01, 2.0445e-01,\n",
      "         1.0000e+00, 3.8584e-02, 1.0000e+00, 9.1412e-01, 8.0851e-01, 7.1789e-02,\n",
      "         9.0066e-01],\n",
      "        [4.5115e-03, 0.0000e+00, 2.1041e-01, 1.0000e+00, 2.5103e-01, 4.9875e-01,\n",
      "         9.1040e-01, 1.7445e-01, 3.0435e-01, 2.2901e-01, 5.1064e-01, 9.9581e-01,\n",
      "         5.4443e-01],\n",
      "        [8.4186e-05, 8.0000e-01, 0.0000e+00, 0.0000e+00, 7.6132e-02, 8.2660e-01,\n",
      "         2.9969e-01, 4.1092e-01, 1.3043e-01, 1.2977e-01, 1.9149e-01, 9.9327e-01,\n",
      "         3.4216e-02],\n",
      "        [1.3782e-02, 0.0000e+00, 2.8152e-01, 0.0000e+00, 3.1481e-01, 4.9454e-01,\n",
      "         9.1452e-01, 2.5892e-01, 1.3043e-01, 2.2901e-01, 8.9362e-01, 1.0000e+00,\n",
      "         4.6882e-01],\n",
      "        [2.4255e-03, 0.0000e+00, 3.5007e-01, 0.0000e+00, 3.3333e-01, 4.8496e-01,\n",
      "         9.5263e-01, 1.2898e-01, 2.1739e-01, 4.6756e-01, 5.5319e-01, 1.0000e+00,\n",
      "         4.2384e-01],\n",
      "        [2.6672e-02, 0.0000e+00, 7.0088e-01, 0.0000e+00, 1.0000e+00, 4.9224e-01,\n",
      "         1.0000e+00, 2.6326e-02, 1.7391e-01, 4.1221e-01, 2.2340e-01, 4.3520e-01,\n",
      "         7.1937e-01],\n",
      "        [8.9918e-05, 1.0000e+00, 3.1525e-02, 0.0000e+00, 5.3498e-02, 6.2368e-01,\n",
      "         3.8723e-01, 6.5429e-01, 1.7391e-01, 1.3168e-01, 2.6596e-01, 9.8991e-01,\n",
      "         6.1258e-02],\n",
      "        [2.3277e-01, 0.0000e+00, 6.4663e-01, 0.0000e+00, 5.6379e-01, 1.1056e-01,\n",
      "         1.0000e+00, 4.4103e-03, 1.0000e+00, 9.1412e-01, 8.0851e-01, 9.3272e-01,\n",
      "         5.9630e-01],\n",
      "        [9.8303e-04, 1.2500e-01, 2.7163e-01, 0.0000e+00, 2.8601e-01, 4.4606e-01,\n",
      "         3.7178e-01, 3.9296e-01, 1.7391e-01, 2.3664e-01, 2.7660e-01, 9.8386e-01,\n",
      "         3.8576e-01],\n",
      "        [1.9900e-03, 0.0000e+00, 1.0000e+00, 0.0000e+00, 4.6091e-01, 3.5505e-01,\n",
      "         9.8249e-01, 5.6907e-02, 1.3043e-01, 1.0000e+00, 7.9787e-01, 8.6674e-01,\n",
      "         6.1369e-01]])\n",
      "Targets tensor([[17.9000],\n",
      "        [21.7000],\n",
      "        [50.0000],\n",
      "        [15.2000],\n",
      "        [18.7000],\n",
      "        [13.8000],\n",
      "        [31.6000],\n",
      "        [11.9000],\n",
      "        [21.7000],\n",
      "        [ 7.0000]])\n",
      "11\n",
      "Tamaño de las características torch.Size([10, 13])\n",
      "Features tensor([[1.2537e-01, 0.0000e+00, 6.4663e-01, 0.0000e+00, 7.3045e-01, 5.8785e-01,\n",
      "         9.4439e-01, 9.0489e-02, 1.0000e+00, 9.1412e-01, 8.0851e-01, 2.7619e-01,\n",
      "         5.9437e-01],\n",
      "        [2.7166e-04, 5.5000e-01, 1.2170e-01, 0.0000e+00, 2.0370e-01, 6.3480e-01,\n",
      "         2.5953e-01, 4.8521e-01, 1.7391e-01, 3.4924e-01, 5.3191e-01, 9.7748e-01,\n",
      "         7.9470e-02],\n",
      "        [4.4487e-04, 5.2500e-01, 1.7815e-01, 0.0000e+00, 4.1152e-02, 5.2769e-01,\n",
      "         4.3975e-01, 5.6267e-01, 2.1739e-01, 2.0229e-01, 4.2553e-01, 1.0000e+00,\n",
      "         1.6198e-01],\n",
      "        [1.3679e-03, 0.0000e+00, 2.9692e-01, 0.0000e+00, 2.7778e-01, 5.5815e-01,\n",
      "         9.7013e-01, 1.1852e-01, 1.7391e-01, 3.7595e-01, 8.8298e-01, 9.9581e-01,\n",
      "         2.9084e-01],\n",
      "        [1.1495e-01, 0.0000e+00, 6.4663e-01, 0.0000e+00, 4.7119e-01, 5.0278e-01,\n",
      "         9.6601e-01, 9.4654e-02, 1.0000e+00, 9.1412e-01, 8.0851e-01, 9.5663e-01,\n",
      "         4.4978e-01],\n",
      "        [5.3142e-04, 2.1000e-01, 1.8988e-01, 0.0000e+00, 1.1111e-01, 5.6524e-01,\n",
      "         1.8744e-01, 5.1697e-01, 1.3043e-01, 1.0687e-01, 4.4681e-01, 1.0000e+00,\n",
      "         9.7958e-02],\n",
      "        [4.1239e-04, 8.0000e-01, 5.3152e-02, 0.0000e+00, 5.7613e-02, 4.0276e-01,\n",
      "         1.9567e-01, 8.5989e-01, 1.3043e-01, 2.8053e-01, 1.0000e+00, 9.6445e-01,\n",
      "         1.7439e-01],\n",
      "        [2.1251e-03, 0.0000e+00, 3.7940e-01, 0.0000e+00, 5.7613e-02, 5.1427e-01,\n",
      "         3.3986e-02, 3.7808e-01, 1.3043e-01, 2.2519e-01, 7.0213e-01, 9.5025e-01,\n",
      "         1.6032e-01],\n",
      "        [2.0461e-03, 0.0000e+00, 2.3644e-01, 0.0000e+00, 1.2963e-01, 4.2633e-01,\n",
      "         3.1308e-01, 3.6108e-01, 8.6957e-02, 8.7786e-02, 5.6383e-01, 1.0000e+00,\n",
      "         3.4272e-01],\n",
      "        [7.8182e-02, 0.0000e+00, 6.4663e-01, 0.0000e+00, 6.4815e-01, 4.1234e-01,\n",
      "         9.6910e-01, 7.2466e-02, 1.0000e+00, 9.1412e-01, 8.0851e-01, 9.9377e-01,\n",
      "         4.2439e-01]])\n",
      "Targets tensor([[13.4000],\n",
      "        [31.2000],\n",
      "        [22.3000],\n",
      "        [19.8000],\n",
      "        [14.6000],\n",
      "        [25.0000],\n",
      "        [18.2000],\n",
      "        [23.4000],\n",
      "        [20.0000],\n",
      "        [15.1000]])\n",
      "12\n",
      "Tamaño de las características torch.Size([10, 13])\n",
      "Features tensor([[1.8287e-03, 0.0000e+00, 9.2339e-01, 0.0000e+00, 4.0329e-01, 4.6465e-01,\n",
      "         8.8054e-01, 7.8504e-02, 4.3478e-02, 1.9084e-03, 6.9149e-01, 9.7004e-01,\n",
      "         3.6093e-01],\n",
      "        [7.3429e-04, 0.0000e+00, 9.2339e-01, 0.0000e+00, 4.0329e-01, 4.6810e-01,\n",
      "         8.3625e-01, 9.7100e-02, 4.3478e-02, 1.9084e-03, 6.9149e-01, 9.5151e-01,\n",
      "         3.4603e-01],\n",
      "        [5.7468e-01, 0.0000e+00, 6.4663e-01, 0.0000e+00, 4.3621e-01, 4.2077e-01,\n",
      "         1.0000e+00, 2.5771e-02, 1.0000e+00, 9.1412e-01, 8.0851e-01, 5.7492e-03,\n",
      "         2.3124e-01],\n",
      "        [1.2493e-03, 1.2500e-01, 2.7163e-01, 0.0000e+00, 2.8601e-01, 4.6906e-01,\n",
      "         8.2389e-01, 4.6350e-01, 1.7391e-01, 2.3664e-01, 2.7660e-01, 1.0000e+00,\n",
      "         3.1843e-01],\n",
      "        [1.8589e-03, 2.5000e-01, 1.7119e-01, 0.0000e+00, 1.3992e-01, 4.6082e-01,\n",
      "         9.3203e-01, 5.1732e-01, 3.0435e-01, 1.8511e-01, 7.5532e-01, 9.5254e-01,\n",
      "         3.5072e-01],\n",
      "        [1.1039e-02, 0.0000e+00, 2.8152e-01, 0.0000e+00, 3.1481e-01, 4.3150e-01,\n",
      "         1.0000e+00, 2.6968e-01, 1.3043e-01, 2.2901e-01, 8.9362e-01, 9.9405e-01,\n",
      "         5.0083e-01],\n",
      "        [4.3779e-04, 0.0000e+00, 4.2045e-01, 0.0000e+00, 3.8683e-01, 4.9032e-01,\n",
      "         7.6004e-01, 1.0529e-01, 0.0000e+00, 1.6412e-01, 8.9362e-01, 1.0000e+00,\n",
      "         2.0281e-01],\n",
      "        [3.2809e-04, 2.5000e-01, 1.6129e-01, 0.0000e+00, 8.4362e-02, 4.9933e-01,\n",
      "         4.5108e-01, 3.8839e-01, 1.3043e-01, 1.7939e-01, 6.8085e-01, 9.8422e-01,\n",
      "         1.5949e-01],\n",
      "        [1.1034e-01, 0.0000e+00, 6.4663e-01, 0.0000e+00, 5.8848e-01, 6.1947e-01,\n",
      "         9.8764e-01, 2.0769e-02, 1.0000e+00, 9.1412e-01, 8.0851e-01, 1.0000e+00,\n",
      "         5.3836e-01],\n",
      "        [1.5009e-01, 0.0000e+00, 6.4663e-01, 0.0000e+00, 6.3374e-01, 4.4568e-01,\n",
      "         9.4542e-01, 5.9335e-02, 1.0000e+00, 9.1412e-01, 8.0851e-01, 1.0000e+00,\n",
      "         4.0342e-01]])\n",
      "Targets tensor([[21.4000],\n",
      "        [20.3000],\n",
      "        [15.0000],\n",
      "        [18.9000],\n",
      "        [16.0000],\n",
      "        [14.5000],\n",
      "        [20.6000],\n",
      "        [22.9000],\n",
      "        [13.3000],\n",
      "        [12.7000]])\n",
      "13\n",
      "Tamaño de las características torch.Size([10, 13])\n",
      "Features tensor([[8.4894e-03, 2.0000e-01, 1.2867e-01, 0.0000e+00, 5.3909e-01, 3.8302e-01,\n",
      "         6.1689e-01, 7.7922e-02, 1.7391e-01, 1.4695e-01, 4.2553e-02, 9.8865e-01,\n",
      "         2.4062e-01],\n",
      "        [2.7411e-01, 0.0000e+00, 6.4663e-01, 0.0000e+00, 6.4815e-01, 2.0904e-01,\n",
      "         1.0000e+00, 3.0700e-02, 1.0000e+00, 9.1412e-01, 8.0851e-01, 1.0000e+00,\n",
      "         7.3262e-01],\n",
      "        [2.7869e-01, 0.0000e+00, 6.4663e-01, 0.0000e+00, 6.3374e-01, 3.4259e-01,\n",
      "         9.5881e-01, 5.2124e-02, 1.0000e+00, 9.1412e-01, 8.0851e-01, 1.0000e+00,\n",
      "         4.9779e-01],\n",
      "        [5.3872e-04, 0.0000e+00, 1.3160e-01, 0.0000e+00, 2.5720e-01, 5.2769e-01,\n",
      "         7.2606e-01, 1.9896e-01, 1.7391e-01, 2.0802e-01, 4.2553e-01, 9.9672e-01,\n",
      "         1.2583e-01],\n",
      "        [2.1547e-03, 0.0000e+00, 3.7133e-01, 0.0000e+00, 2.1399e-01, 5.0220e-01,\n",
      "         4.0680e-01, 2.5605e-01, 1.3043e-01, 1.7176e-01, 6.3830e-01, 9.9175e-01,\n",
      "         2.1358e-01],\n",
      "        [1.5786e-01, 0.0000e+00, 6.4663e-01, 0.0000e+00, 4.3621e-01, 5.9322e-01,\n",
      "         1.0000e+00, 3.6183e-02, 1.0000e+00, 9.1412e-01, 8.0851e-01, 8.7574e-02,\n",
      "         5.3780e-01],\n",
      "        [1.6202e-01, 0.0000e+00, 6.4663e-01, 0.0000e+00, 7.3045e-01, 5.5566e-01,\n",
      "         9.3100e-01, 7.9386e-02, 1.0000e+00, 9.1412e-01, 8.0851e-01, 6.8511e-02,\n",
      "         4.5033e-01],\n",
      "        [4.5983e-02, 0.0000e+00, 7.0088e-01, 0.0000e+00, 1.0000e+00, 3.6540e-01,\n",
      "         1.0000e+00, 2.5662e-02, 1.7391e-01, 4.1221e-01, 2.2340e-01, 1.0000e+00,\n",
      "         6.8129e-01],\n",
      "        [4.9634e-02, 0.0000e+00, 6.4663e-01, 0.0000e+00, 4.0947e-01, 4.6791e-01,\n",
      "         9.4336e-01, 1.2828e-01, 1.0000e+00, 9.1412e-01, 8.0851e-01, 8.3456e-01,\n",
      "         5.4056e-01],\n",
      "        [5.4884e-04, 3.3000e-01, 6.3050e-02, 0.0000e+00, 1.7901e-01, 7.0416e-01,\n",
      "         3.9341e-01, 2.6302e-01, 2.6087e-01, 6.6794e-02, 6.1702e-01, 9.9188e-01,\n",
      "         1.4349e-01]])\n",
      "Targets tensor([[22.8000],\n",
      "        [10.5000],\n",
      "        [ 8.3000],\n",
      "        [24.6000],\n",
      "        [25.0000],\n",
      "        [17.2000],\n",
      "        [ 9.6000],\n",
      "        [15.6000],\n",
      "        [19.1000],\n",
      "        [36.1000]])\n",
      "14\n",
      "Tamaño de las características torch.Size([10, 13])\n",
      "Features tensor([[1.0360e-03, 0.0000e+00, 9.2339e-01, 0.0000e+00, 4.0329e-01, 4.4415e-01,\n",
      "         9.5675e-01, 7.9722e-02, 4.3478e-02, 1.9084e-03, 6.9149e-01, 9.5582e-01,\n",
      "         4.3736e-01],\n",
      "        [5.7752e-03, 2.0000e-01, 1.2867e-01, 0.0000e+00, 5.3909e-01, 9.2681e-01,\n",
      "         9.1246e-01, 1.0538e-01, 1.7391e-01, 1.4695e-01, 4.2553e-02, 9.7468e-01,\n",
      "         1.1534e-01],\n",
      "        [2.6166e-03, 0.0000e+00, 3.3834e-01, 0.0000e+00, 4.1152e-01, 4.7097e-01,\n",
      "         6.4264e-01, 1.1635e-01, 2.1739e-01, 3.8931e-01, 7.0213e-01, 1.0000e+00,\n",
      "         3.0877e-01],\n",
      "        [2.6664e-02, 0.0000e+00, 6.4663e-01, 0.0000e+00, 4.0741e-01, 4.4261e-01,\n",
      "         4.0165e-01, 2.3592e-01, 1.0000e+00, 9.1412e-01, 8.0851e-01, 9.3401e-01,\n",
      "         3.2036e-01],\n",
      "        [3.7091e-04, 0.0000e+00, 1.0814e-01, 0.0000e+00, 2.1399e-01, 5.4493e-01,\n",
      "         7.3120e-01, 1.7846e-01, 4.3478e-02, 1.5840e-01, 5.5319e-01, 9.9155e-01,\n",
      "         1.7853e-01],\n",
      "        [1.8548e-03, 0.0000e+00, 3.5007e-01, 0.0000e+00, 3.3333e-01, 4.5354e-01,\n",
      "         8.7848e-01, 1.2126e-01, 2.1739e-01, 4.6756e-01, 5.5319e-01, 8.6890e-01,\n",
      "         3.8714e-01],\n",
      "        [3.5867e-03, 0.0000e+00, 7.8556e-01, 0.0000e+00, 4.9177e-01, 5.4991e-01,\n",
      "         9.8764e-01, 6.2099e-02, 1.3043e-01, 4.7710e-01, 9.1489e-01, 1.0000e+00,\n",
      "         3.7693e-01],\n",
      "        [4.0787e-03, 0.0000e+00, 3.4604e-01, 0.0000e+00, 3.2716e-01, 5.7597e-01,\n",
      "         8.6921e-01, 2.2485e-01, 1.3043e-01, 2.2328e-01, 6.1702e-01, 9.9695e-01,\n",
      "         2.0833e-01],\n",
      "        [2.5002e-03, 0.0000e+00, 2.9692e-01, 0.0000e+00, 2.7778e-01, 5.4493e-01,\n",
      "         8.4964e-01, 1.4414e-01, 1.7391e-01, 3.7595e-01, 8.8298e-01, 1.7772e-01,\n",
      "         2.4558e-01],\n",
      "        [2.9455e-03, 0.0000e+00, 3.3834e-01, 0.0000e+00, 4.1152e-01, 4.2786e-01,\n",
      "         6.9722e-01, 1.6033e-01, 2.1739e-01, 3.8931e-01, 7.0213e-01, 1.0000e+00,\n",
      "         3.4134e-01]])\n",
      "Targets tensor([[18.8000],\n",
      "        [48.8000],\n",
      "        [21.2000],\n",
      "        [20.6000],\n",
      "        [22.0000],\n",
      "        [18.3000],\n",
      "        [18.0000],\n",
      "        [23.8000],\n",
      "        [18.6000],\n",
      "        [18.3000]])\n",
      "15\n",
      "Tamaño de las características torch.Size([10, 13])\n",
      "Features tensor([[2.5391e-01, 0.0000e+00, 6.4663e-01, 0.0000e+00, 6.4815e-01, 2.7572e-01,\n",
      "         8.9186e-01, 3.5355e-02, 1.0000e+00, 9.1412e-01, 8.0851e-01, 1.0000e+00,\n",
      "         8.3499e-01],\n",
      "        [4.5544e-04, 0.0000e+00, 1.0814e-01, 0.0000e+00, 2.1399e-01, 5.4723e-01,\n",
      "         6.5088e-01, 1.7848e-01, 4.3478e-02, 1.5840e-01, 5.5319e-01, 9.8810e-01,\n",
      "         1.9536e-01],\n",
      "        [7.0507e-04, 0.0000e+00, 6.3050e-02, 0.0000e+00, 1.5021e-01, 6.8710e-01,\n",
      "         5.2832e-01, 4.4854e-01, 8.6957e-02, 6.6794e-02, 6.4894e-01, 1.0000e+00,\n",
      "         9.9338e-02],\n",
      "        [8.5793e-04, 0.0000e+00, 4.9340e-01, 0.0000e+00, 1.0700e-01, 4.9167e-01,\n",
      "         1.5963e-01, 3.9767e-01, 1.3043e-01, 1.9466e-01, 3.6170e-01, 1.0000e+00,\n",
      "         1.8902e-01],\n",
      "        [3.3399e-03, 0.0000e+00, 2.5367e-01, 0.0000e+00, 2.2222e-01, 5.2711e-01,\n",
      "         2.6777e-01, 3.8977e-01, 1.7391e-01, 1.9084e-01, 7.4468e-01, 1.0000e+00,\n",
      "         1.2196e-01],\n",
      "        [9.2636e-02, 0.0000e+00, 6.4663e-01, 0.0000e+00, 6.7490e-01, 7.3424e-01,\n",
      "         9.9279e-01, 1.2032e-01, 1.0000e+00, 9.1412e-01, 8.0851e-01, 9.4697e-01,\n",
      "         4.1418e-01],\n",
      "        [1.7201e-03, 0.0000e+00, 2.3644e-01, 0.0000e+00, 1.2963e-01, 5.0776e-01,\n",
      "         3.7075e-02, 4.1751e-01, 8.6957e-02, 8.7786e-02, 5.6383e-01, 9.9385e-01,\n",
      "         1.5756e-01],\n",
      "        [1.4980e-03, 0.0000e+00, 2.9692e-01, 0.0000e+00, 2.7778e-01, 4.9933e-01,\n",
      "         8.9701e-01, 1.1743e-01, 1.7391e-01, 3.7595e-01, 8.8298e-01, 9.8938e-01,\n",
      "         2.9249e-01],\n",
      "        [4.5320e-02, 0.0000e+00, 6.4663e-01, 0.0000e+00, 3.0247e-01, 5.1121e-01,\n",
      "         9.0422e-01, 1.7911e-01, 1.0000e+00, 9.1412e-01, 8.0851e-01, 9.9604e-01,\n",
      "         3.0740e-01],\n",
      "        [1.0855e-03, 3.0000e-01, 1.6386e-01, 0.0000e+00, 8.8477e-02, 5.3593e-01,\n",
      "         5.1493e-01, 5.3705e-01, 2.1739e-01, 2.1565e-01, 4.2553e-01, 9.3910e-01,\n",
      "         2.6187e-01]])\n",
      "Targets tensor([[ 7.4000],\n",
      "        [22.6000],\n",
      "        [36.2000],\n",
      "        [23.9000],\n",
      "        [23.0000],\n",
      "        [17.8000],\n",
      "        [24.7000],\n",
      "        [20.1000],\n",
      "        [19.6000],\n",
      "        [22.2000]])\n",
      "16\n",
      "Tamaño de las características torch.Size([7, 13])\n",
      "Features tensor([[4.4476e-03, 0.0000e+00, 3.4604e-01, 0.0000e+00, 3.2716e-01, 5.4053e-01,\n",
      "         6.6220e-01, 2.1851e-01, 1.3043e-01, 2.2328e-01, 6.1702e-01, 9.9574e-01,\n",
      "         2.3813e-01],\n",
      "        [6.3291e-04, 0.0000e+00, 4.2045e-01, 0.0000e+00, 3.8683e-01, 5.8095e-01,\n",
      "         6.8177e-01, 1.2267e-01, 0.0000e+00, 1.6412e-01, 8.9362e-01, 9.8762e-01,\n",
      "         2.1909e-01],\n",
      "        [1.6104e-01, 0.0000e+00, 6.4663e-01, 0.0000e+00, 6.4815e-01, 2.5273e-01,\n",
      "         1.0000e+00, 4.1821e-02, 1.0000e+00, 9.1412e-01, 8.0851e-01, 9.3953e-01,\n",
      "         7.9719e-01],\n",
      "        [1.0084e-03, 4.0000e-01, 2.1811e-01, 0.0000e+00, 1.2757e-01, 6.3096e-01,\n",
      "         4.1092e-01, 2.8533e-01, 1.3043e-01, 1.2786e-01, 5.3191e-01, 1.0000e+00,\n",
      "         3.4492e-02],\n",
      "        [1.1684e-03, 2.5000e-01, 1.7119e-01, 0.0000e+00, 1.3992e-01, 5.5470e-01,\n",
      "         6.6838e-01, 5.5433e-01, 3.0435e-01, 1.8511e-01, 7.5532e-01, 1.0000e+00,\n",
      "         1.3797e-01],\n",
      "        [1.0000e+00, 0.0000e+00, 6.4663e-01, 0.0000e+00, 5.8848e-01, 6.5281e-01,\n",
      "         9.1658e-01, 2.6089e-02, 1.0000e+00, 9.1412e-01, 8.0851e-01, 1.0000e+00,\n",
      "         4.2715e-01],\n",
      "        [3.1617e-04, 8.2500e-01, 5.7551e-02, 0.0000e+00, 6.1728e-02, 4.9837e-01,\n",
      "         3.6560e-01, 4.6744e-01, 4.3478e-02, 3.0725e-01, 2.2340e-01, 9.9211e-01,\n",
      "         1.5728e-01]])\n",
      "Targets tensor([[23.1000],\n",
      "        [22.4000],\n",
      "        [10.2000],\n",
      "        [32.0000],\n",
      "        [22.2000],\n",
      "        [10.4000],\n",
      "        [24.1000]])\n"
     ]
    }
   ],
   "source": [
    "# Example: how to use dataloaders\n",
    "print(test_dl)\n",
    "\n",
    "for i, (inputs, targets) in enumerate(test_dl):\n",
    "    print(i)\n",
    "    print(\"Tamaño de las características\", inputs.shape)\n",
    "    print(\"Features\",inputs)\n",
    "    print(\"Targets\",targets)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0220741f",
   "metadata": {},
   "source": [
    "## Regresión eliminando anomalías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dbb9d4e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cantidad de datos de entrenamiento y prueba 332 163\n"
     ]
    }
   ],
   "source": [
    "# prepare the data\n",
    "boston_array = boston_df.to_numpy()\n",
    "\n",
    "# compute the IsolationForest algorithm to detect anomalies.\n",
    "clf = IsolationForest(n_estimators=100, contamination=.02)\n",
    "predictions = clf.fit_predict(boston_array)\n",
    "\n",
    "# data without anomalies.\n",
    "boston_df_clean = pd.DataFrame(boston_array[np.where(predictions>-1)])\n",
    "\n",
    "# prepara data before train the model. \n",
    "train_dl, test_dl, dataset = prepare_data(boston_df_clean)\n",
    "print(\"Cantidad de datos de entrenamiento y prueba\", len(train_dl.dataset), len(test_dl.dataset))\n",
    "\n",
    "# define the network and train the model \n",
    "n_inputs = data.shape[1] -1 \n",
    "n_layer_1 = 10\n",
    "n_layer_2 = 8\n",
    "n_output = 1\n",
    "model = MLP( n_inputs, n_output, n_layer_1, n_layer_2 )\n",
    "\n",
    "# train the model\n",
    "model = train_model(train_dl, model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6dd139d",
   "metadata": {},
   "source": [
    "### Evaluación del modelo después de remover las anomalías  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "62918ee1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 16.729, RMSE: 4.090\n",
      "MAE: 2.876\n",
      "Predicted: 25.971\n"
     ]
    }
   ],
   "source": [
    "# evaluate the model using the Mean Squared Error (MSE)\n",
    "mse, mae = evaluate_model(test_dl, model)\n",
    "print('MSE: %.3f, RMSE: %.3f' % (mse, sqrt(mse)))\n",
    "print('MAE: %.3f' % (mae))\n",
    "\n",
    "\n",
    "# make a single prediction\n",
    "row = [0.00632,18.00,2.310,0,0.5380,6.5750,65.20,4.0900,1,296.0,15.30,396.90,4.98]\n",
    "yhat = predict(row, model)\n",
    "print('Predicted: %.3f' % yhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afade6aa",
   "metadata": {},
   "source": [
    "#### Referencias:\n",
    "    \n",
    "[1] Brownlee, J. (2016). Support Vector Machines for Machine Learning. Recuperado de https://machinelearningmastery.com/support-vector-machines-for-machine-learning/\n",
    "\n",
    "[2] Maklin, C. (2019). Support Vector Machine Python Example. https://towardsdatascience.com/support-vector-machine-python-example-d67d9b63f1c8\n",
    "\n",
    "[3] Brownlee, J. (2020). https://machinelearningmastery.com/pytorch-tutorial-develop-deep-learning-models/\n",
    "\n",
    "[4] DATASETS & DATALOADERS. Creating a Custom Dataset for your files. Recuperado de https://pytorch.org/tutorials/beginner/basics/data_tutorial.html\n",
    "\n",
    "[5] Boston Housing Data. https://raw.githubusercontent.com/jbrownlee/Datasets/master/housing.names\n",
    "\n",
    "[6] Brownlee, J. (2021). Weight Initialization for Deep Learning Neural Networks.  https://machinelearningmastery.com/weight-initialization-for-deep-learning-neural-networks/\n",
    "\n",
    "[7] Vapnik, V. The Nature of Statistical Learning Theory. Springer, New York, 1995.\n",
    "\n",
    "[8] Glorot, X. & Bengio, Y. (2010). Understanding the difficulty of training deep feedforward neural networks. Recuperado de https://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf\n",
    "\n",
    "[9] Bishop, C. (2006). Pattern recognition and machine learning. springer. Recuperado de https://www.microsoft.com/en-us/research/uploads/prod/2006/01/Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44885519",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb856498",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
