{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GItxU96mTk16"
   },
   "source": [
    "# Matemática para Ciencia de los Datos\n",
    "# Trabajo Práctico 6\n",
    "\n",
    "Profesor: Luis Alexánder Calvo Valverde \n",
    "\n",
    "Instituto Tecnológico de Costa Rica, \n",
    "\n",
    "Programa Ciencia de Datos\n",
    "\n",
    "---\n",
    "\n",
    "Fecha de entrega: Lunes 5 de Junio del 2023, a más tardar a las 3:00 pm.\n",
    "\n",
    "Medio de entrega: Por medio del TEC-Digital.\n",
    "\n",
    "Entregables: Un archivo jupyter ( .IPYNB ). \n",
    "\n",
    "Estudiante:\n",
    "1. **Marco Ferraro**\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicio 1 (50 puntos)\n",
    "\n",
    "\n",
    "\n",
    "El algoritmo del descenso de gradiente sigue la idea de modificar el punto óptimo estimado de forma iterativa. Para una función en una\n",
    "variable $f\\left(x\\right)$, la estimación del punto óptimo en una iteración $i+1$ está dada por: \n",
    "\n",
    "\\begin{equation}\n",
    "x\\left(t+1\\right)=x\\left(t\\right)+\\alpha f'\\left(x\\left(t\\right)\\right)\n",
    "\\end{equation}\n",
    "\n",
    "donde el coeficiente $\\alpha$ determina el *grado de confianza o velocidad* con la que el proceso de optimización iterativa sigue\n",
    "la dirección de la derivada. Para la optimización de una función multivariable $f\\left(\\overrightarrow{x}\\left(t\\right)\\right)$ con $\\overrightarrow{x}\\in\\mathbb{R}^{n}$, la posición óptima se estima usando el vector gradiente:\n",
    "\n",
    "\\begin{equation}\n",
    "\\overrightarrow{x}\\left(t+1\\right)=\\overrightarrow{x}\\left(t\\right)+\\alpha\\nabla_{\\overrightarrow{x}}f\\left(\\overrightarrow{x}\\left(t\\right)\\right)\n",
    "\\end{equation}\n",
    "\n",
    "Para la función: \n",
    "\n",
    "\\begin{equation}\n",
    "f\\left(\\overrightarrow{x}\\right)=x^{2}-y^{2},\n",
    "\\end{equation}\n",
    "\n",
    "Implemente la función en python denominada:\n",
    "\n",
    "$$funcion\\_SGD \\left(tasa\\_aprendizaje, iteraciones, xy, tolerancia\\right)$$\n",
    "\n",
    "donde los parámetros corresponden a:\n",
    "\n",
    "* tasa_aprendizaje: es el $\\alpha$\n",
    "* iteraciones: es el máximo número de iteraciones a ejecutar\n",
    "* xy: es el vector con los dos valores iniciales [x,y]\n",
    "* tolerancia: es el valor mínimo para un cambio entre iteración. Si la función de costo no mejora en al menos \"tolerancia\", sale del ciclo de iteración.\n",
    "\n",
    "**Nota:** \n",
    "1. Para iniciar la implementación puede utilizar el código en el cuaderno \"070_1_LACV_Optimizacion\".\n",
    "1. Cada iteración le generará un vector con dos valores ($\\overrightarrow{x}\\left(t+1\\right)$), por lo que para saber el valor de la función de pérdida en ese punto, evalúelo en la función inicial ($x^{2}-y^{2}$) para saber si aumentó o disminuyó.\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def f(x, y):\n",
    "    return (x**2) - (y**2)\n",
    "\n",
    "def dx_f(x, y):\n",
    "    return 2*x\n",
    "\n",
    "\n",
    "def dy_f(x, y):\n",
    "    return -2*y\n",
    "\n",
    "def funcion_SGD(lr, iter, xy, tolerance=1e-5):\n",
    "    current_iter = 0\n",
    "    change = 1\n",
    "    cur_x = xy\n",
    "\n",
    "    print(\"Original value in f:\", f(xy[0], xy[1]), \"\\n\")\n",
    "\n",
    "    while current_iter < iter and change > tolerance:\n",
    "        prev_x = cur_x\n",
    "        gradient = np.array([dx_f(prev_x[0], prev_x[1]), dy_f(prev_x[0], prev_x[1])])\n",
    "        cur_x = cur_x - (lr * gradient)\n",
    "        change = sum(abs(cur_x - prev_x))\n",
    "\n",
    "        if (current_iter % 50) == 0:\n",
    "          print(\"Iteration:\", current_iter, \" f:\", f(cur_x[0], cur_x[1]), \"\\nX value is\", cur_x)\n",
    "\n",
    "        current_iter += 1\n",
    "    \n",
    "    print(\"\\nThe local minimum occurs at\", cur_x)\n",
    "    print(\"f:\", f(cur_x[0], cur_x[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original value in f: 24 \n",
      "\n",
      "Iteration: 0  f: -44.0 \n",
      "X value is [ 3.5 -7.5]\n",
      "Iteration: 50  f: -2.2869066236355854e+19 \n",
      "X value is [ 3.10862447e-15 -4.78216125e+09]\n",
      "Iteration: 100  f: -9.297674498183656e+36 \n",
      "X value is [ 2.76101317e-30 -3.04920883e+18]\n",
      "Iteration: 150  f: -3.7800734923206925e+54 \n",
      "X value is [ 2.45227231e-45 -1.94424111e+27]\n",
      "Iteration: 200  f: -1.5368311302075538e+72 \n",
      "X value is [ 2.17805535e-60 -1.23968993e+36]\n",
      "Iteration: 250  f: -6.248158739699581e+89 \n",
      "X value is [ 1.93450176e-75 -7.90452955e+44]\n",
      "Iteration: 300  f: -2.540258774639206e+107 \n",
      "X value is [ 1.71818271e-90 -5.04009799e+53]\n",
      "Iteration: 350  f: -1.032770598661478e+125 \n",
      "X value is [ 1.52605281e-105 -3.21367484e+062]\n",
      "Iteration: 400  f: -4.198844307155597e+142 \n",
      "X value is [ 1.35540717e-120 -2.04910817e+071]\n",
      "Iteration: 450  f: -1.707087085804216e+160 \n",
      "X value is [ 1.20384340e-135 -1.30655543e+080]\n",
      "\n",
      "The local minimum occurs at [ 2.13845545e-150 -5.55391888e+088]\n",
      "f: -3.0846014922654313e+177\n"
     ]
    }
   ],
   "source": [
    "funcion_SGD(lr=0.25, iter=500, xy=np.array([7, -5]), tolerance=1e-2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicio 2\n",
    "\n",
    "Para la función  $f_{1}\\left(x_{1},x_{2}\\right)=x_{1}^4 + x_{2}^4$\n",
    "\n",
    "Realice lo siguiente:\n",
    "\n",
    "1. En una celda de texto:\n",
    "\n",
    " - Calcule el vector gradiente. **(15 puntos)**\n",
    "\n",
    " - Calcule la matriz Hessiana. **(15 puntos)**\n",
    "\n",
    "2. Para el resultado obtenido en el punto anterior: **(20 puntos)**\n",
    "  - Evalúela en el punto $x_{1},x_{2}\\in\\left[4,4\\right]$. \n",
    "  - Luego aplique el criterio de la segunda derivada parcial ¿qué conclusiones saca para ese punto? \n",
    "\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculo del vector gradiente\n",
    "\n",
    "Aplicamos las derivadas parciales:\n",
    "\n",
    "$\n",
    "\\begin{equation}\n",
    "\\nabla_{\\overrightarrow{x}}f=\\begin{bmatrix}4x_{1}^3 \\\\\n",
    " 4x_{2}^3\n",
    "\\end{bmatrix}\n",
    "\\end{equation}\n",
    "$\n",
    "\n",
    "Ahora, calculamos las segundas derivadas parciales de segundo orden para la matriz hessiana:\n",
    "\n",
    "$\n",
    "  12x_{1}^2 \n",
    "$\n",
    "\n",
    "\n",
    "$\n",
    "  12x_{2}^2 \n",
    "$\n",
    "\n",
    "Para la derivada $ f_{x_{1}x_{2}}$ podemos utilizar cualquiera de las derivadas de primer orden. Como no hay mulitplicacion entre los valores de $x_{1}$ y $x_{2}$ la derivada de segundo orden tiene un valor de 0.\n",
    "\n",
    "Con estos valores, podemos montar la matriz Hessiana.\n",
    "\n",
    "$\n",
    "\\begin{bmatrix}\n",
    "12x_{1}^2 & 0 \\\\\n",
    "0 & 12x_{2}^2  \\\\\n",
    "\\end{bmatrix}\n",
    "$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remplazamos los valores en la matriz Hessiana\n",
    "\n",
    "$\n",
    "\\begin{bmatrix}\n",
    "12(4)^2 & 0 \\\\\n",
    "0 & 12(4)^2  \\\\\n",
    "\\end{bmatrix}\n",
    "$\n",
    "\n",
    "\n",
    "$\n",
    "\\begin{bmatrix}\n",
    "192 & 0 \\\\\n",
    "0 & 192  \\\\\n",
    "\\end{bmatrix}\n",
    "$\n",
    "\n",
    "$\n",
    "= 36864\n",
    "$\n",
    "\n",
    "Adicionalmente, las derivadas de segundo orden son mayor que 0, por lo tanto el punto punto $(4, 4)$ es un minimo local"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Trabajo_Práctico_1_Matemática_para_Ciencia_de_los_Datos.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
