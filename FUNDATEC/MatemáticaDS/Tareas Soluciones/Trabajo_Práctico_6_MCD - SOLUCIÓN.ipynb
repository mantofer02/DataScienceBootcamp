{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GItxU96mTk16"
   },
   "source": [
    "# Matemática para Ciencia de los Datos\n",
    "# Trabajo Práctico 6\n",
    "\n",
    "Profesor: Luis Alexánder Calvo Valverde \n",
    "\n",
    "Instituto Tecnológico de Costa Rica, \n",
    "\n",
    "Programa Ciencia de Datos\n",
    "\n",
    "---\n",
    "\n",
    "Fecha de entrega: \n",
    "\n",
    "Medio de entrega: Por medio del TEC-Digital.\n",
    "\n",
    "Entregables: Un archivo jupyter ( .IPYNB ). \n",
    "\n",
    "Estudiante:\n",
    "1. **Nombre_Estudiante**\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicio 1 (50 puntos)\n",
    "\n",
    "\n",
    "\n",
    "El algoritmo del descenso de gradiente sigue la idea de modificar el punto óptimo estimado de forma iterativa. Para una función en una\n",
    "variable $f\\left(x\\right)$, la estimación del punto óptimo en una iteración $i+1$ está dada por: \n",
    "\n",
    "\\begin{equation}\n",
    "x\\left(t+1\\right)=x\\left(t\\right)+\\alpha f'\\left(x\\left(t\\right)\\right)\n",
    "\\end{equation}\n",
    "\n",
    "donde el coeficiente $\\alpha$ determina el *grado de confianza o velocidad* con la que el proceso de optimización iterativa sigue\n",
    "la dirección de la derivada. Para la optimización de una función multivariable $f\\left(\\overrightarrow{x}\\left(t\\right)\\right)$ con $\\overrightarrow{x}\\in\\mathbb{R}^{n}$, la posición óptima se estima usando el vector gradiente:\n",
    "\n",
    "\\begin{equation}\n",
    "\\overrightarrow{x}\\left(t+1\\right)=\\overrightarrow{x}\\left(t\\right)+\\alpha\\nabla_{\\overrightarrow{x}}f\\left(\\overrightarrow{x}\\left(t\\right)\\right)\n",
    "\\end{equation}\n",
    "\n",
    "Para la función: \n",
    "\n",
    "\\begin{equation}\n",
    "f\\left(\\overrightarrow{x}\\right)=x^{2}-y^{2},\n",
    "\\end{equation}\n",
    "\n",
    "Implemente la función en python denominada:\n",
    "\n",
    "$$funcion\\_SGD \\left(tasa\\_aprendizaje, iteraciones, xy, tolerancia\\right)$$\n",
    "\n",
    "donde los parámetros corresponden a:\n",
    "\n",
    "* tasa_aprendizaje: es el $\\alpha$\n",
    "* iteraciones: es el máximo número de iteraciones a ejecutar\n",
    "* xy: es el vector con los dos valores iniciales [x,y]\n",
    "* tolerancia: es el valor mínimo para un cambio entre iteración. Si la función de costo no mejora en al menos \"tolerancia\", sale del ciclo de iteración.\n",
    "\n",
    "**Nota:** \n",
    "1. Para iniciar la implementación puede utilizar el código en el cuaderno \"070_1_LACV_Optimizacion\".\n",
    "1. Cada iteración le generará un vector con dos valores ($\\overrightarrow{x}\\left(t+1\\right)$), por lo que para saber el valor de la función de pérdida en ese punto, evalúelo en la función inicial ($x^{2}-y^{2}$) para saber si aumentó o disminuyó.\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1]\n",
      "Iteration 100 \n",
      "X value is [0.         7.24464612]\n",
      "cambio_paso_previo:  2.038052531814529\n",
      "Iteration 200 \n",
      "X value is [ 0.         52.48489738]\n",
      "cambio_paso_previo:  106.96697798470223\n",
      "Iteration 300 \n",
      "X value is [  0.         380.23450806]\n",
      "cambio_paso_previo:  5614.150862437673\n",
      "Iteration 400 \n",
      "X value is [   0.         2754.66445285]\n",
      "cambio_paso_previo:  294658.1318836296\n",
      "Iteration 500 \n",
      "X value is [    0.         19956.56913545]\n",
      "cambio_paso_previo:  15465101.813714445\n",
      "Iteration 600 \n",
      "X value is [     0.        144578.2811208]\n",
      "cambio_paso_previo:  811684281.6440773\n",
      "Iteration 700 \n",
      "X value is [      0.         1047418.48310537]\n",
      "cambio_paso_previo:  42601166226.00037\n",
      "Iteration 800 \n",
      "X value is [      0.        7588176.2478151]\n",
      "cambio_paso_previo:  2235917837584.9453\n",
      "Iteration 900 \n",
      "X value is [       0.         54973651.59834822]\n",
      "cambio_paso_previo:  117351918252870.0\n",
      "Iteration 1000 \n",
      "X value is [0.00000000e+00 3.98264652e+08]\n",
      "cambio_paso_previo:  6159203386696416.0\n",
      "Cantidad de veces que iteró:  1000\n",
      "x:  0.0 , y:  398264651.6581294\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "def funcion( x, y):\n",
    "    return x**2 - y**2\n",
    "\n",
    "def funcion_SGD(tasa_aprendizaje, iteraciones, xy, tolerancia):\n",
    "\n",
    "    x = xy[0]\n",
    "    y = xy[1]\n",
    "    \n",
    "    # Por 100 solo para asegurar que en la primera iteración entre\n",
    "    cambio_paso_previo = tolerancia * 100\n",
    "    \n",
    "    contador = 0\n",
    "    while(cambio_paso_previo > tolerancia) and (contador < iteraciones):\n",
    "        # guardar los valores previos de x e y\n",
    "        previoX = x\n",
    "        previoY = y\n",
    "        \n",
    "        # Ojo que acá se pone el vector gradiente de la función multiplicado por la tasa de aprendizaje\n",
    "        resultado = np.array([x,y]) - tasa_aprendizaje * np.array( [2.0*x, -2.0*y] )\n",
    "        # nuevos x e y\n",
    "        x = resultado[0]\n",
    "        y = resultado[1]\n",
    "        \n",
    "        # medir el cambio\n",
    "        cambio_paso_previo = abs( funcion(previoX, previoY)  - funcion(x, y)  )\n",
    "        \n",
    "        # Descontamos la iteración\n",
    "        contador += 1\n",
    "        if (contador % 100) == 0:\n",
    "            print(\"Iteration\",contador,\"\\nX value is\",resultado) #Print iterations\n",
    "            print(\"cambio_paso_previo: \", cambio_paso_previo)\n",
    "    print(\"Cantidad de veces que iteró: \", contador)\n",
    "    return x,y\n",
    "\n",
    "test_xy = np.array([0, 1])\n",
    "lr = 0.01\n",
    "max_it = 1000\n",
    "t = 0.001\n",
    "print(test_xy)\n",
    "x, y = funcion_SGD(lr, max_it, test_xy, t)\n",
    "print(\"x: \", x, \", y: \", y )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicio 2\n",
    "\n",
    "Para la función  $f_{1}\\left(x_{1},x_{2}\\right)=x_{1}^4 + x_{2}^4$\n",
    "\n",
    "Realice lo siguiente:\n",
    "\n",
    "1. En una celda de texto:\n",
    "\n",
    " - Calcule el vector gradiente. **(15 puntos)**\n",
    "\n",
    " - Calcule la matriz Hessiana. **(15 puntos)**\n",
    "\n",
    "2. Para el resultado obtenido en el punto anterior: **(20 puntos)**\n",
    "  - Evalúela en el punto $x_{1},x_{2}\\in\\left[4,4\\right]$. \n",
    "  - Luego aplique el criterio de la segunda derivada parcial ¿qué conclusiones saca para ese punto? \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluado:  [[192, 0], [0, 192]]\n",
      "det de H:  36863.99999999999\n",
      " Es mayor que cero por lo que es un máximo o mínimo local\n",
      "Y como fxx es mayor que cero (192) es un mínimo local\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def funcion(x1, x2):\n",
    "  return  x1**4 + x2**4\n",
    "\n",
    "def deme_vector_gradiente(x1,x2):\n",
    "    respuesta = [ [4*x1**3],\n",
    "                  [4*x2**3]\n",
    "                ]\n",
    "    return respuesta\n",
    "\n",
    "def deme_matriz_Hessiana(x1,x2):\n",
    "    respuesta = [[ 12*(x1**2),0 ],\n",
    "                 [ 0,12*(x2**2) ]\n",
    "                ]\n",
    "    return respuesta\n",
    "\n",
    "\n",
    "x1 = 4\n",
    "x2 = 4\n",
    "\n",
    "H_evaluado = deme_matriz_Hessiana(x1, x2)\n",
    "print( \"evaluado: \", H_evaluado)\n",
    "\n",
    "hessiano = np.linalg.det(H_evaluado)\n",
    "print(\"det de H: \", hessiano)\n",
    "\n",
    "print( \" Es mayor que cero por lo que es un máximo o mínimo local\")\n",
    "print(\"Y como fxx es mayor que cero (192) es un mínimo local\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Trabajo_Práctico_1_Matemática_para_Ciencia_de_los_Datos.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
