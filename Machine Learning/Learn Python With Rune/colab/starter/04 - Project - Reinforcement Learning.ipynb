{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "77333f45",
   "metadata": {},
   "source": [
    "<a \n",
    " href=\"https://colab.research.google.com/github/LearnPythonWithRune/MachineLearningWithPython/blob/main/colab/final/04 - Project - Reinforcement Learning.ipynb\"\n",
    " target=\"_parent\">\n",
    "<img \n",
    " src=\"https://colab.research.google.com/assets/colab-badge.svg\"\n",
    "alt=\"Open In Colab\"/>\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa880c3e",
   "metadata": {},
   "source": [
    "# Project: Reinforcement Learning\n",
    "- Bigger field - more learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39b3c535",
   "metadata": {},
   "source": [
    "### Project field\n",
    "- Use Reinforcement Learning with Q-learning to find solutions to this field.\n",
    "![Field](https://raw.githubusercontent.com/LearnPythonWithRune/MachineLearningWithPython/main/jupyter/final/img/field-2.png \"Field\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "effaa2b7",
   "metadata": {},
   "source": [
    "### Step 1: Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dad84ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ae15067",
   "metadata": {},
   "source": [
    "### Step 2: Create a field\n",
    "\n",
    "![Field](img/field-3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea04310",
   "metadata": {},
   "source": [
    "- **__init__**:\n",
    "    - Use a list of list with integer values to represent all the states\n",
    "        - Goal end state should be 1, illegal states -1, other states 0\n",
    "    - Set the state to be random fo the size of states\n",
    "- **done**:\n",
    "    - Check if current state has non-negative values\n",
    "- **get_possible_actions**:\n",
    "    - Set a list to all possible actions **actions = [0, 1, 2, 3]**\n",
    "        - action = 0 is left\n",
    "        - action = 1 is right\n",
    "        - action = 2 is up\n",
    "        - action = 3 is down\n",
    "    - Then check if state is in a position where a possible actions should be removed.\n",
    "    - Finally, return the remaining actions\n",
    "- **update_next_state**:\n",
    "    - Get the current state\n",
    "    - Check if move is illegal, then return current state and -10 in reward\n",
    "    - Otherwise opdate state and return the reward according to new state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "04c96397",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "24a0ca19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1, 2), False, [0, 1, 2, 3])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f2fa616f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((0, 2), False, [0, 1, 3])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e2cf74c6",
   "metadata": {},
   "source": [
    "### Step 3: Train the model\n",
    "- Create a $q$-table initialized to all 0\n",
    "    - Use **q_table = np.zeros(...)** *(insert values for ...)*\n",
    "- Set **alpha = .5, gamma = 0.5,** and **epsilon = 0.5**\n",
    "- Create *for*-loop iterating 10000\n",
    "    - Create new field\n",
    "    - While field not done\n",
    "        - Get possible actions and assign to **actions**\n",
    "        - With probability epsilon take a random action, otherwise take the best action\n",
    "            - HINT: **random.uniform(0, 1) < epsilon**\n",
    "            - HINT: Random action: **random.choice(actions)**, and best action: **np.argmax(q_table[field.state])**\n",
    "        - Get current state and assign it to **cur_x, cur_y**\n",
    "        - Update next state and get it and the reward\n",
    "        - Update **q_table[cur_x, cur_y, action] = (1 - alpha)*q_table[cur_x, cur_y, action] + alpha*(reward + gamma*np.max(q_table[next_x, next_y]))**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fe1068f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "def3b371",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[  0.      ,   0.      ,   0.      ,   0.      ],\n",
       "        [ -1.      ,   0.      ,   0.      ,   0.      ],\n",
       "        [  0.      , -10.      ,   0.      ,   0.      ],\n",
       "        [  0.      ,   0.03125 ,   0.      ,   0.03125 ],\n",
       "        [  0.015625,   0.0625  ,   0.      ,   0.0625  ],\n",
       "        [  0.03125 ,   0.125   ,   0.      ,   0.125   ],\n",
       "        [  0.0625  ,   0.25    ,   0.      ,   0.25    ],\n",
       "        [  0.125   ,   0.5     ,   0.      ,   0.5     ],\n",
       "        [  0.25    ,   0.25    ,   0.      ,   1.      ],\n",
       "        [  0.5     ,   0.125   ,   0.      ,   0.5     ],\n",
       "        [  0.25    ,   0.0625  ,   0.      ,   0.25    ],\n",
       "        [  0.125   ,   0.      ,   0.      ,   0.125   ]],\n",
       "\n",
       "       [[  0.      ,   0.      ,   0.      ,   0.      ],\n",
       "        [ -1.      ,   0.      ,   0.      ,   0.      ],\n",
       "        [  0.      , -10.      ,   0.      ,   0.      ],\n",
       "        [  0.      ,   0.0625  ,   0.015625,   0.015625],\n",
       "        [  0.03125 ,   0.125   ,   0.03125 ,   0.03125 ],\n",
       "        [  0.0625  ,   0.25    ,   0.0625  ,   0.0625  ],\n",
       "        [  0.125   ,   0.5     ,   0.125   ,   0.125   ],\n",
       "        [  0.25    ,   1.      ,   0.25    ,   0.25    ],\n",
       "        [  0.      ,   0.      ,   0.      ,   0.      ],\n",
       "        [  1.      ,   0.25    ,   0.25    ,   0.25    ],\n",
       "        [  0.5     ,   0.125   ,   0.125   ,   0.125   ],\n",
       "        [  0.25    ,   0.      ,   0.0625  ,   0.0625  ]],\n",
       "\n",
       "       [[ -5.      ,   0.      ,  -1.      ,   0.      ],\n",
       "        [  0.      ,   0.      ,   0.      ,   0.      ],\n",
       "        [  0.      , -10.      ,   0.      ,   0.      ],\n",
       "        [  0.      ,   0.03125 ,   0.03125 ,   0.      ],\n",
       "        [  0.015625,   0.0625  ,   0.0625  ,   0.      ],\n",
       "        [  0.03125 ,   0.125   ,   0.125   ,   0.      ],\n",
       "        [  0.0625  ,   0.25    ,   0.25    ,   0.      ],\n",
       "        [  0.125   ,   0.5     ,   0.5     ,   0.      ],\n",
       "        [  0.25    ,   0.25    ,   1.      ,   0.      ],\n",
       "        [  0.5     ,   0.125   ,   0.5     ,   0.      ],\n",
       "        [  0.25    ,   0.0625  ,   0.25    ,   0.      ],\n",
       "        [  0.125   ,   0.      ,   0.125   ,   0.      ]]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "83d9654e",
   "metadata": {},
   "source": [
    "### Step 4: Solve a task\n",
    "- To see the path make a variable **path = np.zeros((3, 11))**\n",
    "- Create a field **Field()**\n",
    "- To count steps assign **steps = 1**\n",
    "- Assign the start state in the path to **np.nan**.\n",
    "- The we begin: while not solved.\n",
    "    - Get the **action** to take\n",
    "    - Get the next **state**\n",
    "    - Update **path** with **steps**\n",
    "    - Increment **steps** with one\n",
    "- see the **path**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fd966afa",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 11 is out of bounds for axis 0 with size 11",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/adel/Documents/Project/perso/Rune/repos/MachineLearningWithPython/04 - Project - Reinforcement Learning.ipynb Cell 14'\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/adel/Documents/Project/perso/Rune/repos/MachineLearningWithPython/04%20-%20Project%20-%20Reinforcement%20Learning.ipynb#ch0000013?line=1'>2</a>\u001b[0m field \u001b[39m=\u001b[39m Field()\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/adel/Documents/Project/perso/Rune/repos/MachineLearningWithPython/04%20-%20Project%20-%20Reinforcement%20Learning.ipynb#ch0000013?line=2'>3</a>\u001b[0m steps \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/adel/Documents/Project/perso/Rune/repos/MachineLearningWithPython/04%20-%20Project%20-%20Reinforcement%20Learning.ipynb#ch0000013?line=3'>4</a>\u001b[0m path[field\u001b[39m.\u001b[39mstate[\u001b[39m0\u001b[39m]][field\u001b[39m.\u001b[39mstate[\u001b[39m1\u001b[39m]] \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mnan\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/adel/Documents/Project/perso/Rune/repos/MachineLearningWithPython/04%20-%20Project%20-%20Reinforcement%20Learning.ipynb#ch0000013?line=5'>6</a>\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mnot\u001b[39;00m field\u001b[39m.\u001b[39mdone():\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/adel/Documents/Project/perso/Rune/repos/MachineLearningWithPython/04%20-%20Project%20-%20Reinforcement%20Learning.ipynb#ch0000013?line=6'>7</a>\u001b[0m     action \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39margmax(q_table[field\u001b[39m.\u001b[39mstate])\n",
      "\u001b[0;31mIndexError\u001b[0m: index 11 is out of bounds for axis 0 with size 11"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4ae142e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  5.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0., nan,  1.,  2.,  3.,  4.,  0.,  0.]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "021fd13f",
   "metadata": {},
   "source": [
    "> ### Note\n",
    "> - The training phase (Step 3) could just take random actions\n",
    "> - Our example (Step 4) does not learn anything new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81d50a73",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
