{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e615d705",
   "metadata": {},
   "source": [
    "<a \n",
    " href=\"https://colab.research.google.com/github/LearnPythonWithRune/MachineLearningWithPython/blob/main/colab/final/13 - Project - Information Retrieval (IR).ipynb\"\n",
    " target=\"_parent\">\n",
    "<img \n",
    " src=\"https://colab.research.google.com/assets/colab-badge.svg\"\n",
    "alt=\"Open In Colab\"/>\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f42ffed",
   "metadata": {},
   "source": [
    "# Project: Information Retrieval (IR)\n",
    "- Calculate the TF-IDF of the corpus form 'files/holmes'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "803e001a",
   "metadata": {},
   "source": [
    "### Step 1: Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a43ae8c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import nltk\n",
    "import math\n",
    "from os import system\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa401848",
   "metadata": {},
   "source": [
    "### Step 2: Read the corpus\n",
    "- Read all the Sherlock Holmes texts in files/holmes/\n",
    "- Create a dictionary (dict) calleds corpus\n",
    "- Use os.listdir(...) ([docs](https://docs.python.org/3/library/os.html)) to iterate over all the filenames in 'files/holmes'\n",
    "- For each filename open the file and read the content and add it to the **corpus[filename]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03b7b9a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create local directories in Google Colab\n",
    "!mkdir -p files/holmes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f800a174",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This part, only for colabs, in order to have all the fullpath name in the list \"holmes_files\"\n",
    "\n",
    "REMOTE_DIRECTORY = \"https://raw.githubusercontent.com/LearnPythonWithRune/MachineLearningWithPython/main/jupyter/final/files/holmes/\"\n",
    "\n",
    "FILES = [\"bachelor.txt\", \"clerk.txt\", \"face.txt \", \"problem.txt\", \"twisted.txt\", \"blaze.txt\", \"copper.txt\" , \"gloria_scott.txt\", \"ritual.txt\", \"bohemia.txt\", \"coronet.txt\", \"interpreter.txt\", \"speckled.txt\", \"boscombe.txt\", \"crooked.txt \", \"league.txt\", \"squires.txt\", \"carbuncle.txt\", \"engineer.txt\", \"atient.txt\", \"treaty.txt\"]\n",
    "\n",
    "holmes_files = []\n",
    "for filename in FILES:\n",
    "    full_name = REMOTE_DIRECTORY + filename\n",
    "    system(\"curl -o files/holmes/\"+filename+\" \"+full_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34b211aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = {}\n",
    "\n",
    "for filename in os.listdir('files/holmes/'):\n",
    "  with open(f'files/holmes/{filename}') as f:\n",
    "    corpus[filename] = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6531566a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8b4ec6ab",
   "metadata": {},
   "source": [
    "### Step 3: Tokenize the content\n",
    "- Iterate over **filename** in **corpus**\n",
    "- For each filename assign **corpus[filename]** to be the list of word (in lower) for word in word_tokenize(...) of the content of filename if word is alpha.\n",
    "    - HINT: Use list comprehension\n",
    "    - HINT: Use **.isalpha()**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf670974",
   "metadata": {},
   "outputs": [],
   "source": [
    "for filename in corpus:\n",
    "  corpus[filename] = [word.lower() for word in nltk.word_tokenize(corpus[filename]) if word.isalpha()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d5ed1ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dd72c541",
   "metadata": {},
   "source": [
    "### Step 4: Get all words\n",
    "- Create a set **words**\n",
    "    - HINT: **words = set()**\n",
    "- For each **filename** in **corpus** update the set **words** with the content\n",
    "    - HINT: apply **update(...)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5535b68b",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = set()\n",
    "for filename in corpus:\n",
    "  words.update(corpus[filename])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebd85638",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2e74793c",
   "metadata": {},
   "source": [
    "### Step 5: Calculate term frequency (TF)\n",
    "- Createa empty dictionary (dict) called **tf**\n",
    "- Iterate over **filename** in **corpus**\n",
    "- For each filename add **tf[filename]** with the word frequency.\n",
    "    - HINT: Use dict comprehension with **word** in **words**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d54bf0ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf = {}\n",
    "\n",
    "for filename in corpus:\n",
    "  tf[filename] = {word: corpus[filename].count(word) for word in words}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0676a4a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f4bf275e",
   "metadata": {},
   "source": [
    "### Step 6: Calculate the inverse document frequency (IDF)\n",
    "- Create an empty dictionary called **idf**\n",
    "- Iterate **word** in **words**\n",
    "- For each **word** calculate the number of documents word is in the corpus\n",
    "    - HINT: **freq = sum(word in corpus[filename] for filename in corpus)**\n",
    "- Update **idf[word]** to be the logarithm of number of documents divided by the calcualted frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0df5f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "idf = {}\n",
    "\n",
    "for word in words:\n",
    "  freq = sum(word in corpus[filename] for filename in corpus)\n",
    "  idf[word] = math.log(len(corpus) / freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "136e76cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e84cef46",
   "metadata": {},
   "source": [
    "### Step 7: Calculate the Term Frequence-Inverse Document Frequency (TF-IDF)\n",
    "- Create a dictionary tfidf\n",
    "- Iterate over **filename** in **corpus**\n",
    "- For each **filename** calculate the TF-IDF for each word and add it as pairs **(word, tf-idf)**\n",
    "    - HINT: Use list comprehension **[(word, tf[filename][word] * idf[word]) for word in words]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "225e886e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = {}\n",
    "\n",
    "for filename in corpus:\n",
    "  tfidf[filename] = [(word, tf[filename][word] * idf[word]) for word in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0993c90d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3694d85d",
   "metadata": {},
   "source": [
    "### Step 8: Sort the values\n",
    "- Iterate over **filename** in **corpus**\n",
    "- For each **filename** sort the values in **tfidf** by second item in reverse order\n",
    "    - HINT: Use **sorted** ([docs](https://docs.python.org/3/howto/sorting.html)) with **key=lambda x: x[1]** and **reverse=True**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c3b7b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "for filename in corpus:\n",
    "  tfidf[filename] = sorted(tfidf[filename], key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a6d8b15",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d2c17b08",
   "metadata": {},
   "source": [
    "### Step 9: Print the top five words\n",
    "- Iterate **filename** in **corpus**\n",
    "- For each **filename** print the filename and iterate over the first file elements of **tfidf[filename]** and print the **term** and **score**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7469b1d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for filename in corpus:\n",
    "  print(filename)\n",
    "  for term, score in tfidf[filename][:5]:\n",
    "    print(f' {term}: {score}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fa5cdcb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c8fd30",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
