{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d274932",
   "metadata": {},
   "source": [
    "<a \n",
    " href=\"https://colab.research.google.com/github/LearnPythonWithRune/MachineLearningWithPython/blob/main/colab/final/14 - Project - Word2Vec.ipynb\"\n",
    " target=\"_parent\">\n",
    "<img \n",
    " src=\"https://colab.research.google.com/assets/colab-badge.svg\"\n",
    "alt=\"Open In Colab\"/>\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56b13b1e",
   "metadata": {},
   "source": [
    "# Project: Create a Word2Vec Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b981a3",
   "metadata": {},
   "source": [
    "### Step 1: Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f684a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import nltk\n",
    "from os import system\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9efc384",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create local directories in Google Colab\n",
    "!mkdir -p files/holmes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00e93184",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This part, only for colabs, in order to have all the fullpath name in the list \"holmes_files\"\n",
    "\n",
    "REMOTE_DIRECTORY = \"https://raw.githubusercontent.com/LearnPythonWithRune/MachineLearningWithPython/main/jupyter/final/files/holmes/\"\n",
    "\n",
    "FILES = [\"bachelor.txt\", \"clerk.txt\", \"face.txt \", \"problem.txt\", \"twisted.txt\", \"blaze.txt\", \"copper.txt\" , \"gloria_scott.txt\", \"ritual.txt\", \"bohemia.txt\", \"coronet.txt\", \"interpreter.txt\", \"speckled.txt\", \"boscombe.txt\", \"crooked.txt \", \"league.txt\", \"squires.txt\", \"carbuncle.txt\", \"engineer.txt\", \"atient.txt\", \"treaty.txt\"]\n",
    "\n",
    "holmes_files = []\n",
    "for filename in FILES:\n",
    "    full_name = REMOTE_DIRECTORY + filename\n",
    "    system(\"curl -o files/holmes/\"+filename+\" \"+full_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a98c3250",
   "metadata": {},
   "source": [
    "### Step 2: Download stopwords\n",
    "- Execute the following cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbc59a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9012c520",
   "metadata": {},
   "source": [
    "### Step 3: Read content and sentinize\n",
    "- Initialize an empty list called **all_sentences**\n",
    "- For each filename in **'files/holmes'**:\n",
    "    - HINT: Use **os.listdir(...)** ([docs](https://docs.python.org/3/library/os.html#os.listdir))\n",
    "- Open the file and read the content and convert to lowercase and apply **nltk.sent_tokenize** on content.\n",
    "    - Use **lower()** on content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12d17b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_sentences = []\n",
    "\n",
    "for filename in os.listdir('files/holmes'):\n",
    "    with open(f'files/holmes/{filename}') as f:\n",
    "        content = f.read()\n",
    "        all_sentences += nltk.sent_tokenize(content.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31914b8b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "64e4362c",
   "metadata": {},
   "source": [
    "### Step 4: Tokenize each sentence\n",
    "- Get all words by applying **nltk.word_tokenize** on them and assign the result to **all_words**\n",
    "    - HINT: Use list comprehension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74bc20df",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = [nltk.word_tokenize(sent) for sent in all_sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb5143be",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words[0][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "959014e8",
   "metadata": {},
   "source": [
    "### Step 5: Remove all stop words\n",
    "- Use **stopwords.words('english')** to filter all the words in **all_words**\n",
    "    - HINT: iterate over the length of **all_words**, for each index use list comprehension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "056ed61a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(all_words)):\n",
    "    all_words[i] = [w for w in all_words[i] if w not in stopwords.words('english')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ffef523",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "40656352",
   "metadata": {},
   "source": [
    "### Step 6: Remove special characters\n",
    "- Iterate over items in **all_words** to remove words with special characters\n",
    "    - HINT: Use **isalpha()** ([doc](https://docs.python.org/3/library/stdtypes.html#str.isalpha))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e6a0003",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(all_words)):\n",
    "    all_words[i] = [w for w in all_words[i] if w.isalpha()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a52d6b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b8bca8cf",
   "metadata": {},
   "source": [
    "### Step 7: Install gensim and python-Levenshtein\n",
    "- Run the following cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8036c213",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa499e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install python-Levenshtein"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e16a94c5",
   "metadata": {},
   "source": [
    "### Step 8: Import another library\n",
    "- Run the following cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e7179bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6dad3d2",
   "metadata": {},
   "source": [
    "### Step 9: Create a model\n",
    "- Use **Word2Vec** on **all_words**\n",
    "    - Use **min_count=2** : Ignores all words with total frequency lower than this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5fa076a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec(all_words, min_count=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1940078",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1da22183",
   "metadata": {},
   "source": [
    "### Step 10: Find distances\n",
    "- Try to run **model.wv.distance('holmes', 'watson')**\n",
    "- Try to run **model.wv.distance('holmes', 'water')**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18791254",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.distance('holmes', 'watson')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f285841f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.distance('holmes', 'water')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b26dd8c",
   "metadata": {},
   "source": [
    "### Step 11: Find closests words\n",
    "- Get all the words\n",
    "    - HINT: **words = model.wv.index2entity**\n",
    "- Implement a function **closets_words(word)**\n",
    "    - HINT: **distances = {w: model.wv.distance(word, w) for w in words}**\n",
    "    - HINT: **sorted(distances, key=lambda w: distances[w])[:15]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a2702f",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = model.wv.index2entity\n",
    "\n",
    "def closets_words(word):\n",
    "    distances = {w: model.wv.distance(word, w) for w in words}\n",
    "    return sorted(distances, key=lambda w: distances[w])[:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d196ecab",
   "metadata": {},
   "outputs": [],
   "source": [
    "closets_words('holmes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c22630",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
